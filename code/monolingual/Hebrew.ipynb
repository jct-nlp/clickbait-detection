{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vPRssl5D_GRy","executionInfo":{"status":"ok","timestamp":1724537036166,"user_tz":-180,"elapsed":33504,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"}},"outputId":"d327feea-04f6-4106-ed54-619f14f8909a"},"id":"vPRssl5D_GRy","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","id":"gMiWbNRKfRZro1tVCaNhFTwH","metadata":{"tags":[],"id":"gMiWbNRKfRZro1tVCaNhFTwH"},"source":["!pip install torch torchvision transformers pandas"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, Dataset\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import time\n","import traceback\n","\n","class ClickbaitDataset(Dataset):\n","    def __init__(self, titles, labels, tokenizer, max_len):\n","        self.titles = titles\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.titles)\n","\n","    def __getitem__(self, idx):\n","        title = self.titles[idx]\n","        label = self.labels[idx]\n","\n","        try:\n","            label = int(label)\n","        except ValueError:\n","            raise ValueError(f\"Label {label} at index {idx} is not a valid integer.\")\n","\n","        encoding = self.tokenizer.encode_plus(\n","            title,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'title_text': title,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","def create_data_loader(titles, labels, tokenizer, max_len, batch_size):\n","    ds = ClickbaitDataset(\n","        titles=titles,\n","        labels=labels,\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","\n","    return DataLoader(ds, batch_size=batch_size, num_workers=4, pin_memory=True)\n","\n","def train_epoch(\n","    model,\n","    data_loader,\n","    loss_fn,\n","    optimizer,\n","    device,\n","    scheduler,\n","    n_examples\n","):\n","    model.train()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    for batch_idx, d in enumerate(data_loader):\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        labels = d[\"labels\"].to(device)\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        _, preds = torch.max(outputs.logits, dim=1)\n","        loss = loss_fn(outputs.logits, labels)\n","\n","        correct_predictions += torch.sum(preds == labels)\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        if batch_idx % 10 == 0:\n","            print(f'Batch {batch_idx}/{len(data_loader)}, Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Training epoch completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model.eval()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    all_labels = []\n","    all_preds = []\n","\n","    with torch.no_grad():\n","        for batch_idx, d in enumerate(data_loader):\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            labels = d[\"labels\"].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            loss = loss_fn(outputs.logits, labels)\n","\n","            correct_predictions += torch.sum(preds == labels)\n","            losses.append(loss.item())\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","\n","            if batch_idx % 10 == 0:\n","                print(f'Batch {batch_idx}/{len(data_loader)}, Test Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Test evaluation completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses), all_labels, all_preds\n","\n","def clean_dataset(df):\n","    df = df.dropna(subset=['label'])\n","    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n","    df = df.dropna(subset=['label'])\n","    df['label'] = df['label'].astype(int)\n","    return df\n","\n","def main():\n","    print(\"Starting main process\")\n","\n","    try:\n","        print('Loading the dataset.')\n","        df_train = pd.read_excel('/content/drive/MyDrive/clickbait/hebrew.xlsx')\n","        print('Training dataset loaded successfully.')\n","\n","        print('Cleaning the training dataset.')\n","        df_train = clean_dataset(df_train)\n","        print('Training dataset cleaned.')\n","\n","        print('Loading the test dataset.')\n","        df_test = pd.read_excel('/content/drive/MyDrive/clickbait/‏‏hebrew_test.xlsx')\n","        print('Test dataset loaded successfully.')\n","\n","        RANDOM_SEED = 42\n","        MAX_LEN = 128\n","        BATCH_SIZE = 16\n","        EPOCHS = 5\n","        EARLY_STOPPING_PATIENCE = 2\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f'Using device: {device}')\n","\n","        print('Loading the tokenizer and model.')\n","        tokenizer = BertTokenizer.from_pretrained('avichr/heBERT')\n","        model = BertForSequenceClassification.from_pretrained('avichr/heBERT', num_labels=2)\n","\n","        # Adding Dropout\n","        model.config.hidden_dropout_prob = 0.3\n","        model.config.attention_probs_dropout_prob = 0.3\n","\n","        # Apply L2 regularization\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n","\n","        model = model.to(device)\n","        print('Model and tokenizer loaded and moved to device.')\n","\n","        print('Creating data loaders.')\n","        train_titles = df_train['title'].tolist()\n","        train_labels = df_train['label'].tolist()\n","        test_titles = df_test['title'].tolist()\n","        test_labels = df_test['label'].tolist()\n","\n","        train_data_loader = create_data_loader(train_titles, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        test_data_loader = create_data_loader(test_titles, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        print('Data loaders created.')\n","\n","        total_steps = len(train_data_loader) * EPOCHS\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","        loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","        print('Optimizer, scheduler, and loss function defined.')\n","\n","        best_val_loss = float('inf')\n","        patience_counter = 0\n","\n","        for epoch in range(EPOCHS):\n","            print(f'Starting epoch {epoch + 1}/{EPOCHS}')\n","\n","            train_acc, train_loss = train_epoch(\n","                model,\n","                train_data_loader,\n","                loss_fn,\n","                optimizer,\n","                device,\n","                scheduler,\n","                len(df_train)\n","            )\n","            print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","            test_acc, test_loss, all_labels, all_preds = eval_model(\n","                model,\n","                test_data_loader,\n","                loss_fn,\n","                device,\n","                len(df_test)\n","            )\n","            print(f'Test loss {test_loss} accuracy {test_acc}')\n","\n","            # Calculate additional metrics\n","            precision = precision_score(all_labels, all_preds, average='weighted')\n","            recall = recall_score(all_labels, all_preds, average='weighted')\n","            f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","            print(f'Test Precision: {precision}')\n","            print(f'Test Recall: {recall}')\n","            print(f'Test F1 Score: {f1}')\n","\n","            # Early stopping\n","            if test_loss < best_val_loss:\n","                best_val_loss = test_loss\n","                patience_counter = 0\n","                print('Validation loss improved, saving model.')\n","                # Save model checkpoint if needed\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= EARLY_STOPPING_PATIENCE:\n","                    print('Early stopping triggered.')\n","                    break\n","\n","    except Exception as e:\n","        print(f'An error occurred: {e}')\n","        traceback.print_exc()\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYViO0Wa_IzA","outputId":"dc15cbd2-6306-4e2e-c252-5241d3ae743a","executionInfo":{"status":"ok","timestamp":1724545814006,"user_tz":-180,"elapsed":4405998,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"}}},"id":"qYViO0Wa_IzA","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting main process\n","Loading the dataset.\n","Training dataset loaded successfully.\n","Cleaning the training dataset.\n","Training dataset cleaned.\n","Loading the test dataset.\n","Test dataset loaded successfully.\n","Using device: cpu\n","Loading the tokenizer and model.\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model and tokenizer loaded and moved to device.\n","Creating data loaders.\n","Data loaders created.\n","Optimizer, scheduler, and loss function defined.\n","Starting epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Batch 0/66, Loss: 0.7183716297149658\n","Batch 10/66, Loss: 0.5318698287010193\n","Batch 20/66, Loss: 0.4278326630592346\n","Batch 30/66, Loss: 0.4228333532810211\n","Batch 40/66, Loss: 0.38035330176353455\n","Batch 50/66, Loss: 0.30478718876838684\n","Batch 60/66, Loss: 0.28793928027153015\n","Training epoch completed in: 22m 59s\n","Train loss 0.46569276736541226 accuracy 0.796384395813511\n","Batch 0/17, Test Loss: 0.4037087559700012\n","Batch 10/17, Test Loss: 0.41212737560272217\n","Test evaluation completed in: 1m 49s\n","Test loss 0.3120093231692034 accuracy 0.8787878787878788\n","Test Precision: 0.8790529758011772\n","Test Recall: 0.8787878787878788\n","Test F1 Score: 0.8787182126454499\n","Validation loss improved, saving model.\n","Starting epoch 2/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Batch 0/66, Loss: 0.3205702006816864\n","Batch 10/66, Loss: 0.31417325139045715\n","Batch 20/66, Loss: 0.18482893705368042\n","Batch 30/66, Loss: 0.1350516378879547\n","Batch 40/66, Loss: 0.2185259312391281\n","Batch 50/66, Loss: 0.17412744462490082\n","Batch 60/66, Loss: 0.09435988962650299\n","Training epoch completed in: 22m 24s\n","Train loss 0.22420745151061 accuracy 0.9181731684110371\n","Batch 0/17, Test Loss: 0.4352775514125824\n","Batch 10/17, Test Loss: 0.4576916992664337\n","Test evaluation completed in: 1m 49s\n","Test loss 0.3273055969792254 accuracy 0.8825757575757576\n","Test Precision: 0.8835155118490375\n","Test Recall: 0.8825757575757576\n","Test F1 Score: 0.8824220675551813\n","Starting epoch 3/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Batch 0/66, Loss: 0.1428309679031372\n","Batch 10/66, Loss: 0.17387254536151886\n","Batch 20/66, Loss: 0.04425901547074318\n","Batch 30/66, Loss: 0.10249293595552444\n","Batch 40/66, Loss: 0.10581494867801666\n","Batch 50/66, Loss: 0.036190878599882126\n","Batch 60/66, Loss: 0.04705500975251198\n","Training epoch completed in: 22m 32s\n","Train loss 0.09330155975608663 accuracy 0.9705042816365367\n","Batch 0/17, Test Loss: 0.7020514607429504\n","Batch 10/17, Test Loss: 0.6937597990036011\n","Test evaluation completed in: 1m 49s\n","Test loss 0.4944565953577266 accuracy 0.8712121212121212\n","Test Precision: 0.8778845690610397\n","Test Recall: 0.8712121212121212\n","Test F1 Score: 0.8704071969696969\n","Early stopping triggered.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rtqyLyR2LXND"},"id":"rtqyLyR2LXND","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47642,"status":"ok","timestamp":1722721570694,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"},"user_tz":-180},"id":"X26qiUbJHNLn","outputId":"fc4b3891-3bbf-4925-ad2b-821729ad0fc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install torch torchvision transformers pandas"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["91f6333ecd874ddfa3e2b0e039737684","c2a50843ae1841a7a411f3eea046236a","083ee4e531ba45d5af31375f0aee6a23","f5b56b80974e4e29bb40cd0a39161c82","79e401238a904a80a948b32fc8b99efb","672661d3462a455d926cd08e44a5bf4e","8593e9393fe34f5d97efaae477bb6751","57f265ce2fc349bd93f6d302dada4fd0","64b1a2dae9ec4135b2f29f93f9dac5fd","81069a2af9a444bfb7500a6b6a0dfec9","fb6065d565cc40c9b0f8a4cd0cf4d19d","be9372aa4a6241bfa59f3bad889fb778","bdc0602d98c14fe7a1d28cf55137eee7","21ae599f7815447f9deac5541aec769e","3480f20a225e4fd5b3c87b2e726bb411","41a1f63a17414a66bd8389aee7cc20ba","b39288d717c143cca285c133e7f83c7b","82417b2a2b19438d880f42dc59375b86","cc9ad4e26b904cb89f6116bcb5dffda2","7470d9c2c33c4541aee2b2e631bf84a1","78740c6ab8cd4649b478a2ed29345bda","f0eca848c94c40a5b272512b681c3a3c","36645a6856434a47ac7b82874aac8dc0","86fc3cd63ce443558380aced28839a0f","d6c0312cfe59498498f1fe84ca2614cd","2417fe58bd904ed18ce0e2722ec5a56c","b9297fec5e9140178535e8c2dbadc477","4dc79bac87204ccf89b9c8c1440c0aa3","c1d89d3dc2854d009c0f7e1530daa04d","192799d22be940ae87ba8455764bbac4","aba2a197938b4c5ea11109637230695e","4bedbdb25177477782044be30ad227f8","471c79730e9a419f805aeaa24620b8b1","d2200377bdb4492aaa25df3cf5b013fb","070e2584fafe4cfba48412aeaac26a2d","d55a9788f2644a1fa3e4547db45a295a","f1780afbe5f148bb98cb0887a348e7ce","3dafde9f4dfd4750ae7a366990a71da9","d5059b8da02043f08b92a94f6fcd614c","fbe5d6afc98941c1860ce11c4f243e4c","8f83c089a9474164804e465fa2257be1","2c6f07c0fda548f1a77d853666d3f202","7f2f39954de14169bebc4edab8e799dc","c3c51cd07a3546049a42a4c06c873395"]},"id":"pPSyyuKOEnOS","executionInfo":{"status":"ok","timestamp":1722722990853,"user_tz":-180,"elapsed":1071307,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"}},"outputId":"e5fe0705-3894-4c3d-c250-c54977d15d50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting main process\n","Loading the dataset.\n","Dataset loaded successfully.\n","Cleaning the dataset.\n","Dataset cleaned.\n","Splitting the dataset into training and test sets.\n","Dataset split into training and test sets.\n","Using device: cuda\n","Loading the tokenizer and model.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f6333ecd874ddfa3e2b0e039737684"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/397k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be9372aa4a6241bfa59f3bad889fb778"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36645a6856434a47ac7b82874aac8dc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/500M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2200377bdb4492aaa25df3cf5b013fb"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dumitrescustefan/bert-base-romanian-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model and tokenizer loaded and moved to device.\n","Creating data loaders.\n","Data loaders created.\n","Optimizer, scheduler, and loss function defined.\n","Starting epoch 1/10\n","Batch 0/544, Loss: 0.669551432132721\n","Batch 10/544, Loss: 0.6300897002220154\n","Batch 20/544, Loss: 0.5320746898651123\n","Batch 30/544, Loss: 0.4507359266281128\n","Batch 40/544, Loss: 0.4290192127227783\n","Batch 50/544, Loss: 0.4806038439273834\n","Batch 60/544, Loss: 0.2670822739601135\n","Batch 70/544, Loss: 0.29422712326049805\n","Batch 80/544, Loss: 0.524325430393219\n","Batch 90/544, Loss: 0.20689520239830017\n","Batch 100/544, Loss: 0.38651731610298157\n","Batch 110/544, Loss: 0.2962563931941986\n","Batch 120/544, Loss: 0.3030029833316803\n","Batch 130/544, Loss: 0.21169297397136688\n","Batch 140/544, Loss: 0.48967409133911133\n","Batch 150/544, Loss: 0.48120129108428955\n","Batch 160/544, Loss: 0.3500380516052246\n","Batch 170/544, Loss: 0.17729361355304718\n","Batch 180/544, Loss: 0.17849715054035187\n","Batch 190/544, Loss: 0.2666100859642029\n","Batch 200/544, Loss: 0.36878958344459534\n","Batch 210/544, Loss: 0.2560492157936096\n","Batch 220/544, Loss: 0.2685468792915344\n","Batch 230/544, Loss: 0.180303156375885\n","Batch 240/544, Loss: 0.1490183025598526\n","Batch 250/544, Loss: 0.16478584706783295\n","Batch 260/544, Loss: 0.3832536041736603\n","Batch 270/544, Loss: 0.16489876806735992\n","Batch 280/544, Loss: 0.11884400993585587\n","Batch 290/544, Loss: 0.44076472520828247\n","Batch 300/544, Loss: 0.1026512011885643\n","Batch 310/544, Loss: 0.6435056328773499\n","Batch 320/544, Loss: 0.18909472227096558\n","Batch 330/544, Loss: 0.22457124292850494\n","Batch 340/544, Loss: 0.2729173004627228\n","Batch 350/544, Loss: 0.3137539029121399\n","Batch 360/544, Loss: 0.1933877170085907\n","Batch 370/544, Loss: 0.22505557537078857\n","Batch 380/544, Loss: 0.19219832122325897\n","Batch 390/544, Loss: 0.3809196352958679\n","Batch 400/544, Loss: 0.1338208168745041\n","Batch 410/544, Loss: 0.09764115512371063\n","Batch 420/544, Loss: 0.20309430360794067\n","Batch 430/544, Loss: 0.41204094886779785\n","Batch 440/544, Loss: 0.2720176875591278\n","Batch 450/544, Loss: 0.2283056378364563\n","Batch 460/544, Loss: 0.5624867677688599\n","Batch 470/544, Loss: 0.5590286254882812\n","Batch 480/544, Loss: 0.3298541307449341\n","Batch 490/544, Loss: 0.1709626317024231\n","Batch 500/544, Loss: 0.34589725732803345\n","Batch 510/544, Loss: 0.1902993768453598\n","Batch 520/544, Loss: 0.04855755344033241\n","Batch 530/544, Loss: 0.2600537836551666\n","Batch 540/544, Loss: 0.2705230116844177\n","Training epoch completed in: 1m 31s\n","Train loss 0.33353586718850936 accuracy 0.8571264235591856\n","Batch 0/136, Test Loss: 0.09962651133537292\n","Batch 10/136, Test Loss: 0.5899107456207275\n","Batch 20/136, Test Loss: 0.15494146943092346\n","Batch 30/136, Test Loss: 0.41935089230537415\n","Batch 40/136, Test Loss: 0.2505880892276764\n","Batch 50/136, Test Loss: 0.4164905548095703\n","Batch 60/136, Test Loss: 0.1799677312374115\n","Batch 70/136, Test Loss: 0.4211985468864441\n","Batch 80/136, Test Loss: 0.2663755714893341\n","Batch 90/136, Test Loss: 0.3441382646560669\n","Batch 100/136, Test Loss: 0.5809522867202759\n","Batch 110/136, Test Loss: 0.2579168677330017\n","Batch 120/136, Test Loss: 0.18183360993862152\n","Batch 130/136, Test Loss: 0.17860940098762512\n","Test evaluation completed in: 0m 7s\n","Test loss 0.2726683066883946 accuracy 0.8891444342226311\n","Test Precision: 0.8900971313216254\n","Test Recall: 0.8891444342226311\n","Test F1 Score: 0.8893615941441618\n","Starting epoch 2/10\n","Batch 0/544, Loss: 0.31430989503860474\n","Batch 10/544, Loss: 0.30999451875686646\n","Batch 20/544, Loss: 0.16212806105613708\n","Batch 30/544, Loss: 0.161674365401268\n","Batch 40/544, Loss: 0.22300702333450317\n","Batch 50/544, Loss: 0.18492664396762848\n","Batch 60/544, Loss: 0.12158294767141342\n","Batch 70/544, Loss: 0.07587984204292297\n","Batch 80/544, Loss: 0.25793251395225525\n","Batch 90/544, Loss: 0.08843351900577545\n","Batch 100/544, Loss: 0.18714028596878052\n","Batch 110/544, Loss: 0.09377606213092804\n","Batch 120/544, Loss: 0.18614768981933594\n","Batch 130/544, Loss: 0.08400656282901764\n","Batch 140/544, Loss: 0.30831295251846313\n","Batch 150/544, Loss: 0.3197353482246399\n","Batch 160/544, Loss: 0.25154465436935425\n","Batch 170/544, Loss: 0.065951868891716\n","Batch 180/544, Loss: 0.05497240647673607\n","Batch 190/544, Loss: 0.08694709837436676\n","Batch 200/544, Loss: 0.21340446174144745\n","Batch 210/544, Loss: 0.16471265256404877\n","Batch 220/544, Loss: 0.19800718128681183\n","Batch 230/544, Loss: 0.13554957509040833\n","Batch 240/544, Loss: 0.051429107785224915\n","Batch 250/544, Loss: 0.11769882589578629\n","Batch 260/544, Loss: 0.36166542768478394\n","Batch 270/544, Loss: 0.08198444545269012\n","Batch 280/544, Loss: 0.04855557158589363\n","Batch 290/544, Loss: 0.17389914393424988\n","Batch 300/544, Loss: 0.04370429739356041\n","Batch 310/544, Loss: 0.3547452390193939\n","Batch 320/544, Loss: 0.08323212713003159\n","Batch 330/544, Loss: 0.1463785171508789\n","Batch 340/544, Loss: 0.14202702045440674\n","Batch 350/544, Loss: 0.06969813257455826\n","Batch 360/544, Loss: 0.06391789019107819\n","Batch 370/544, Loss: 0.11897801607847214\n","Batch 380/544, Loss: 0.0477897971868515\n","Batch 390/544, Loss: 0.19397307932376862\n","Batch 400/544, Loss: 0.05913753807544708\n","Batch 410/544, Loss: 0.1003723070025444\n","Batch 420/544, Loss: 0.11155764013528824\n","Batch 430/544, Loss: 0.3544837236404419\n","Batch 440/544, Loss: 0.25399139523506165\n","Batch 450/544, Loss: 0.17846596240997314\n","Batch 460/544, Loss: 0.2744951844215393\n","Batch 470/544, Loss: 0.28056904673576355\n","Batch 480/544, Loss: 0.2385667860507965\n","Batch 490/544, Loss: 0.12799741327762604\n","Batch 500/544, Loss: 0.2280513048171997\n","Batch 510/544, Loss: 0.16701406240463257\n","Batch 520/544, Loss: 0.027717867866158485\n","Batch 530/544, Loss: 0.09561413526535034\n","Batch 540/544, Loss: 0.0949704498052597\n","Training epoch completed in: 1m 33s\n","Train loss 0.19251915518714882 accuracy 0.922121246980329\n","Batch 0/136, Test Loss: 0.2379867285490036\n","Batch 10/136, Test Loss: 0.787203311920166\n","Batch 20/136, Test Loss: 0.30597642064094543\n","Batch 30/136, Test Loss: 0.39978861808776855\n","Batch 40/136, Test Loss: 0.23970691859722137\n","Batch 50/136, Test Loss: 0.46255016326904297\n","Batch 60/136, Test Loss: 0.2539636194705963\n","Batch 70/136, Test Loss: 0.5768144726753235\n","Batch 80/136, Test Loss: 0.3868507742881775\n","Batch 90/136, Test Loss: 0.325855016708374\n","Batch 100/136, Test Loss: 0.7119309902191162\n","Batch 110/136, Test Loss: 0.3092482089996338\n","Batch 120/136, Test Loss: 0.21527883410453796\n","Batch 130/136, Test Loss: 0.10253879427909851\n","Test evaluation completed in: 0m 7s\n","Test loss 0.3145179788595723 accuracy 0.8730450781968722\n","Test Precision: 0.8731450727799557\n","Test Recall: 0.8730450781968722\n","Test F1 Score: 0.8725426573045731\n","Starting epoch 3/10\n","Batch 0/544, Loss: 0.11527757346630096\n","Batch 10/544, Loss: 0.16830070316791534\n","Batch 20/544, Loss: 0.06929927319288254\n","Batch 30/544, Loss: 0.1593651920557022\n","Batch 40/544, Loss: 0.11222927272319794\n","Batch 50/544, Loss: 0.08837129920721054\n","Batch 60/544, Loss: 0.08197718858718872\n","Batch 70/544, Loss: 0.01838555373251438\n","Batch 80/544, Loss: 0.16866347193717957\n","Batch 90/544, Loss: 0.04694367200136185\n","Batch 100/544, Loss: 0.06103025749325752\n","Batch 110/544, Loss: 0.049752362072467804\n","Batch 120/544, Loss: 0.19564460217952728\n","Batch 130/544, Loss: 0.05301135033369064\n","Batch 140/544, Loss: 0.14785519242286682\n","Batch 150/544, Loss: 0.1402416080236435\n","Batch 160/544, Loss: 0.040400516241788864\n","Batch 170/544, Loss: 0.05694073066115379\n","Batch 180/544, Loss: 0.007922827266156673\n","Batch 190/544, Loss: 0.11479849368333817\n","Batch 200/544, Loss: 0.07753688842058182\n","Batch 210/544, Loss: 0.11585891246795654\n","Batch 220/544, Loss: 0.017655322328209877\n","Batch 230/544, Loss: 0.025424961000680923\n","Batch 240/544, Loss: 0.033820345997810364\n","Batch 250/544, Loss: 0.04056493565440178\n","Batch 260/544, Loss: 0.07911290228366852\n","Batch 270/544, Loss: 0.011220099404454231\n","Batch 280/544, Loss: 0.03109602816402912\n","Batch 290/544, Loss: 0.04353267326951027\n","Batch 300/544, Loss: 0.013327996246516705\n","Batch 310/544, Loss: 0.23260174691677094\n","Batch 320/544, Loss: 0.020890798419713974\n","Batch 330/544, Loss: 0.01853465475142002\n","Batch 340/544, Loss: 0.014823558740317822\n","Batch 350/544, Loss: 0.00934914406388998\n","Batch 360/544, Loss: 0.012206737883388996\n","Batch 370/544, Loss: 0.0540802925825119\n","Batch 380/544, Loss: 0.026185862720012665\n","Batch 390/544, Loss: 0.04548080265522003\n","Batch 400/544, Loss: 0.006001150235533714\n","Batch 410/544, Loss: 0.4410839080810547\n","Batch 420/544, Loss: 0.045649003237485886\n","Batch 430/544, Loss: 0.4360603392124176\n","Batch 440/544, Loss: 0.006036879029124975\n","Batch 450/544, Loss: 0.031561870127916336\n","Batch 460/544, Loss: 0.3122924268245697\n","Batch 470/544, Loss: 0.09642486274242401\n","Batch 480/544, Loss: 0.19391445815563202\n","Batch 490/544, Loss: 0.06378340721130371\n","Batch 500/544, Loss: 0.0630512684583664\n","Batch 510/544, Loss: 0.1480286866426468\n","Batch 520/544, Loss: 0.03672719746828079\n","Batch 530/544, Loss: 0.06224709749221802\n","Batch 540/544, Loss: 0.024430081248283386\n","Training epoch completed in: 1m 33s\n","Train loss 0.09352343259182389 accuracy 0.9674450707465777\n","Batch 0/136, Test Loss: 0.40176454186439514\n","Batch 10/136, Test Loss: 1.1778159141540527\n","Batch 20/136, Test Loss: 0.3177851736545563\n","Batch 30/136, Test Loss: 0.48802947998046875\n","Batch 40/136, Test Loss: 0.3450935184955597\n","Batch 50/136, Test Loss: 0.4059702754020691\n","Batch 60/136, Test Loss: 0.31062549352645874\n","Batch 70/136, Test Loss: 0.9320023655891418\n","Batch 80/136, Test Loss: 0.5497369766235352\n","Batch 90/136, Test Loss: 0.320780873298645\n","Batch 100/136, Test Loss: 0.760793149471283\n","Batch 110/136, Test Loss: 0.4524710476398468\n","Batch 120/136, Test Loss: 0.3373238742351532\n","Batch 130/136, Test Loss: 0.13026264309883118\n","Test evaluation completed in: 0m 7s\n","Test loss 0.3914764285864829 accuracy 0.8735050597976081\n","Test Precision: 0.8735543164021059\n","Test Recall: 0.8735050597976081\n","Test F1 Score: 0.8735280053553314\n","Starting epoch 4/10\n","Batch 0/544, Loss: 0.060456283390522\n","Batch 10/544, Loss: 0.024315301328897476\n","Batch 20/544, Loss: 0.004896293859928846\n","Batch 30/544, Loss: 0.13669075071811676\n","Batch 40/544, Loss: 0.04126032441854477\n","Batch 50/544, Loss: 0.08109501004219055\n","Batch 60/544, Loss: 0.01117735169827938\n","Batch 70/544, Loss: 0.20302295684814453\n","Batch 80/544, Loss: 0.14060258865356445\n","Batch 90/544, Loss: 0.01314615085721016\n","Batch 100/544, Loss: 0.06369809061288834\n","Batch 110/544, Loss: 0.00415563490241766\n","Batch 120/544, Loss: 0.009435690939426422\n","Batch 130/544, Loss: 0.007309656124562025\n","Batch 140/544, Loss: 0.03243372216820717\n","Batch 150/544, Loss: 0.06303666532039642\n","Batch 160/544, Loss: 0.01076732948422432\n","Batch 170/544, Loss: 0.008125875145196915\n","Batch 180/544, Loss: 0.006634769961237907\n","Batch 190/544, Loss: 0.002221551025286317\n","Batch 200/544, Loss: 0.019265191629529\n","Batch 210/544, Loss: 0.01694444939494133\n","Batch 220/544, Loss: 0.008152324706315994\n","Batch 230/544, Loss: 0.011382970958948135\n","Batch 240/544, Loss: 0.007348290644586086\n","Batch 250/544, Loss: 0.013087168335914612\n","Batch 260/544, Loss: 0.05618346109986305\n","Batch 270/544, Loss: 0.007077130023390055\n","Batch 280/544, Loss: 0.011118108406662941\n","Batch 290/544, Loss: 0.02123725228011608\n","Batch 300/544, Loss: 0.006250359583646059\n","Batch 310/544, Loss: 0.057630524039268494\n","Batch 320/544, Loss: 0.001954740844666958\n","Batch 330/544, Loss: 0.016938168555498123\n","Batch 340/544, Loss: 0.018098676577210426\n","Batch 350/544, Loss: 0.0036440666299313307\n","Batch 360/544, Loss: 0.0030611890833824873\n","Batch 370/544, Loss: 0.010300942696630955\n","Batch 380/544, Loss: 0.010117968544363976\n","Batch 390/544, Loss: 0.01578426919877529\n","Batch 400/544, Loss: 0.004563955590128899\n","Batch 410/544, Loss: 0.31018728017807007\n","Batch 420/544, Loss: 0.0062064072117209435\n","Batch 430/544, Loss: 0.20712785422801971\n","Batch 440/544, Loss: 0.0033872302155941725\n","Batch 450/544, Loss: 0.013462440110743046\n","Batch 460/544, Loss: 0.011872351169586182\n","Batch 470/544, Loss: 0.2003084421157837\n","Batch 480/544, Loss: 0.027415035292506218\n","Batch 490/544, Loss: 0.04117414355278015\n","Batch 500/544, Loss: 0.01896418258547783\n","Batch 510/544, Loss: 0.0020881211385130882\n","Batch 520/544, Loss: 0.004878368694335222\n","Batch 530/544, Loss: 0.008757914416491985\n","Batch 540/544, Loss: 0.0017430005827918649\n","Training epoch completed in: 1m 33s\n","Train loss 0.048732014030974824 accuracy 0.983434947659036\n","Batch 0/136, Test Loss: 0.7110869884490967\n","Batch 10/136, Test Loss: 1.3593491315841675\n","Batch 20/136, Test Loss: 0.8038334846496582\n","Batch 30/136, Test Loss: 0.538266658782959\n","Batch 40/136, Test Loss: 0.45908960700035095\n","Batch 50/136, Test Loss: 0.3581221103668213\n","Batch 60/136, Test Loss: 0.39434221386909485\n","Batch 70/136, Test Loss: 0.8941770195960999\n","Batch 80/136, Test Loss: 0.5467860698699951\n","Batch 90/136, Test Loss: 0.6611517071723938\n","Batch 100/136, Test Loss: 1.0716681480407715\n","Batch 110/136, Test Loss: 0.44137755036354065\n","Batch 120/136, Test Loss: 0.47107550501823425\n","Batch 130/136, Test Loss: 0.04340654984116554\n","Test evaluation completed in: 0m 7s\n","Test loss 0.49504212884451537 accuracy 0.8707451701931923\n","Test Precision: 0.8724629112686756\n","Test Recall: 0.8707451701931923\n","Test F1 Score: 0.869647306919708\n","Starting epoch 5/10\n","Batch 0/544, Loss: 0.01370624266564846\n","Batch 10/544, Loss: 0.002385991159826517\n","Batch 20/544, Loss: 0.005248082336038351\n","Batch 30/544, Loss: 0.040003642439842224\n","Batch 40/544, Loss: 0.03187048062682152\n","Batch 50/544, Loss: 0.011896034702658653\n","Batch 60/544, Loss: 0.0023471440654248\n","Batch 70/544, Loss: 0.0018234014278277755\n","Batch 80/544, Loss: 0.06107759475708008\n","Batch 90/544, Loss: 0.0054648458026349545\n","Batch 100/544, Loss: 0.01189645566046238\n","Batch 110/544, Loss: 0.0011250918032601476\n","Batch 120/544, Loss: 0.008074556477367878\n","Batch 130/544, Loss: 0.0011581103317439556\n","Batch 140/544, Loss: 0.014124752953648567\n","Batch 150/544, Loss: 0.008073626086115837\n","Batch 160/544, Loss: 0.003385938936844468\n","Batch 170/544, Loss: 0.011916086077690125\n","Batch 180/544, Loss: 0.0007391914259642363\n","Batch 190/544, Loss: 0.002152965869754553\n","Batch 200/544, Loss: 0.00492781912907958\n","Batch 210/544, Loss: 0.06558914482593536\n","Batch 220/544, Loss: 0.08149198442697525\n","Batch 230/544, Loss: 0.009963500313460827\n","Batch 240/544, Loss: 0.002097118878737092\n","Batch 250/544, Loss: 0.0006528468802571297\n","Batch 260/544, Loss: 0.10876403003931046\n","Batch 270/544, Loss: 0.0012761034304276109\n","Batch 280/544, Loss: 0.009584633633494377\n","Batch 290/544, Loss: 0.00886568147689104\n","Batch 300/544, Loss: 0.12670795619487762\n","Batch 310/544, Loss: 0.009176782332360744\n","Batch 320/544, Loss: 0.009719771333038807\n","Batch 330/544, Loss: 0.008004199713468552\n","Batch 340/544, Loss: 0.0136249465867877\n","Batch 350/544, Loss: 0.01307634636759758\n","Batch 360/544, Loss: 0.0012604136718437076\n","Batch 370/544, Loss: 0.0059070526622235775\n","Batch 380/544, Loss: 0.0007139155641198158\n","Batch 390/544, Loss: 0.002207402139902115\n","Batch 400/544, Loss: 0.005280837416648865\n","Batch 410/544, Loss: 0.14327870309352875\n","Batch 420/544, Loss: 0.0007996853673830628\n","Batch 430/544, Loss: 0.029716096818447113\n","Batch 440/544, Loss: 0.0011915821814909577\n","Batch 450/544, Loss: 0.006491365376859903\n","Batch 460/544, Loss: 0.05431254953145981\n","Batch 470/544, Loss: 0.0023177198600023985\n","Batch 480/544, Loss: 0.008624395355582237\n","Batch 490/544, Loss: 0.001680348184891045\n","Batch 500/544, Loss: 0.008707856759428978\n","Batch 510/544, Loss: 0.001620408147573471\n","Batch 520/544, Loss: 0.0006600917549803853\n","Batch 530/544, Loss: 0.0018537347204983234\n","Batch 540/544, Loss: 0.001076792599633336\n","Training epoch completed in: 1m 33s\n","Train loss 0.02802529068225447 accuracy 0.989991947544001\n","Batch 0/136, Test Loss: 0.27056849002838135\n","Batch 10/136, Test Loss: 1.6110851764678955\n","Batch 20/136, Test Loss: 0.9264259934425354\n","Batch 30/136, Test Loss: 0.6365951895713806\n","Batch 40/136, Test Loss: 0.5297232866287231\n","Batch 50/136, Test Loss: 0.4476526081562042\n","Batch 60/136, Test Loss: 0.41581210494041443\n","Batch 70/136, Test Loss: 1.3084025382995605\n","Batch 80/136, Test Loss: 0.43956050276756287\n","Batch 90/136, Test Loss: 0.9621485471725464\n","Batch 100/136, Test Loss: 1.364314079284668\n","Batch 110/136, Test Loss: 0.7959655523300171\n","Batch 120/136, Test Loss: 0.5204342007637024\n","Batch 130/136, Test Loss: 0.15868356823921204\n","Test evaluation completed in: 0m 7s\n","Test loss 0.5191057106250596 accuracy 0.8836246550137994\n","Test Precision: 0.8843107108022319\n","Test Recall: 0.8836246550137995\n","Test F1 Score: 0.8838092826333317\n","Starting epoch 6/10\n","Batch 0/544, Loss: 0.007117732428014278\n","Batch 10/544, Loss: 0.004269404802471399\n","Batch 20/544, Loss: 0.0009529965464025736\n","Batch 30/544, Loss: 0.030015818774700165\n","Batch 40/544, Loss: 0.004064989276230335\n","Batch 50/544, Loss: 0.004637366160750389\n","Batch 60/544, Loss: 0.001802609651349485\n","Batch 70/544, Loss: 0.0008243533666245639\n","Batch 80/544, Loss: 0.0013240501284599304\n","Batch 90/544, Loss: 0.0037749679759144783\n","Batch 100/544, Loss: 0.17185531556606293\n","Batch 110/544, Loss: 0.0004951117443852127\n","Batch 120/544, Loss: 0.008724112994968891\n","Batch 130/544, Loss: 0.0007581915124319494\n","Batch 140/544, Loss: 0.012166665866971016\n","Batch 150/544, Loss: 0.02553768828511238\n","Batch 160/544, Loss: 0.01719602197408676\n","Batch 170/544, Loss: 0.0004752070817630738\n","Batch 180/544, Loss: 0.0003807317989412695\n","Batch 190/544, Loss: 0.0005322223878465593\n","Batch 200/544, Loss: 0.0007887011743150651\n","Batch 210/544, Loss: 0.1731717884540558\n","Batch 220/544, Loss: 0.00034335823147557676\n","Batch 230/544, Loss: 0.0032325908541679382\n","Batch 240/544, Loss: 0.007439705077558756\n","Batch 250/544, Loss: 0.0003481922030914575\n","Batch 260/544, Loss: 0.011512193828821182\n","Batch 270/544, Loss: 0.00022625798010267317\n","Batch 280/544, Loss: 0.0008737638127058744\n","Batch 290/544, Loss: 0.001947013079188764\n","Batch 300/544, Loss: 0.0019868873059749603\n","Batch 310/544, Loss: 0.019521920010447502\n","Batch 320/544, Loss: 0.017644723877310753\n","Batch 330/544, Loss: 0.0007664657314307988\n","Batch 340/544, Loss: 0.001660540234297514\n","Batch 350/544, Loss: 0.0010032624704763293\n","Batch 360/544, Loss: 0.000580448831897229\n","Batch 370/544, Loss: 0.0007922142976894975\n","Batch 380/544, Loss: 0.0013868347741663456\n","Batch 390/544, Loss: 0.05146243795752525\n","Batch 400/544, Loss: 0.0008607201161794364\n","Batch 410/544, Loss: 0.09936266392469406\n","Batch 420/544, Loss: 0.0012468972709029913\n","Batch 430/544, Loss: 0.029246099293231964\n","Batch 440/544, Loss: 0.03865603357553482\n","Batch 450/544, Loss: 0.10756494104862213\n","Batch 460/544, Loss: 0.002212420105934143\n","Batch 470/544, Loss: 0.04699448496103287\n","Batch 480/544, Loss: 0.007349411491304636\n","Batch 490/544, Loss: 0.0019126704428344965\n","Batch 500/544, Loss: 0.0051963673904538155\n","Batch 510/544, Loss: 0.0007973922183737159\n","Batch 520/544, Loss: 0.0006428154301829636\n","Batch 530/544, Loss: 0.00679293368011713\n","Batch 540/544, Loss: 0.0005534149240702391\n","Training epoch completed in: 1m 33s\n","Train loss 0.0170164719506829 accuracy 0.993327965029334\n","Batch 0/136, Test Loss: 0.22003404796123505\n","Batch 10/136, Test Loss: 1.6149842739105225\n","Batch 20/136, Test Loss: 0.6760337352752686\n","Batch 30/136, Test Loss: 0.5731015801429749\n","Batch 40/136, Test Loss: 0.48625800013542175\n","Batch 50/136, Test Loss: 0.4811949133872986\n","Batch 60/136, Test Loss: 0.3859741985797882\n","Batch 70/136, Test Loss: 1.3286176919937134\n","Batch 80/136, Test Loss: 0.3796960711479187\n","Batch 90/136, Test Loss: 1.0048800706863403\n","Batch 100/136, Test Loss: 1.3570228815078735\n","Batch 110/136, Test Loss: 0.673466682434082\n","Batch 120/136, Test Loss: 0.3256550133228302\n","Batch 130/136, Test Loss: 0.10716672986745834\n","Test evaluation completed in: 0m 8s\n","Test loss 0.5173791593872866 accuracy 0.8850045998160073\n","Test Precision: 0.8857885315052164\n","Test Recall: 0.8850045998160073\n","Test F1 Score: 0.8852034778454464\n","Starting epoch 7/10\n","Batch 0/544, Loss: 0.0074081276543438435\n","Batch 10/544, Loss: 0.0024719135835766792\n","Batch 20/544, Loss: 0.001917384215630591\n","Batch 30/544, Loss: 0.10395481437444687\n","Batch 40/544, Loss: 0.0016631251201033592\n","Batch 50/544, Loss: 0.0032240478321909904\n","Batch 60/544, Loss: 0.01846628449857235\n","Batch 70/544, Loss: 0.0005917565431445837\n","Batch 80/544, Loss: 0.0024684739764779806\n","Batch 90/544, Loss: 0.0008315811865031719\n","Batch 100/544, Loss: 0.004108494613319635\n","Batch 110/544, Loss: 0.00043566536623984575\n","Batch 120/544, Loss: 0.00841798447072506\n","Batch 130/544, Loss: 0.00042981424485333264\n","Batch 140/544, Loss: 0.016466349363327026\n","Batch 150/544, Loss: 0.0023927015718072653\n","Batch 160/544, Loss: 0.0007829532842151821\n","Batch 170/544, Loss: 0.0016495794989168644\n","Batch 180/544, Loss: 0.0003054497065022588\n","Batch 190/544, Loss: 0.0012286260025575757\n","Batch 200/544, Loss: 0.0012271772138774395\n","Batch 210/544, Loss: 0.03149064630270004\n","Batch 220/544, Loss: 0.00041357867303304374\n","Batch 230/544, Loss: 0.0017630651127547026\n","Batch 240/544, Loss: 0.001359648653306067\n","Batch 250/544, Loss: 0.00040115247247740626\n","Batch 260/544, Loss: 0.03593355044722557\n","Batch 270/544, Loss: 0.0003321194089949131\n","Batch 280/544, Loss: 0.0004728376225102693\n","Batch 290/544, Loss: 0.00057026935974136\n","Batch 300/544, Loss: 0.00249626231379807\n","Batch 310/544, Loss: 0.004207722842693329\n","Batch 320/544, Loss: 0.00016722455620765686\n","Batch 330/544, Loss: 0.0006483150064013898\n","Batch 340/544, Loss: 0.0021118337754160166\n","Batch 350/544, Loss: 0.007325627841055393\n","Batch 360/544, Loss: 0.0012005941243842244\n","Batch 370/544, Loss: 0.0009179523913189769\n","Batch 380/544, Loss: 0.0004207729652989656\n","Batch 390/544, Loss: 0.0014754852745682001\n","Batch 400/544, Loss: 0.0013459138572216034\n","Batch 410/544, Loss: 0.03892461583018303\n","Batch 420/544, Loss: 0.00021150872635189444\n","Batch 430/544, Loss: 0.010119094513356686\n","Batch 440/544, Loss: 0.00046618067426607013\n","Batch 450/544, Loss: 0.002004466950893402\n","Batch 460/544, Loss: 0.0017951943445950747\n","Batch 470/544, Loss: 0.25310271978378296\n","Batch 480/544, Loss: 0.00047406021622009575\n","Batch 490/544, Loss: 0.0013027401873841882\n","Batch 500/544, Loss: 0.0010119415819644928\n","Batch 510/544, Loss: 0.0023271695245057344\n","Batch 520/544, Loss: 0.0002495093795005232\n","Batch 530/544, Loss: 0.003280123695731163\n","Batch 540/544, Loss: 0.0012174320872873068\n","Training epoch completed in: 1m 33s\n","Train loss 0.012802844006273517 accuracy 0.9948234211434488\n","Batch 0/136, Test Loss: 0.303496778011322\n","Batch 10/136, Test Loss: 1.704576849937439\n","Batch 20/136, Test Loss: 0.806456446647644\n","Batch 30/136, Test Loss: 0.8890151977539062\n","Batch 40/136, Test Loss: 0.429602712392807\n","Batch 50/136, Test Loss: 0.590869665145874\n","Batch 60/136, Test Loss: 0.4670342206954956\n","Batch 70/136, Test Loss: 1.54478120803833\n","Batch 80/136, Test Loss: 0.3776451051235199\n","Batch 90/136, Test Loss: 1.0169987678527832\n","Batch 100/136, Test Loss: 1.4797545671463013\n","Batch 110/136, Test Loss: 0.7683426737785339\n","Batch 120/136, Test Loss: 0.5189194679260254\n","Batch 130/136, Test Loss: 0.20666790008544922\n","Test evaluation completed in: 0m 7s\n","Test loss 0.6070837435103492 accuracy 0.8854645814167433\n","Test Precision: 0.8876380536545262\n","Test Recall: 0.8854645814167433\n","Test F1 Score: 0.8858055111427436\n","Starting epoch 8/10\n","Batch 0/544, Loss: 0.0025608972646296024\n","Batch 10/544, Loss: 0.0006543203489854932\n","Batch 20/544, Loss: 0.0011648830259218812\n","Batch 30/544, Loss: 0.05338664725422859\n","Batch 40/544, Loss: 0.0004012309364043176\n","Batch 50/544, Loss: 0.002617057878524065\n","Batch 60/544, Loss: 0.00028137912158854306\n","Batch 70/544, Loss: 0.00018819552497006953\n","Batch 80/544, Loss: 0.00035090127494186163\n","Batch 90/544, Loss: 0.0006136049050837755\n","Batch 100/544, Loss: 0.0006116317235864699\n","Batch 110/544, Loss: 0.00033769963192753494\n","Batch 120/544, Loss: 0.000643561128526926\n","Batch 130/544, Loss: 0.00022763153538107872\n","Batch 140/544, Loss: 0.008229799568653107\n","Batch 150/544, Loss: 0.0012972744880244136\n","Batch 160/544, Loss: 0.0014129322953522205\n","Batch 170/544, Loss: 0.0006435692775994539\n","Batch 180/544, Loss: 0.0001894173474283889\n","Batch 190/544, Loss: 0.0002917605743277818\n","Batch 200/544, Loss: 0.0005120204295963049\n","Batch 210/544, Loss: 0.04907916858792305\n","Batch 220/544, Loss: 0.0001719916908768937\n","Batch 230/544, Loss: 0.0004673526273109019\n","Batch 240/544, Loss: 0.0003293401678092778\n","Batch 250/544, Loss: 0.00015631632413715124\n","Batch 260/544, Loss: 0.0026846344117075205\n","Batch 270/544, Loss: 0.00011330863344483078\n","Batch 280/544, Loss: 0.00021032500080764294\n","Batch 290/544, Loss: 0.00031565531389787793\n","Batch 300/544, Loss: 0.07623997330665588\n","Batch 310/544, Loss: 0.003198300488293171\n","Batch 320/544, Loss: 0.00018467160407453775\n","Batch 330/544, Loss: 0.0003988494863733649\n","Batch 340/544, Loss: 0.0023755670990794897\n","Batch 350/544, Loss: 0.00029972128686495125\n","Batch 360/544, Loss: 0.00045398558722808957\n","Batch 370/544, Loss: 0.0004860617336817086\n","Batch 380/544, Loss: 0.00027209013933315873\n","Batch 390/544, Loss: 0.0007799509912729263\n","Batch 400/544, Loss: 0.0003242876846343279\n","Batch 410/544, Loss: 0.02583792246878147\n","Batch 420/544, Loss: 0.0001546702696941793\n","Batch 430/544, Loss: 0.004655252210795879\n","Batch 440/544, Loss: 0.00024607189698144794\n","Batch 450/544, Loss: 0.000342171493684873\n","Batch 460/544, Loss: 0.00047202868154272437\n","Batch 470/544, Loss: 0.009585147723555565\n","Batch 480/544, Loss: 0.0006710334564559162\n","Batch 490/544, Loss: 0.0004785620840266347\n","Batch 500/544, Loss: 0.0038406476378440857\n","Batch 510/544, Loss: 0.00019911720301024616\n","Batch 520/544, Loss: 0.00019961500947829336\n","Batch 530/544, Loss: 0.0007091101724654436\n","Batch 540/544, Loss: 0.0001430720294592902\n","Training epoch completed in: 1m 33s\n","Train loss 0.008547749108696307 accuracy 0.9949384562291499\n","Batch 0/136, Test Loss: 0.2758329212665558\n","Batch 10/136, Test Loss: 1.8393125534057617\n","Batch 20/136, Test Loss: 1.028740644454956\n","Batch 30/136, Test Loss: 0.6228789687156677\n","Batch 40/136, Test Loss: 0.734515905380249\n","Batch 50/136, Test Loss: 0.6366626024246216\n","Batch 60/136, Test Loss: 0.5230655670166016\n","Batch 70/136, Test Loss: 1.490718126296997\n","Batch 80/136, Test Loss: 0.44267401099205017\n","Batch 90/136, Test Loss: 1.1015690565109253\n","Batch 100/136, Test Loss: 1.2976847887039185\n","Batch 110/136, Test Loss: 0.7662050724029541\n","Batch 120/136, Test Loss: 0.6482088565826416\n","Batch 130/136, Test Loss: 0.06580905616283417\n","Test evaluation completed in: 0m 7s\n","Test loss 0.6337911431371194 accuracy 0.8859245630174792\n","Test Precision: 0.8858979840643906\n","Test Recall: 0.8859245630174793\n","Test F1 Score: 0.8859105138343728\n","Starting epoch 9/10\n","Batch 0/544, Loss: 0.0007136622443795204\n","Batch 10/544, Loss: 0.0002229603851446882\n","Batch 20/544, Loss: 0.00021331880998332053\n","Batch 30/544, Loss: 0.08034629374742508\n","Batch 40/544, Loss: 0.000522395595908165\n","Batch 50/544, Loss: 0.0008487874292768538\n","Batch 60/544, Loss: 0.00023824538220651448\n","Batch 70/544, Loss: 0.00021240991191007197\n","Batch 80/544, Loss: 0.0003876584523823112\n","Batch 90/544, Loss: 0.00027949202922172844\n","Batch 100/544, Loss: 0.00027883212896995246\n","Batch 110/544, Loss: 0.0002492633066140115\n","Batch 120/544, Loss: 0.000495065760333091\n","Batch 130/544, Loss: 0.00041603395948186517\n","Batch 140/544, Loss: 0.00047632341738790274\n","Batch 150/544, Loss: 0.0009301023674197495\n","Batch 160/544, Loss: 0.0009162059868685901\n","Batch 170/544, Loss: 0.0008842798415571451\n","Batch 180/544, Loss: 0.0001460200292058289\n","Batch 190/544, Loss: 0.00020966041483916342\n","Batch 200/544, Loss: 0.0006272946484386921\n","Batch 210/544, Loss: 0.04789147526025772\n","Batch 220/544, Loss: 0.00020634742395486683\n","Batch 230/544, Loss: 0.0003712901961989701\n","Batch 240/544, Loss: 0.0002170577790820971\n","Batch 250/544, Loss: 0.00015209252887871116\n","Batch 260/544, Loss: 0.0008495846414007246\n","Batch 270/544, Loss: 9.098181180888787e-05\n","Batch 280/544, Loss: 0.00013053025759290904\n","Batch 290/544, Loss: 0.00033234391594305634\n","Batch 300/544, Loss: 0.000574400182813406\n","Batch 310/544, Loss: 0.0047545828856527805\n","Batch 320/544, Loss: 0.00012755482748616487\n","Batch 330/544, Loss: 0.00020590951316989958\n","Batch 340/544, Loss: 0.0009551091934554279\n","Batch 350/544, Loss: 0.0009471435332670808\n","Batch 360/544, Loss: 0.00020909684826619923\n","Batch 370/544, Loss: 0.0003320070682093501\n","Batch 380/544, Loss: 0.00039508414920419455\n","Batch 390/544, Loss: 0.0008002847898751497\n","Batch 400/544, Loss: 0.00041508168214932084\n","Batch 410/544, Loss: 0.029309384524822235\n","Batch 420/544, Loss: 0.00018046385957859457\n","Batch 430/544, Loss: 0.0044939047656953335\n","Batch 440/544, Loss: 0.0002944852167274803\n","Batch 450/544, Loss: 0.00025566702242940664\n","Batch 460/544, Loss: 0.00044099369551986456\n","Batch 470/544, Loss: 0.01777365431189537\n","Batch 480/544, Loss: 0.00035215620300732553\n","Batch 490/544, Loss: 0.00019239389803260565\n","Batch 500/544, Loss: 0.0009920460870489478\n","Batch 510/544, Loss: 0.00016306746692862362\n","Batch 520/544, Loss: 0.00011086100857937708\n","Batch 530/544, Loss: 0.004790083039551973\n","Batch 540/544, Loss: 0.0004110823792871088\n","Training epoch completed in: 1m 33s\n","Train loss 0.008080015095522279 accuracy 0.9955136316576556\n","Batch 0/136, Test Loss: 0.2670469880104065\n","Batch 10/136, Test Loss: 1.8232877254486084\n","Batch 20/136, Test Loss: 1.0311468839645386\n","Batch 30/136, Test Loss: 0.6352769732475281\n","Batch 40/136, Test Loss: 0.6733072400093079\n","Batch 50/136, Test Loss: 0.720305323600769\n","Batch 60/136, Test Loss: 0.5511708855628967\n","Batch 70/136, Test Loss: 1.5069384574890137\n","Batch 80/136, Test Loss: 0.43230536580085754\n","Batch 90/136, Test Loss: 1.1257736682891846\n","Batch 100/136, Test Loss: 1.3090667724609375\n","Batch 110/136, Test Loss: 0.877266526222229\n","Batch 120/136, Test Loss: 0.6993576884269714\n","Batch 130/136, Test Loss: 0.031634025275707245\n","Test evaluation completed in: 0m 7s\n","Test loss 0.6591767464705138 accuracy 0.8840846366145354\n","Test Precision: 0.8839618130855242\n","Test Recall: 0.8840846366145354\n","Test F1 Score: 0.8839958757718038\n","Starting epoch 10/10\n","Batch 0/544, Loss: 0.0006299017113633454\n","Batch 10/544, Loss: 0.00018785089196171612\n","Batch 20/544, Loss: 0.00016621714166831225\n","Batch 30/544, Loss: 0.02425275556743145\n","Batch 40/544, Loss: 0.000534643535502255\n","Batch 50/544, Loss: 0.0013330976944416761\n","Batch 60/544, Loss: 0.00023008929565548897\n","Batch 70/544, Loss: 0.00013142749958205968\n","Batch 80/544, Loss: 0.00018975549028255045\n","Batch 90/544, Loss: 0.0005497466190718114\n","Batch 100/544, Loss: 0.0003575992304831743\n","Batch 110/544, Loss: 0.0001331919338554144\n","Batch 120/544, Loss: 0.0001630535552976653\n","Batch 130/544, Loss: 0.0001652401260798797\n","Batch 140/544, Loss: 0.000804660317953676\n","Batch 150/544, Loss: 0.0006582483765669167\n","Batch 160/544, Loss: 0.0004467667022254318\n","Batch 170/544, Loss: 0.00031076130107976496\n","Batch 180/544, Loss: 0.0001471334253437817\n","Batch 190/544, Loss: 0.000242636539041996\n","Batch 200/544, Loss: 0.0008722429629415274\n","Batch 210/544, Loss: 0.03730206936597824\n","Batch 220/544, Loss: 0.00014416042540688068\n","Batch 230/544, Loss: 0.0004689747584052384\n","Batch 240/544, Loss: 0.00013855850556865335\n","Batch 250/544, Loss: 0.00012008343765046448\n","Batch 260/544, Loss: 0.0010280703427270055\n","Batch 270/544, Loss: 9.343850251752883e-05\n","Batch 280/544, Loss: 0.00010548641148488969\n","Batch 290/544, Loss: 0.00015024833555798978\n","Batch 300/544, Loss: 0.0010751669760793447\n","Batch 310/544, Loss: 0.0024751797318458557\n","Batch 320/544, Loss: 0.00023118544777389616\n","Batch 330/544, Loss: 0.00023412416339851916\n","Batch 340/544, Loss: 0.0011150536593049765\n","Batch 350/544, Loss: 0.0003182935470249504\n","Batch 360/544, Loss: 0.0001486414112150669\n","Batch 370/544, Loss: 0.0003312896587885916\n","Batch 380/544, Loss: 0.001013565924949944\n","Batch 390/544, Loss: 0.0002950059133581817\n","Batch 400/544, Loss: 0.00014294346328824759\n","Batch 410/544, Loss: 0.02913396991789341\n","Batch 420/544, Loss: 0.00012162127677584067\n","Batch 430/544, Loss: 0.0029468752909451723\n","Batch 440/544, Loss: 0.00045782350935041904\n","Batch 450/544, Loss: 0.00010141704115085304\n","Batch 460/544, Loss: 0.0005916861700825393\n","Batch 470/544, Loss: 0.030446074903011322\n","Batch 480/544, Loss: 0.0006201267242431641\n","Batch 490/544, Loss: 0.00031527248211205006\n","Batch 500/544, Loss: 0.0015646812971681356\n","Batch 510/544, Loss: 9.411027713213116e-05\n","Batch 520/544, Loss: 0.0001027512043947354\n","Batch 530/544, Loss: 0.00018389985780231655\n","Batch 540/544, Loss: 0.00013298435078468174\n","Training epoch completed in: 1m 33s\n","Train loss 0.005509620226669487 accuracy 0.9967790176003681\n","Batch 0/136, Test Loss: 0.24063099920749664\n","Batch 10/136, Test Loss: 1.7838153839111328\n","Batch 20/136, Test Loss: 1.035391926765442\n","Batch 30/136, Test Loss: 0.6391708254814148\n","Batch 40/136, Test Loss: 0.6840046644210815\n","Batch 50/136, Test Loss: 0.7893375158309937\n","Batch 60/136, Test Loss: 0.5560790300369263\n","Batch 70/136, Test Loss: 1.5703939199447632\n","Batch 80/136, Test Loss: 0.4254836142063141\n","Batch 90/136, Test Loss: 1.1419384479522705\n","Batch 100/136, Test Loss: 1.3860516548156738\n","Batch 110/136, Test Loss: 0.8819316625595093\n","Batch 120/136, Test Loss: 0.6982922554016113\n","Batch 130/136, Test Loss: 0.0354396216571331\n","Test evaluation completed in: 0m 7s\n","Test loss 0.6783060412141876 accuracy 0.8873045078196872\n","Test Precision: 0.8872910592050895\n","Test Recall: 0.8873045078196872\n","Test F1 Score: 0.887297593317506\n"]}],"source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split\n","import time\n","import traceback\n","\n","class ClickbaitDataset(Dataset):\n","    def __init__(self, titles, labels, tokenizer, max_len):\n","        self.titles = titles\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.titles)\n","\n","    def __getitem__(self, idx):\n","        title = self.titles[idx]\n","        label = self.labels[idx]\n","\n","        # Ensure the label is a valid integer\n","        try:\n","            label = int(label)\n","        except ValueError:\n","            raise ValueError(f\"Label {label} at index {idx} is not a valid integer.\")\n","\n","        encoding = self.tokenizer.encode_plus(\n","            title,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'title_text': title,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","def create_data_loader(titles, labels, tokenizer, max_len, batch_size):\n","    ds = ClickbaitDataset(\n","        titles=titles,\n","        labels=labels,\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","\n","    return DataLoader(ds, batch_size=batch_size, num_workers=4, pin_memory=True)\n","\n","def train_epoch(\n","    model,\n","    data_loader,\n","    loss_fn,\n","    optimizer,\n","    device,\n","    scheduler,\n","    n_examples\n","):\n","    model.train()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    for batch_idx, d in enumerate(data_loader):\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        labels = d[\"labels\"].to(device)\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        _, preds = torch.max(outputs.logits, dim=1)\n","        loss = loss_fn(outputs.logits, labels)\n","\n","        correct_predictions += torch.sum(preds == labels)\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        if batch_idx % 10 == 0:\n","            print(f'Batch {batch_idx}/{len(data_loader)}, Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Training epoch completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model.eval()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    all_labels = []\n","    all_preds = []\n","\n","    with torch.no_grad():\n","        for batch_idx, d in enumerate(data_loader):\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            labels = d[\"labels\"].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            loss = loss_fn(outputs.logits, labels)\n","\n","            correct_predictions += torch.sum(preds == labels)\n","            losses.append(loss.item())\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","\n","            if batch_idx % 10 == 0:\n","                print(f'Batch {batch_idx}/{len(data_loader)}, Test Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Test evaluation completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses), all_labels, all_preds\n","\n","def clean_dataset(df):\n","    # Remove rows where labels are NaN\n","    df = df.dropna(subset=['label'])\n","    # Convert labels to integers, and filter out rows where this conversion fails\n","    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n","    df = df.dropna(subset=['label'])\n","    df['label'] = df['label'].astype(int)\n","    return df\n","\n","def main():\n","    print(\"Starting main process\")\n","\n","    try:\n","        print('Loading the dataset.')\n","        df = pd.read_csv('/content/drive/MyDrive/clickbait/lithuanian.csv')\n","        print('Dataset loaded successfully.')\n","\n","        print('Cleaning the dataset.')\n","        df = clean_dataset(df)\n","        print('Dataset cleaned.')\n","\n","        print('Splitting the dataset into training and test sets.')\n","        df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n","        print('Dataset split into training and test sets.')\n","\n","        RANDOM_SEED = 42\n","        MAX_LEN = 128\n","        BATCH_SIZE = 16\n","        EPOCHS = 10\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f'Using device: {device}')\n","\n","        print('Loading the tokenizer and model.')\n","        tokenizer = BertTokenizer.from_pretrained('dumitrescustefan/bert-base-romanian-cased-v1')\n","        model = BertForSequenceClassification.from_pretrained('dumitrescustefan/bert-base-romanian-cased-v1', num_labels=2)\n","        model = model.to(device)\n","        print('Model and tokenizer loaded and moved to device.')\n","\n","        print('Creating data loaders.')\n","        train_titles = df_train['title'].tolist()\n","        train_labels = df_train['label'].tolist()\n","        test_titles = df_test['title'].tolist()\n","        test_labels = df_test['label'].tolist()\n","\n","        train_data_loader = create_data_loader(train_titles, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        test_data_loader = create_data_loader(test_titles, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        print('Data loaders created.')\n","\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","        total_steps = len(train_data_loader) * EPOCHS\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","        loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","        print('Optimizer, scheduler, and loss function defined.')\n","\n","        for epoch in range(EPOCHS):\n","            print(f'Starting epoch {epoch + 1}/{EPOCHS}')\n","\n","            train_acc, train_loss = train_epoch(\n","                model,\n","                train_data_loader,\n","                loss_fn,\n","                optimizer,\n","                device,\n","                scheduler,\n","                len(df_train)\n","            )\n","            print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","            test_acc, test_loss, all_labels, all_preds = eval_model(\n","                model,\n","                test_data_loader,\n","                loss_fn,\n","                device,\n","                len(df_test)\n","            )\n","            print(f'Test loss {test_loss} accuracy {test_acc}')\n","\n","            # Calculate additional metrics\n","            precision = precision_score(all_labels, all_preds, average='weighted')\n","            recall = recall_score(all_labels, all_preds, average='weighted')\n","            f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","            print(f'Test Precision: {precision}')\n","            print(f'Test Recall: {recall}')\n","            print(f'Test F1 Score: {f1}')\n","\n","    except Exception as e:\n","        print(f'An error occurred: {e}')\n","        traceback.print_exc()\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17969,"status":"ok","timestamp":1722721513450,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"},"user_tz":-180},"id":"Ic7ZMvG1IbEx","outputId":"7d00f71d-f411-47cd-a8fd-d23cfbcf2656"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1ZDoY_cCvGd99UeEdfZDeQTAR73hRmiF8","authorship_tag":"ABX9TyNX8DVK9CW3/Rjtp3jAhZeH"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"91f6333ecd874ddfa3e2b0e039737684":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2a50843ae1841a7a411f3eea046236a","IPY_MODEL_083ee4e531ba45d5af31375f0aee6a23","IPY_MODEL_f5b56b80974e4e29bb40cd0a39161c82"],"layout":"IPY_MODEL_79e401238a904a80a948b32fc8b99efb"}},"c2a50843ae1841a7a411f3eea046236a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_672661d3462a455d926cd08e44a5bf4e","placeholder":"​","style":"IPY_MODEL_8593e9393fe34f5d97efaae477bb6751","value":"tokenizer_config.json: 100%"}},"083ee4e531ba45d5af31375f0aee6a23":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57f265ce2fc349bd93f6d302dada4fd0","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_64b1a2dae9ec4135b2f29f93f9dac5fd","value":29}},"f5b56b80974e4e29bb40cd0a39161c82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81069a2af9a444bfb7500a6b6a0dfec9","placeholder":"​","style":"IPY_MODEL_fb6065d565cc40c9b0f8a4cd0cf4d19d","value":" 29.0/29.0 [00:00&lt;00:00, 2.27kB/s]"}},"79e401238a904a80a948b32fc8b99efb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"672661d3462a455d926cd08e44a5bf4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8593e9393fe34f5d97efaae477bb6751":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57f265ce2fc349bd93f6d302dada4fd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64b1a2dae9ec4135b2f29f93f9dac5fd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"81069a2af9a444bfb7500a6b6a0dfec9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb6065d565cc40c9b0f8a4cd0cf4d19d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be9372aa4a6241bfa59f3bad889fb778":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bdc0602d98c14fe7a1d28cf55137eee7","IPY_MODEL_21ae599f7815447f9deac5541aec769e","IPY_MODEL_3480f20a225e4fd5b3c87b2e726bb411"],"layout":"IPY_MODEL_41a1f63a17414a66bd8389aee7cc20ba"}},"bdc0602d98c14fe7a1d28cf55137eee7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b39288d717c143cca285c133e7f83c7b","placeholder":"​","style":"IPY_MODEL_82417b2a2b19438d880f42dc59375b86","value":"vocab.txt: 100%"}},"21ae599f7815447f9deac5541aec769e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc9ad4e26b904cb89f6116bcb5dffda2","max":397396,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7470d9c2c33c4541aee2b2e631bf84a1","value":397396}},"3480f20a225e4fd5b3c87b2e726bb411":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78740c6ab8cd4649b478a2ed29345bda","placeholder":"​","style":"IPY_MODEL_f0eca848c94c40a5b272512b681c3a3c","value":" 397k/397k [00:00&lt;00:00, 1.88MB/s]"}},"41a1f63a17414a66bd8389aee7cc20ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b39288d717c143cca285c133e7f83c7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82417b2a2b19438d880f42dc59375b86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc9ad4e26b904cb89f6116bcb5dffda2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7470d9c2c33c4541aee2b2e631bf84a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78740c6ab8cd4649b478a2ed29345bda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0eca848c94c40a5b272512b681c3a3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36645a6856434a47ac7b82874aac8dc0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86fc3cd63ce443558380aced28839a0f","IPY_MODEL_d6c0312cfe59498498f1fe84ca2614cd","IPY_MODEL_2417fe58bd904ed18ce0e2722ec5a56c"],"layout":"IPY_MODEL_b9297fec5e9140178535e8c2dbadc477"}},"86fc3cd63ce443558380aced28839a0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dc79bac87204ccf89b9c8c1440c0aa3","placeholder":"​","style":"IPY_MODEL_c1d89d3dc2854d009c0f7e1530daa04d","value":"config.json: 100%"}},"d6c0312cfe59498498f1fe84ca2614cd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_192799d22be940ae87ba8455764bbac4","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aba2a197938b4c5ea11109637230695e","value":385}},"2417fe58bd904ed18ce0e2722ec5a56c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bedbdb25177477782044be30ad227f8","placeholder":"​","style":"IPY_MODEL_471c79730e9a419f805aeaa24620b8b1","value":" 385/385 [00:00&lt;00:00, 36.6kB/s]"}},"b9297fec5e9140178535e8c2dbadc477":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dc79bac87204ccf89b9c8c1440c0aa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1d89d3dc2854d009c0f7e1530daa04d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"192799d22be940ae87ba8455764bbac4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aba2a197938b4c5ea11109637230695e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4bedbdb25177477782044be30ad227f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"471c79730e9a419f805aeaa24620b8b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2200377bdb4492aaa25df3cf5b013fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_070e2584fafe4cfba48412aeaac26a2d","IPY_MODEL_d55a9788f2644a1fa3e4547db45a295a","IPY_MODEL_f1780afbe5f148bb98cb0887a348e7ce"],"layout":"IPY_MODEL_3dafde9f4dfd4750ae7a366990a71da9"}},"070e2584fafe4cfba48412aeaac26a2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5059b8da02043f08b92a94f6fcd614c","placeholder":"​","style":"IPY_MODEL_fbe5d6afc98941c1860ce11c4f243e4c","value":"pytorch_model.bin: 100%"}},"d55a9788f2644a1fa3e4547db45a295a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f83c089a9474164804e465fa2257be1","max":500386589,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2c6f07c0fda548f1a77d853666d3f202","value":500386589}},"f1780afbe5f148bb98cb0887a348e7ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f2f39954de14169bebc4edab8e799dc","placeholder":"​","style":"IPY_MODEL_c3c51cd07a3546049a42a4c06c873395","value":" 500M/500M [00:57&lt;00:00, 9.63MB/s]"}},"3dafde9f4dfd4750ae7a366990a71da9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5059b8da02043f08b92a94f6fcd614c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbe5d6afc98941c1860ce11c4f243e4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f83c089a9474164804e465fa2257be1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c6f07c0fda548f1a77d853666d3f202":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f2f39954de14169bebc4edab8e799dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3c51cd07a3546049a42a4c06c873395":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
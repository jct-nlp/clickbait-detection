{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyORXJg4q2Ce7DRvkMfXYx2O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ZI_QNAobrz8A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721763788046,"user_tz":-180,"elapsed":112094,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"}},"outputId":"9934a47b-0bda-40e1-92e0-dd0ee61a553d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["pip install openpyxl\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"toLcRrP4s6Z1","executionInfo":{"status":"ok","timestamp":1721763798505,"user_tz":-180,"elapsed":6380,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"}},"outputId":"2a67a26c-cd83-4d66-86a8-277e63bc4689"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import time\n","import traceback\n","\n","class ClickbaitDataset(Dataset):\n","    def __init__(self, titles, labels, tokenizer, max_len):\n","        self.titles = titles\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.titles)\n","\n","    def __getitem__(self, idx):\n","        title = self.titles[idx]\n","        label = self.labels[idx]\n","\n","        # Ensure the label is a valid integer\n","        try:\n","            label = int(label)\n","        except ValueError:\n","            raise ValueError(f\"Label {label} at index {idx} is not a valid integer.\")\n","\n","        encoding = self.tokenizer.encode_plus(\n","            title,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'title_text': title,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","def create_data_loader(titles, labels, tokenizer, max_len, batch_size):\n","    ds = ClickbaitDataset(\n","        titles=titles,\n","        labels=labels,\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","\n","    return DataLoader(ds, batch_size=batch_size, num_workers=4, pin_memory=True)\n","\n","def train_epoch(\n","    model,\n","    data_loader,\n","    loss_fn,\n","    optimizer,\n","    device,\n","    scheduler,\n","    n_examples\n","):\n","    model.train()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    for batch_idx, d in enumerate(data_loader):\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        labels = d[\"labels\"].to(device)\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        _, preds = torch.max(outputs.logits, dim=1)\n","        loss = loss_fn(outputs.logits, labels)\n","\n","        correct_predictions += torch.sum(preds == labels)\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        if batch_idx % 10 == 0:\n","            print(f'Batch {batch_idx}/{len(data_loader)}, Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Training epoch completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model.eval()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    all_labels = []\n","    all_preds = []\n","\n","    with torch.no_grad():\n","        for batch_idx, d in enumerate(data_loader):\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            labels = d[\"labels\"].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            loss = loss_fn(outputs.logits, labels)\n","\n","            correct_predictions += torch.sum(preds == labels)\n","            losses.append(loss.item())\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","\n","            if batch_idx % 10 == 0:\n","                print(f'Batch {batch_idx}/{len(data_loader)}, test Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'test completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses), all_labels, all_preds\n","\n","def clean_dataset(df):\n","    # Remove rows where labels are NaN\n","    df = df.dropna(subset=['label'])\n","    # Convert labels to integers, and filter out rows where this conversion fails\n","    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n","    df = df.dropna(subset=['label'])\n","    df['label'] = df['label'].astype(int)\n","    return df\n","\n","def main():\n","    print(\"Starting main process\")\n","\n","    try:\n","        print('Loading the dataset.')\n","        df = pd.read_excel('/content/drive/MyDrive/clickbait/arabic.xlsx')  # Use pd.read_excel for XLSX files\n","        print('Dataset loaded successfully.')\n","\n","        print('Cleaning the dataset.')\n","        df = clean_dataset(df)\n","        print('Dataset cleaned.')\n","\n","        print('Splitting the dataset into training and test sets.')\n","        df_train, df_test = random_split(df, [int(0.8 * len(df)), len(df) - int(0.8 * len(df))])\n","        print('Dataset split into training and test sets.')\n","\n","        RANDOM_SEED = 42\n","        MAX_LEN = 128\n","        BATCH_SIZE = 16\n","        EPOCHS = 10\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f'Using device: {device}')\n","\n","        print('Loading the tokenizer and model.')\n","        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","        model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n","        model = model.to(device)\n","        print('Model and tokenizer loaded and moved to device.')\n","\n","        print('Creating data loaders.')\n","        train_titles = [df.iloc[i].title for i in df_train.indices]\n","        train_labels = [df.iloc[i].label for i in df_train.indices]\n","        test_titles = [df.iloc[i].title for i in df_test.indices]\n","        test_labels = [df.iloc[i].label for i in df_test.indices]\n","\n","        train_data_loader = create_data_loader(train_titles, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        test_data_loader = create_data_loader(test_titles, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        print('Data loaders created.')\n","\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","        total_steps = len(train_data_loader) * EPOCHS\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","        loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","        print('Optimizer, scheduler, and loss function defined.')\n","\n","        for epoch in range(EPOCHS):\n","            print(f'Starting epoch {epoch + 1}/{EPOCHS}')\n","\n","            train_acc, train_loss = train_epoch(\n","                model,\n","                train_data_loader,\n","                loss_fn,\n","                optimizer,\n","                device,\n","                scheduler,\n","                len(df_train)\n","            )\n","            print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","            test_acc, test_loss, all_labels, all_preds = eval_model(\n","                model,\n","                test_data_loader,\n","                loss_fn,\n","                device,\n","                len(df_test)\n","            )\n","            print(f'test loss {test_loss} accuracy {test_acc}')\n","\n","            # Calculate additional metrics\n","            precision = precision_score(all_labels, all_preds, average='weighted')\n","            recall = recall_score(all_labels, all_preds, average='weighted')\n","            f1 = f1_score(all_labels, all_preds, average='weighted')\n","            accuracy = accuracy_score(all_labels, all_preds)\n","\n","            print(f'Test Precision: {precision}')\n","            print(f'Test Recall: {recall}')\n","            print(f'Test F1 Score: {f1}')\n","            print(f'Test Accuracy: {accuracy}')\n","\n","    except Exception as e:\n","        print(f'An error occurred: {e}')\n","        traceback.print_exc()\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SleSiTrOtX0F","executionInfo":{"status":"ok","timestamp":1721766909285,"user_tz":-180,"elapsed":67557,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"}},"outputId":"8b0b101a-28d8-4305-e0fc-7a66f8217982"},"execution_count":7,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Starting main process\n","Loading the dataset.\n","Dataset loaded successfully.\n","Cleaning the dataset.\n","Dataset cleaned.\n","Splitting the dataset into training and test sets.\n","Dataset split into training and test sets.\n","Using device: cuda\n","Loading the tokenizer and model.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model and tokenizer loaded and moved to device.\n","Creating data loaders.\n","Data loaders created.\n","Optimizer, scheduler, and loss function defined.\n","Starting epoch 1/10\n","Batch 0/774, Loss: 0.7239124774932861\n","Batch 10/774, Loss: 0.7020589113235474\n","Batch 20/774, Loss: 0.715133547782898\n","Batch 30/774, Loss: 0.7172030806541443\n","Batch 40/774, Loss: 0.6756219267845154\n","Batch 50/774, Loss: 0.6937083601951599\n","Batch 60/774, Loss: 0.643505871295929\n","Batch 70/774, Loss: 0.6289806962013245\n","Batch 80/774, Loss: 0.5488048791885376\n","Batch 90/774, Loss: 0.8170941472053528\n","Batch 100/774, Loss: 0.6192912459373474\n","Batch 110/774, Loss: 0.5544247031211853\n","Batch 120/774, Loss: 0.550691545009613\n","Batch 130/774, Loss: 0.32311686873435974\n","Batch 140/774, Loss: 0.41098347306251526\n","Batch 150/774, Loss: 0.49459147453308105\n","Batch 160/774, Loss: 0.18418629467487335\n","Batch 170/774, Loss: 0.6211909055709839\n","Batch 180/774, Loss: 0.4202614724636078\n","Batch 190/774, Loss: 0.46695902943611145\n","Batch 200/774, Loss: 0.6642058491706848\n","Batch 210/774, Loss: 0.2119673788547516\n","Batch 220/774, Loss: 0.2978864908218384\n","Batch 230/774, Loss: 0.27589496970176697\n","Batch 240/774, Loss: 0.5117939710617065\n","Batch 250/774, Loss: 0.2656145989894867\n","Batch 260/774, Loss: 0.37660685181617737\n","Batch 270/774, Loss: 0.2581753730773926\n","Batch 280/774, Loss: 0.16189329326152802\n","Batch 290/774, Loss: 0.38469427824020386\n","Batch 300/774, Loss: 0.20745962858200073\n","Batch 310/774, Loss: 0.39316150546073914\n","Batch 320/774, Loss: 0.4680434465408325\n","Batch 330/774, Loss: 0.6032187938690186\n","Batch 340/774, Loss: 0.25586843490600586\n","Batch 350/774, Loss: 0.2535863220691681\n","Batch 360/774, Loss: 0.6033435463905334\n","Batch 370/774, Loss: 0.5384183526039124\n","Batch 380/774, Loss: 0.27022576332092285\n","Batch 390/774, Loss: 0.44448086619377136\n","Batch 400/774, Loss: 0.22318978607654572\n","Batch 410/774, Loss: 0.2051779180765152\n","Batch 420/774, Loss: 0.3465399146080017\n","Batch 430/774, Loss: 0.31401965022087097\n","Batch 440/774, Loss: 0.2882310748100281\n","Batch 450/774, Loss: 0.4949939250946045\n","Batch 460/774, Loss: 0.3829168379306793\n","Batch 470/774, Loss: 0.4709267020225525\n","Batch 480/774, Loss: 0.3014385402202606\n","Batch 490/774, Loss: 0.2532203197479248\n","Batch 500/774, Loss: 0.22224029898643494\n","Batch 510/774, Loss: 0.38510414958000183\n","Batch 520/774, Loss: 0.38024869561195374\n","Batch 530/774, Loss: 0.2853242754936218\n","Batch 540/774, Loss: 0.5842521786689758\n","Batch 550/774, Loss: 0.16968129575252533\n","Batch 560/774, Loss: 0.32364991307258606\n","Batch 570/774, Loss: 0.2703787088394165\n","Batch 580/774, Loss: 0.24432559311389923\n","Batch 590/774, Loss: 0.12733130156993866\n","Batch 600/774, Loss: 0.17153489589691162\n","Batch 610/774, Loss: 0.20424282550811768\n","Batch 620/774, Loss: 0.3240053653717041\n","Batch 630/774, Loss: 0.47470492124557495\n","Batch 640/774, Loss: 0.5370901823043823\n","Batch 650/774, Loss: 0.5352131724357605\n","Batch 660/774, Loss: 0.3553689420223236\n","Batch 670/774, Loss: 0.15316341817378998\n","Batch 680/774, Loss: 0.42698392271995544\n","Batch 690/774, Loss: 0.29386961460113525\n","Batch 700/774, Loss: 0.18776406347751617\n","Batch 710/774, Loss: 0.5520426034927368\n","Batch 720/774, Loss: 0.4746917188167572\n","Batch 730/774, Loss: 0.44239702820777893\n","Batch 740/774, Loss: 0.7073979377746582\n","Batch 750/774, Loss: 0.24284373223781586\n","Batch 760/774, Loss: 0.30510884523391724\n","Batch 770/774, Loss: 0.5450223088264465\n","Training epoch completed in: 1m 6s\n","Train loss 0.4089255146104698 accuracy 0.8088389755191081\n","Batch 0/194, test Loss: 0.30282577872276306\n","Batch 10/194, test Loss: 0.42278528213500977\n","Batch 20/194, test Loss: 0.24814237654209137\n","Batch 30/194, test Loss: 0.2088099718093872\n","Batch 40/194, test Loss: 0.3154277503490448\n","Batch 50/194, test Loss: 0.09721372276544571\n","Batch 60/194, test Loss: 0.4103996753692627\n","Batch 70/194, test Loss: 0.2781096398830414\n","Batch 80/194, test Loss: 0.24527888000011444\n","Batch 90/194, test Loss: 0.30406853556632996\n","Batch 100/194, test Loss: 0.17066513001918793\n","Batch 110/194, test Loss: 0.1534392088651657\n","Batch 120/194, test Loss: 0.24268808960914612\n","Batch 130/194, test Loss: 0.2926230728626251\n","Batch 140/194, test Loss: 0.5369612574577332\n","Batch 150/194, test Loss: 0.15701889991760254\n","Batch 160/194, test Loss: 0.26418840885162354\n","Batch 170/194, test Loss: 0.32224810123443604\n","Batch 180/194, test Loss: 0.3719370663166046\n","Batch 190/194, test Loss: 0.40158611536026\n","test completed in: 0m 5s\n","test loss 0.3334737764959483 accuracy 0.860096930533118\n","Test Precision: 0.8669013211199322\n","Test Recall: 0.860096930533118\n","Test F1 Score: 0.8594342816867819\n","Test Accuracy: 0.860096930533118\n","Starting epoch 2/10\n","Batch 0/774, Loss: 0.45949050784111023\n","Batch 10/774, Loss: 0.22253885865211487\n","Batch 20/774, Loss: 0.3022322654724121\n","Batch 30/774, Loss: 0.42300570011138916\n","Batch 40/774, Loss: 0.439130961894989\n","Batch 50/774, Loss: 0.4727473258972168\n","Batch 60/774, Loss: 0.3711755573749542\n","Batch 70/774, Loss: 0.24710452556610107\n","Batch 80/774, Loss: 0.16948942840099335\n","Batch 90/774, Loss: 0.7293807864189148\n","Batch 100/774, Loss: 0.4175158441066742\n","Batch 110/774, Loss: 0.26716676354408264\n","Batch 120/774, Loss: 0.27915507555007935\n","Batch 130/774, Loss: 0.36135172843933105\n","Batch 140/774, Loss: 0.26953187584877014\n","Batch 150/774, Loss: 0.3101888597011566\n","Batch 160/774, Loss: 0.13973677158355713\n","Batch 170/774, Loss: 0.3102312386035919\n","Batch 180/774, Loss: 0.28632140159606934\n","Batch 190/774, Loss: 0.38080647587776184\n","Batch 200/774, Loss: 0.4308176636695862\n","Batch 210/774, Loss: 0.1251552253961563\n","Batch 220/774, Loss: 0.24480098485946655\n","Batch 230/774, Loss: 0.23403696715831757\n","Batch 240/774, Loss: 0.4309849739074707\n","Batch 250/774, Loss: 0.20831631124019623\n","Batch 260/774, Loss: 0.2701531946659088\n","Batch 270/774, Loss: 0.08692825585603714\n","Batch 280/774, Loss: 0.11423590034246445\n","Batch 290/774, Loss: 0.08437977731227875\n","Batch 300/774, Loss: 0.07678107172250748\n","Batch 310/774, Loss: 0.4384050965309143\n","Batch 320/774, Loss: 0.43876171112060547\n","Batch 330/774, Loss: 0.4728095531463623\n","Batch 340/774, Loss: 0.25826728343963623\n","Batch 350/774, Loss: 0.31081950664520264\n","Batch 360/774, Loss: 0.5444555282592773\n","Batch 370/774, Loss: 0.4221414625644684\n","Batch 380/774, Loss: 0.2903347611427307\n","Batch 390/774, Loss: 0.30375251173973083\n","Batch 400/774, Loss: 0.20641304552555084\n","Batch 410/774, Loss: 0.2367120236158371\n","Batch 420/774, Loss: 0.18817229568958282\n","Batch 430/774, Loss: 0.28665587306022644\n","Batch 440/774, Loss: 0.2748866081237793\n","Batch 450/774, Loss: 0.43049877882003784\n","Batch 460/774, Loss: 0.3031199276447296\n","Batch 470/774, Loss: 0.3111971318721771\n","Batch 480/774, Loss: 0.21814215183258057\n","Batch 490/774, Loss: 0.2353745549917221\n","Batch 500/774, Loss: 0.10155022144317627\n","Batch 510/774, Loss: 0.41861873865127563\n","Batch 520/774, Loss: 0.35235217213630676\n","Batch 530/774, Loss: 0.270334392786026\n","Batch 540/774, Loss: 0.563341498374939\n","Batch 550/774, Loss: 0.06276250630617142\n","Batch 560/774, Loss: 0.09303895384073257\n","Batch 570/774, Loss: 0.18670901656150818\n","Batch 580/774, Loss: 0.240498885512352\n","Batch 590/774, Loss: 0.08098657429218292\n","Batch 600/774, Loss: 0.1687910109758377\n","Batch 610/774, Loss: 0.21904869377613068\n","Batch 620/774, Loss: 0.27926087379455566\n","Batch 630/774, Loss: 0.421128511428833\n","Batch 640/774, Loss: 0.4795755445957184\n","Batch 650/774, Loss: 0.5998725891113281\n","Batch 660/774, Loss: 0.29616260528564453\n","Batch 670/774, Loss: 0.13829782605171204\n","Batch 680/774, Loss: 0.4278842508792877\n","Batch 690/774, Loss: 0.189896360039711\n","Batch 700/774, Loss: 0.11672148108482361\n","Batch 710/774, Loss: 0.5809118151664734\n","Batch 720/774, Loss: 0.41321060061454773\n","Batch 730/774, Loss: 0.4235634505748749\n","Batch 740/774, Loss: 0.5480252504348755\n","Batch 750/774, Loss: 0.24792823195457458\n","Batch 760/774, Loss: 0.30649417638778687\n","Batch 770/774, Loss: 0.4601946771144867\n","Training epoch completed in: 1m 6s\n","Train loss 0.3050422057105127 accuracy 0.8751716894239315\n","Batch 0/194, test Loss: 0.25979867577552795\n","Batch 10/194, test Loss: 0.3735063374042511\n","Batch 20/194, test Loss: 0.23836863040924072\n","Batch 30/194, test Loss: 0.1930558681488037\n","Batch 40/194, test Loss: 0.26829516887664795\n","Batch 50/194, test Loss: 0.10798795521259308\n","Batch 60/194, test Loss: 0.441582053899765\n","Batch 70/194, test Loss: 0.2522101104259491\n","Batch 80/194, test Loss: 0.24410471320152283\n","Batch 90/194, test Loss: 0.31730225682258606\n","Batch 100/194, test Loss: 0.18950550258159637\n","Batch 110/194, test Loss: 0.17383348941802979\n","Batch 120/194, test Loss: 0.23990020155906677\n","Batch 130/194, test Loss: 0.39196377992630005\n","Batch 140/194, test Loss: 0.5190768837928772\n","Batch 150/194, test Loss: 0.15953174233436584\n","Batch 160/194, test Loss: 0.24102871119976044\n","Batch 170/194, test Loss: 0.2934618592262268\n","Batch 180/194, test Loss: 0.35116851329803467\n","Batch 190/194, test Loss: 0.41264671087265015\n","test completed in: 0m 5s\n","test loss 0.31863107920153855 accuracy 0.8678513731825526\n","Test Precision: 0.8738958548284609\n","Test Recall: 0.8678513731825525\n","Test F1 Score: 0.867305546340018\n","Test Accuracy: 0.8678513731825525\n","Starting epoch 3/10\n","Batch 0/774, Loss: 0.36107730865478516\n","Batch 10/774, Loss: 0.18380001187324524\n","Batch 20/774, Loss: 0.32564839720726013\n","Batch 30/774, Loss: 0.34274035692214966\n","Batch 40/774, Loss: 0.37778204679489136\n","Batch 50/774, Loss: 0.45005252957344055\n","Batch 60/774, Loss: 0.20957620441913605\n","Batch 70/774, Loss: 0.18640382587909698\n","Batch 80/774, Loss: 0.07644680142402649\n","Batch 90/774, Loss: 0.6376575231552124\n","Batch 100/774, Loss: 0.37203019857406616\n","Batch 110/774, Loss: 0.17133896052837372\n","Batch 120/774, Loss: 0.1569853574037552\n","Batch 130/774, Loss: 0.3408302366733551\n","Batch 140/774, Loss: 0.24671289324760437\n","Batch 150/774, Loss: 0.24811699986457825\n","Batch 160/774, Loss: 0.15932317078113556\n","Batch 170/774, Loss: 0.2640572488307953\n","Batch 180/774, Loss: 0.2978977859020233\n","Batch 190/774, Loss: 0.3657330274581909\n","Batch 200/774, Loss: 0.270955353975296\n","Batch 210/774, Loss: 0.1089477390050888\n","Batch 220/774, Loss: 0.18273241817951202\n","Batch 230/774, Loss: 0.2439483404159546\n","Batch 240/774, Loss: 0.4889351725578308\n","Batch 250/774, Loss: 0.27578866481781006\n","Batch 260/774, Loss: 0.2627110183238983\n","Batch 270/774, Loss: 0.07873086631298065\n","Batch 280/774, Loss: 0.2183457314968109\n","Batch 290/774, Loss: 0.08585656434297562\n","Batch 300/774, Loss: 0.06937866657972336\n","Batch 310/774, Loss: 0.31772786378860474\n","Batch 320/774, Loss: 0.45242616534233093\n","Batch 330/774, Loss: 0.549361526966095\n","Batch 340/774, Loss: 0.2557741105556488\n","Batch 350/774, Loss: 0.31922194361686707\n","Batch 360/774, Loss: 0.188380628824234\n","Batch 370/774, Loss: 0.47254130244255066\n","Batch 380/774, Loss: 0.2325248420238495\n","Batch 390/774, Loss: 0.3046351969242096\n","Batch 400/774, Loss: 0.22656530141830444\n","Batch 410/774, Loss: 0.16675928235054016\n","Batch 420/774, Loss: 0.16496486961841583\n","Batch 430/774, Loss: 0.3022729754447937\n","Batch 440/774, Loss: 0.22678858041763306\n","Batch 450/774, Loss: 0.32075709104537964\n","Batch 460/774, Loss: 0.2527126669883728\n","Batch 470/774, Loss: 0.2847265303134918\n","Batch 480/774, Loss: 0.20228298008441925\n","Batch 490/774, Loss: 0.16764582693576813\n","Batch 500/774, Loss: 0.11107249557971954\n","Batch 510/774, Loss: 0.32490378618240356\n","Batch 520/774, Loss: 0.2741096615791321\n","Batch 530/774, Loss: 0.43714284896850586\n","Batch 540/774, Loss: 0.32587188482284546\n","Batch 550/774, Loss: 0.051665909588336945\n","Batch 560/774, Loss: 0.066260926425457\n","Batch 570/774, Loss: 0.09323963522911072\n","Batch 580/774, Loss: 0.17306973040103912\n","Batch 590/774, Loss: 0.07988189160823822\n","Batch 600/774, Loss: 0.08940674364566803\n","Batch 610/774, Loss: 0.19356194138526917\n","Batch 620/774, Loss: 0.21870344877243042\n","Batch 630/774, Loss: 0.42078638076782227\n","Batch 640/774, Loss: 0.34048697352409363\n","Batch 650/774, Loss: 0.6096967458724976\n","Batch 660/774, Loss: 0.2639857530593872\n","Batch 670/774, Loss: 0.14582769572734833\n","Batch 680/774, Loss: 0.41399145126342773\n","Batch 690/774, Loss: 0.11726364493370056\n","Batch 700/774, Loss: 0.12862971425056458\n","Batch 710/774, Loss: 0.6374667882919312\n","Batch 720/774, Loss: 0.3305093050003052\n","Batch 730/774, Loss: 0.40184566378593445\n","Batch 740/774, Loss: 0.5167124271392822\n","Batch 750/774, Loss: 0.2832740247249603\n","Batch 760/774, Loss: 0.35482949018478394\n","Batch 770/774, Loss: 0.4326379597187042\n","Training epoch completed in: 1m 6s\n","Train loss 0.2719044241529603 accuracy 0.8869677627858125\n","Batch 0/194, test Loss: 0.27312102913856506\n","Batch 10/194, test Loss: 0.3776865601539612\n","Batch 20/194, test Loss: 0.21084417402744293\n","Batch 30/194, test Loss: 0.15237756073474884\n","Batch 40/194, test Loss: 0.35618826746940613\n","Batch 50/194, test Loss: 0.08467487245798111\n","Batch 60/194, test Loss: 0.4777453541755676\n","Batch 70/194, test Loss: 0.26550671458244324\n","Batch 80/194, test Loss: 0.19686004519462585\n","Batch 90/194, test Loss: 0.27380627393722534\n","Batch 100/194, test Loss: 0.09305528551340103\n","Batch 110/194, test Loss: 0.16485947370529175\n","Batch 120/194, test Loss: 0.23727509379386902\n","Batch 130/194, test Loss: 0.48702406883239746\n","Batch 140/194, test Loss: 0.5267797708511353\n","Batch 150/194, test Loss: 0.30922847986221313\n","Batch 160/194, test Loss: 0.2172921746969223\n","Batch 170/194, test Loss: 0.3604888319969177\n","Batch 180/194, test Loss: 0.2765444219112396\n","Batch 190/194, test Loss: 0.5334156155586243\n","test completed in: 0m 5s\n","test loss 0.32048787272621676 accuracy 0.870113085621971\n","Test Precision: 0.8749497862650812\n","Test Recall: 0.8701130856219709\n","Test F1 Score: 0.8696844730517009\n","Test Accuracy: 0.8701130856219709\n","Starting epoch 4/10\n","Batch 0/774, Loss: 0.43524083495140076\n","Batch 10/774, Loss: 0.160544291138649\n","Batch 20/774, Loss: 0.2574344873428345\n","Batch 30/774, Loss: 0.3511910140514374\n","Batch 40/774, Loss: 0.31509655714035034\n","Batch 50/774, Loss: 0.41028308868408203\n","Batch 60/774, Loss: 0.18357349932193756\n","Batch 70/774, Loss: 0.14036095142364502\n","Batch 80/774, Loss: 0.1942770779132843\n","Batch 90/774, Loss: 0.48763608932495117\n","Batch 100/774, Loss: 0.3396882712841034\n","Batch 110/774, Loss: 0.09352286159992218\n","Batch 120/774, Loss: 0.2382204234600067\n","Batch 130/774, Loss: 0.26560860872268677\n","Batch 140/774, Loss: 0.2678796350955963\n","Batch 150/774, Loss: 0.3020190894603729\n","Batch 160/774, Loss: 0.16967548429965973\n","Batch 170/774, Loss: 0.23510253429412842\n","Batch 180/774, Loss: 0.27052345871925354\n","Batch 190/774, Loss: 0.2910785377025604\n","Batch 200/774, Loss: 0.28110840916633606\n","Batch 210/774, Loss: 0.12760750949382782\n","Batch 220/774, Loss: 0.20477868616580963\n","Batch 230/774, Loss: 0.2219591736793518\n","Batch 240/774, Loss: 0.3791186213493347\n","Batch 250/774, Loss: 0.23712113499641418\n","Batch 260/774, Loss: 0.19425134360790253\n","Batch 270/774, Loss: 0.060061708092689514\n","Batch 280/774, Loss: 0.11068042367696762\n","Batch 290/774, Loss: 0.047694332897663116\n","Batch 300/774, Loss: 0.07566829770803452\n","Batch 310/774, Loss: 0.1882762461900711\n","Batch 320/774, Loss: 0.2946259677410126\n","Batch 330/774, Loss: 0.48722803592681885\n","Batch 340/774, Loss: 0.1978723704814911\n","Batch 350/774, Loss: 0.17418064177036285\n","Batch 360/774, Loss: 0.17797909677028656\n","Batch 370/774, Loss: 0.42437633872032166\n","Batch 380/774, Loss: 0.23974230885505676\n","Batch 390/774, Loss: 0.25343161821365356\n","Batch 400/774, Loss: 0.24074465036392212\n","Batch 410/774, Loss: 0.22947965562343597\n","Batch 420/774, Loss: 0.21463419497013092\n","Batch 430/774, Loss: 0.2581491768360138\n","Batch 440/774, Loss: 0.12802916765213013\n","Batch 450/774, Loss: 0.3793024718761444\n","Batch 460/774, Loss: 0.2370358556509018\n","Batch 470/774, Loss: 0.22974400222301483\n","Batch 480/774, Loss: 0.1480022817850113\n","Batch 490/774, Loss: 0.11414134502410889\n","Batch 500/774, Loss: 0.07019495964050293\n","Batch 510/774, Loss: 0.3319840729236603\n","Batch 520/774, Loss: 0.251216858625412\n","Batch 530/774, Loss: 0.18899330496788025\n","Batch 540/774, Loss: 0.5101782083511353\n","Batch 550/774, Loss: 0.054286155849695206\n","Batch 560/774, Loss: 0.08574963361024857\n","Batch 570/774, Loss: 0.10609358549118042\n","Batch 580/774, Loss: 0.17525306344032288\n","Batch 590/774, Loss: 0.08841033279895782\n","Batch 600/774, Loss: 0.08237423747777939\n","Batch 610/774, Loss: 0.1634870171546936\n","Batch 620/774, Loss: 0.19213855266571045\n","Batch 630/774, Loss: 0.2785111665725708\n","Batch 640/774, Loss: 0.35356712341308594\n","Batch 650/774, Loss: 0.5103240013122559\n","Batch 660/774, Loss: 0.31386110186576843\n","Batch 670/774, Loss: 0.11513391882181168\n","Batch 680/774, Loss: 0.39047348499298096\n","Batch 690/774, Loss: 0.15143099427223206\n","Batch 700/774, Loss: 0.08823402971029282\n","Batch 710/774, Loss: 0.5615047216415405\n","Batch 720/774, Loss: 0.2661687731742859\n","Batch 730/774, Loss: 0.344184935092926\n","Batch 740/774, Loss: 0.5650901794433594\n","Batch 750/774, Loss: 0.2482021003961563\n","Batch 760/774, Loss: 0.3374056816101074\n","Batch 770/774, Loss: 0.42128992080688477\n","Training epoch completed in: 1m 6s\n","Train loss 0.2492920491911798 accuracy 0.8944009049042579\n","Batch 0/194, test Loss: 0.22950081527233124\n","Batch 10/194, test Loss: 0.4103423058986664\n","Batch 20/194, test Loss: 0.27953511476516724\n","Batch 30/194, test Loss: 0.1584581583738327\n","Batch 40/194, test Loss: 0.4085771441459656\n","Batch 50/194, test Loss: 0.11585280299186707\n","Batch 60/194, test Loss: 0.5148141980171204\n","Batch 70/194, test Loss: 0.2631130516529083\n","Batch 80/194, test Loss: 0.21539418399333954\n","Batch 90/194, test Loss: 0.2706039547920227\n","Batch 100/194, test Loss: 0.14791028201580048\n","Batch 110/194, test Loss: 0.18051555752754211\n","Batch 120/194, test Loss: 0.24129720032215118\n","Batch 130/194, test Loss: 0.530636191368103\n","Batch 140/194, test Loss: 0.4312182068824768\n","Batch 150/194, test Loss: 0.24296121299266815\n","Batch 160/194, test Loss: 0.21520541608333588\n","Batch 170/194, test Loss: 0.2872852385044098\n","Batch 180/194, test Loss: 0.2810257077217102\n","Batch 190/194, test Loss: 0.46798160672187805\n","test completed in: 0m 5s\n","test loss 0.32048990971111146 accuracy 0.8691437802907916\n","Test Precision: 0.8720490017905755\n","Test Recall: 0.8691437802907916\n","Test F1 Score: 0.8688812307866087\n","Test Accuracy: 0.8691437802907916\n","Starting epoch 5/10\n","Batch 0/774, Loss: 0.3118979036808014\n","Batch 10/774, Loss: 0.15907371044158936\n","Batch 20/774, Loss: 0.16242842376232147\n","Batch 30/774, Loss: 0.380510538816452\n","Batch 40/774, Loss: 0.3157663643360138\n","Batch 50/774, Loss: 0.4183335602283478\n","Batch 60/774, Loss: 0.14421693980693817\n","Batch 70/774, Loss: 0.1470426321029663\n","Batch 80/774, Loss: 0.10425382852554321\n","Batch 90/774, Loss: 0.40396106243133545\n","Batch 100/774, Loss: 0.3741340935230255\n","Batch 110/774, Loss: 0.09839607030153275\n","Batch 120/774, Loss: 0.14963071048259735\n","Batch 130/774, Loss: 0.23410426080226898\n","Batch 140/774, Loss: 0.24721494317054749\n","Batch 150/774, Loss: 0.2083694189786911\n","Batch 160/774, Loss: 0.07092799246311188\n","Batch 170/774, Loss: 0.2798275053501129\n","Batch 180/774, Loss: 0.25434568524360657\n","Batch 190/774, Loss: 0.27952706813812256\n","Batch 200/774, Loss: 0.12427840381860733\n","Batch 210/774, Loss: 0.07884423434734344\n","Batch 220/774, Loss: 0.19912488758563995\n","Batch 230/774, Loss: 0.22086454927921295\n","Batch 240/774, Loss: 0.39081987738609314\n","Batch 250/774, Loss: 0.12334281206130981\n","Batch 260/774, Loss: 0.36991024017333984\n","Batch 270/774, Loss: 0.10355284810066223\n","Batch 280/774, Loss: 0.09146028012037277\n","Batch 290/774, Loss: 0.04483950883150101\n","Batch 300/774, Loss: 0.035187073051929474\n","Batch 310/774, Loss: 0.16323742270469666\n","Batch 320/774, Loss: 0.42083054780960083\n","Batch 330/774, Loss: 0.41196393966674805\n","Batch 340/774, Loss: 0.15421682596206665\n","Batch 350/774, Loss: 0.18621774017810822\n","Batch 360/774, Loss: 0.14993682503700256\n","Batch 370/774, Loss: 0.4553801417350769\n","Batch 380/774, Loss: 0.21906325221061707\n","Batch 390/774, Loss: 0.29303884506225586\n","Batch 400/774, Loss: 0.16776879131793976\n","Batch 410/774, Loss: 0.11680430918931961\n","Batch 420/774, Loss: 0.1561753749847412\n","Batch 430/774, Loss: 0.16619370877742767\n","Batch 440/774, Loss: 0.15179771184921265\n","Batch 450/774, Loss: 0.5061324238777161\n","Batch 460/774, Loss: 0.150354266166687\n","Batch 470/774, Loss: 0.19574198126792908\n","Batch 480/774, Loss: 0.15437135100364685\n","Batch 490/774, Loss: 0.19355271756649017\n","Batch 500/774, Loss: 0.08274739235639572\n","Batch 510/774, Loss: 0.24427421391010284\n","Batch 520/774, Loss: 0.22805184125900269\n","Batch 530/774, Loss: 0.16920900344848633\n","Batch 540/774, Loss: 0.25618472695350647\n","Batch 550/774, Loss: 0.034155745059251785\n","Batch 560/774, Loss: 0.043794896453619\n","Batch 570/774, Loss: 0.07631703466176987\n","Batch 580/774, Loss: 0.16519427299499512\n","Batch 590/774, Loss: 0.09053906053304672\n","Batch 600/774, Loss: 0.0757230594754219\n","Batch 610/774, Loss: 0.09545303881168365\n","Batch 620/774, Loss: 0.14519277215003967\n","Batch 630/774, Loss: 0.3576818108558655\n","Batch 640/774, Loss: 0.27382805943489075\n","Batch 650/774, Loss: 0.428164541721344\n","Batch 660/774, Loss: 0.28794339299201965\n","Batch 670/774, Loss: 0.09349840134382248\n","Batch 680/774, Loss: 0.35630789399147034\n","Batch 690/774, Loss: 0.11629793047904968\n","Batch 700/774, Loss: 0.11231879889965057\n","Batch 710/774, Loss: 0.4843543767929077\n","Batch 720/774, Loss: 0.16401761770248413\n","Batch 730/774, Loss: 0.3712005019187927\n","Batch 740/774, Loss: 0.2814512252807617\n","Batch 750/774, Loss: 0.21140319108963013\n","Batch 760/774, Loss: 0.43761804699897766\n","Batch 770/774, Loss: 0.34180745482444763\n","Training epoch completed in: 1m 6s\n","Train loss 0.2213340887260799 accuracy 0.904823462874687\n","Batch 0/194, test Loss: 0.20040950179100037\n","Batch 10/194, test Loss: 0.4518515169620514\n","Batch 20/194, test Loss: 0.21525679528713226\n","Batch 30/194, test Loss: 0.14083141088485718\n","Batch 40/194, test Loss: 0.36071789264678955\n","Batch 50/194, test Loss: 0.18943196535110474\n","Batch 60/194, test Loss: 0.5233190655708313\n","Batch 70/194, test Loss: 0.3498760759830475\n","Batch 80/194, test Loss: 0.2567152976989746\n","Batch 90/194, test Loss: 0.225801482796669\n","Batch 100/194, test Loss: 0.13340987265110016\n","Batch 110/194, test Loss: 0.18951599299907684\n","Batch 120/194, test Loss: 0.2839151620864868\n","Batch 130/194, test Loss: 0.4414547085762024\n","Batch 140/194, test Loss: 0.44175443053245544\n","Batch 150/194, test Loss: 0.19516821205615997\n","Batch 160/194, test Loss: 0.3094591200351715\n","Batch 170/194, test Loss: 0.2754240334033966\n","Batch 180/194, test Loss: 0.2453654706478119\n","Batch 190/194, test Loss: 0.49710190296173096\n","test completed in: 0m 5s\n","test loss 0.33496155303701297 accuracy 0.8542810985460421\n","Test Precision: 0.8544082034650438\n","Test Recall: 0.854281098546042\n","Test F1 Score: 0.8542696276702474\n","Test Accuracy: 0.854281098546042\n","Starting epoch 6/10\n","Batch 0/774, Loss: 0.3188484311103821\n","Batch 10/774, Loss: 0.14239436388015747\n","Batch 20/774, Loss: 0.1525040715932846\n","Batch 30/774, Loss: 0.2710166275501251\n","Batch 40/774, Loss: 0.2761532962322235\n","Batch 50/774, Loss: 0.3909165561199188\n","Batch 60/774, Loss: 0.12247993797063828\n","Batch 70/774, Loss: 0.13382117450237274\n","Batch 80/774, Loss: 0.08939240872859955\n","Batch 90/774, Loss: 0.3482236862182617\n","Batch 100/774, Loss: 0.3416929244995117\n","Batch 110/774, Loss: 0.0874139666557312\n","Batch 120/774, Loss: 0.2102639377117157\n","Batch 130/774, Loss: 0.23921647667884827\n","Batch 140/774, Loss: 0.14674998819828033\n","Batch 150/774, Loss: 0.2853234112262726\n","Batch 160/774, Loss: 0.09160938858985901\n","Batch 170/774, Loss: 0.31975728273391724\n","Batch 180/774, Loss: 0.20362712442874908\n","Batch 190/774, Loss: 0.26740434765815735\n","Batch 200/774, Loss: 0.05649949982762337\n","Batch 210/774, Loss: 0.03705554082989693\n","Batch 220/774, Loss: 0.21748550236225128\n","Batch 230/774, Loss: 0.18515253067016602\n","Batch 240/774, Loss: 0.2975179851055145\n","Batch 250/774, Loss: 0.09668520838022232\n","Batch 260/774, Loss: 0.1637418419122696\n","Batch 270/774, Loss: 0.03745098412036896\n","Batch 280/774, Loss: 0.08128286898136139\n","Batch 290/774, Loss: 0.0363377146422863\n","Batch 300/774, Loss: 0.06708148121833801\n","Batch 310/774, Loss: 0.07022863626480103\n","Batch 320/774, Loss: 0.15967196226119995\n","Batch 330/774, Loss: 0.5387281775474548\n","Batch 340/774, Loss: 0.1647677719593048\n","Batch 350/774, Loss: 0.20718099176883698\n","Batch 360/774, Loss: 0.09579147398471832\n","Batch 370/774, Loss: 0.3609432280063629\n","Batch 380/774, Loss: 0.12971071898937225\n","Batch 390/774, Loss: 0.30160313844680786\n","Batch 400/774, Loss: 0.12563136219978333\n","Batch 410/774, Loss: 0.1098959743976593\n","Batch 420/774, Loss: 0.19961747527122498\n","Batch 430/774, Loss: 0.1783604472875595\n","Batch 440/774, Loss: 0.07168734818696976\n","Batch 450/774, Loss: 0.24311400949954987\n","Batch 460/774, Loss: 0.08523999154567719\n","Batch 470/774, Loss: 0.18163558840751648\n","Batch 480/774, Loss: 0.15309444069862366\n","Batch 490/774, Loss: 0.236226424574852\n","Batch 500/774, Loss: 0.10899528861045837\n","Batch 510/774, Loss: 0.27636516094207764\n","Batch 520/774, Loss: 0.16790105402469635\n","Batch 530/774, Loss: 0.1320289671421051\n","Batch 540/774, Loss: 0.20022740960121155\n","Batch 550/774, Loss: 0.03366285562515259\n","Batch 560/774, Loss: 0.057226914912462234\n","Batch 570/774, Loss: 0.22320711612701416\n","Batch 580/774, Loss: 0.13778866827487946\n","Batch 590/774, Loss: 0.10274296998977661\n","Batch 600/774, Loss: 0.09694568067789078\n","Batch 610/774, Loss: 0.08955251425504684\n","Batch 620/774, Loss: 0.34419941902160645\n","Batch 630/774, Loss: 0.25796574354171753\n","Batch 640/774, Loss: 0.26384881138801575\n","Batch 650/774, Loss: 0.3752778172492981\n","Batch 660/774, Loss: 0.2802388668060303\n","Batch 670/774, Loss: 0.035975001752376556\n","Batch 680/774, Loss: 0.23416057229042053\n","Batch 690/774, Loss: 0.0818486288189888\n","Batch 700/774, Loss: 0.09662164002656937\n","Batch 710/774, Loss: 0.31704893708229065\n","Batch 720/774, Loss: 0.13024145364761353\n","Batch 730/774, Loss: 0.17393545806407928\n","Batch 740/774, Loss: 0.36077800393104553\n","Batch 750/774, Loss: 0.2597539722919464\n","Batch 760/774, Loss: 0.30349141359329224\n","Batch 770/774, Loss: 0.211439311504364\n","Training epoch completed in: 1m 6s\n","Train loss 0.1943274290347231 accuracy 0.9149228407530097\n","Batch 0/194, test Loss: 0.20608414709568024\n","Batch 10/194, test Loss: 0.5577958822250366\n","Batch 20/194, test Loss: 0.25452324748039246\n","Batch 30/194, test Loss: 0.15100881457328796\n","Batch 40/194, test Loss: 0.5329012870788574\n","Batch 50/194, test Loss: 0.1579965054988861\n","Batch 60/194, test Loss: 0.38592806458473206\n","Batch 70/194, test Loss: 0.4003404676914215\n","Batch 80/194, test Loss: 0.14939792454242706\n","Batch 90/194, test Loss: 0.2065223604440689\n","Batch 100/194, test Loss: 0.17055873572826385\n","Batch 110/194, test Loss: 0.2562088966369629\n","Batch 120/194, test Loss: 0.34280669689178467\n","Batch 130/194, test Loss: 0.7993874549865723\n","Batch 140/194, test Loss: 0.33336156606674194\n","Batch 150/194, test Loss: 0.3696266710758209\n","Batch 160/194, test Loss: 0.2619394063949585\n","Batch 170/194, test Loss: 0.2754501700401306\n","Batch 180/194, test Loss: 0.3132689893245697\n","Batch 190/194, test Loss: 0.5899850130081177\n","test completed in: 0m 5s\n","test loss 0.40990564222303555 accuracy 0.8374798061389338\n","Test Precision: 0.8389918640915214\n","Test Recall: 0.8374798061389338\n","Test F1 Score: 0.8373048710753542\n","Test Accuracy: 0.8374798061389338\n","Starting epoch 7/10\n","Batch 0/774, Loss: 0.3052017092704773\n","Batch 10/774, Loss: 0.12614329159259796\n","Batch 20/774, Loss: 0.11004456132650375\n","Batch 30/774, Loss: 0.3508581817150116\n","Batch 40/774, Loss: 0.2286364734172821\n","Batch 50/774, Loss: 0.35871726274490356\n","Batch 60/774, Loss: 0.09722908586263657\n","Batch 70/774, Loss: 0.14140760898590088\n","Batch 80/774, Loss: 0.10859889537096024\n","Batch 90/774, Loss: 0.27312740683555603\n","Batch 100/774, Loss: 0.25361138582229614\n","Batch 110/774, Loss: 0.030292896553874016\n","Batch 120/774, Loss: 0.10509967058897018\n","Batch 130/774, Loss: 0.21767401695251465\n","Batch 140/774, Loss: 0.13817551732063293\n","Batch 150/774, Loss: 0.20615704357624054\n","Batch 160/774, Loss: 0.2637965679168701\n","Batch 170/774, Loss: 0.16051465272903442\n","Batch 180/774, Loss: 0.17445753514766693\n","Batch 190/774, Loss: 0.2581658959388733\n","Batch 200/774, Loss: 0.07498525828123093\n","Batch 210/774, Loss: 0.04561664164066315\n","Batch 220/774, Loss: 0.1289645880460739\n","Batch 230/774, Loss: 0.11927548050880432\n","Batch 240/774, Loss: 0.2508794963359833\n","Batch 250/774, Loss: 0.0608455166220665\n","Batch 260/774, Loss: 0.09795412421226501\n","Batch 270/774, Loss: 0.018991665914654732\n","Batch 280/774, Loss: 0.04782189056277275\n","Batch 290/774, Loss: 0.021318823099136353\n","Batch 300/774, Loss: 0.02471289597451687\n","Batch 310/774, Loss: 0.05434206873178482\n","Batch 320/774, Loss: 0.10863731056451797\n","Batch 330/774, Loss: 0.4072923958301544\n","Batch 340/774, Loss: 0.14247193932533264\n","Batch 350/774, Loss: 0.08812011778354645\n","Batch 360/774, Loss: 0.10914221405982971\n","Batch 370/774, Loss: 0.31754031777381897\n","Batch 380/774, Loss: 0.1540360450744629\n","Batch 390/774, Loss: 0.24018815159797668\n","Batch 400/774, Loss: 0.10903964936733246\n","Batch 410/774, Loss: 0.030449485406279564\n","Batch 420/774, Loss: 0.12878428399562836\n","Batch 430/774, Loss: 0.1173267662525177\n","Batch 440/774, Loss: 0.053701139986515045\n","Batch 450/774, Loss: 0.1656942516565323\n","Batch 460/774, Loss: 0.1347266435623169\n","Batch 470/774, Loss: 0.30713725090026855\n","Batch 480/774, Loss: 0.11822592467069626\n","Batch 490/774, Loss: 0.20100030303001404\n","Batch 500/774, Loss: 0.05938372388482094\n","Batch 510/774, Loss: 0.24558664858341217\n","Batch 520/774, Loss: 0.25379443168640137\n","Batch 530/774, Loss: 0.09865839779376984\n","Batch 540/774, Loss: 0.1260429471731186\n","Batch 550/774, Loss: 0.04586631432175636\n","Batch 560/774, Loss: 0.02995096705853939\n","Batch 570/774, Loss: 0.07146181911230087\n","Batch 580/774, Loss: 0.13882111012935638\n","Batch 590/774, Loss: 0.07264851778745651\n","Batch 600/774, Loss: 0.007925121113657951\n","Batch 610/774, Loss: 0.05320344120264053\n","Batch 620/774, Loss: 0.11141631007194519\n","Batch 630/774, Loss: 0.1729249209165573\n","Batch 640/774, Loss: 0.09665436297655106\n","Batch 650/774, Loss: 0.25773149728775024\n","Batch 660/774, Loss: 0.35534214973449707\n","Batch 670/774, Loss: 0.09267295897006989\n","Batch 680/774, Loss: 0.09716904163360596\n","Batch 690/774, Loss: 0.07413457334041595\n","Batch 700/774, Loss: 0.05212068185210228\n","Batch 710/774, Loss: 0.44253644347190857\n","Batch 720/774, Loss: 0.10850182175636292\n","Batch 730/774, Loss: 0.16592387855052948\n","Batch 740/774, Loss: 0.12596172094345093\n","Batch 750/774, Loss: 0.23621807992458344\n","Batch 760/774, Loss: 0.21543116867542267\n","Batch 770/774, Loss: 0.252328097820282\n","Training epoch completed in: 1m 6s\n","Train loss 0.16053988967116115 accuracy 0.9304354851741133\n","Batch 0/194, test Loss: 0.1752895712852478\n","Batch 10/194, test Loss: 0.796913206577301\n","Batch 20/194, test Loss: 0.4678559899330139\n","Batch 30/194, test Loss: 0.2788084149360657\n","Batch 40/194, test Loss: 0.6650095582008362\n","Batch 50/194, test Loss: 0.41529911756515503\n","Batch 60/194, test Loss: 0.4411114454269409\n","Batch 70/194, test Loss: 0.6113969683647156\n","Batch 80/194, test Loss: 0.174373060464859\n","Batch 90/194, test Loss: 0.20170515775680542\n","Batch 100/194, test Loss: 0.2006598711013794\n","Batch 110/194, test Loss: 0.1618533730506897\n","Batch 120/194, test Loss: 0.4017447531223297\n","Batch 130/194, test Loss: 1.1164997816085815\n","Batch 140/194, test Loss: 0.3661074936389923\n","Batch 150/194, test Loss: 0.48545777797698975\n","Batch 160/194, test Loss: 0.2738737463951111\n","Batch 170/194, test Loss: 0.277875155210495\n","Batch 180/194, test Loss: 0.34783679246902466\n","Batch 190/194, test Loss: 0.6776069402694702\n","test completed in: 0m 5s\n","test loss 0.48099398495840656 accuracy 0.8281098546042004\n","Test Precision: 0.8330443105440253\n","Test Recall: 0.8281098546042003\n","Test F1 Score: 0.827483509446956\n","Test Accuracy: 0.8281098546042003\n","Starting epoch 8/10\n","Batch 0/774, Loss: 0.16634564101696014\n","Batch 10/774, Loss: 0.05343062803149223\n","Batch 20/774, Loss: 0.03968077152967453\n","Batch 30/774, Loss: 0.22547833621501923\n","Batch 40/774, Loss: 0.20955625176429749\n","Batch 50/774, Loss: 0.25227558612823486\n","Batch 60/774, Loss: 0.04395194351673126\n","Batch 70/774, Loss: 0.1588912308216095\n","Batch 80/774, Loss: 0.0727953389286995\n","Batch 90/774, Loss: 0.2608775496482849\n","Batch 100/774, Loss: 0.2345927506685257\n","Batch 110/774, Loss: 0.033091120421886444\n","Batch 120/774, Loss: 0.06719828397035599\n","Batch 130/774, Loss: 0.15160706639289856\n","Batch 140/774, Loss: 0.057128529995679855\n","Batch 150/774, Loss: 0.15165847539901733\n","Batch 160/774, Loss: 0.05254139378666878\n","Batch 170/774, Loss: 0.1660008728504181\n","Batch 180/774, Loss: 0.14592625200748444\n","Batch 190/774, Loss: 0.2704963684082031\n","Batch 200/774, Loss: 0.08955863118171692\n","Batch 210/774, Loss: 0.02808655984699726\n","Batch 220/774, Loss: 0.10153625160455704\n","Batch 230/774, Loss: 0.05481824278831482\n","Batch 240/774, Loss: 0.2504062056541443\n","Batch 250/774, Loss: 0.02946416288614273\n","Batch 260/774, Loss: 0.11530554294586182\n","Batch 270/774, Loss: 0.08777536451816559\n","Batch 280/774, Loss: 0.09721717983484268\n","Batch 290/774, Loss: 0.04269106313586235\n","Batch 300/774, Loss: 0.025612134486436844\n","Batch 310/774, Loss: 0.09062715619802475\n","Batch 320/774, Loss: 0.10746844857931137\n","Batch 330/774, Loss: 0.4440326690673828\n","Batch 340/774, Loss: 0.13997554779052734\n","Batch 350/774, Loss: 0.14160485565662384\n","Batch 360/774, Loss: 0.10547530651092529\n","Batch 370/774, Loss: 0.3046344816684723\n","Batch 380/774, Loss: 0.12993189692497253\n","Batch 390/774, Loss: 0.19402876496315002\n","Batch 400/774, Loss: 0.18775838613510132\n","Batch 410/774, Loss: 0.03079286776483059\n","Batch 420/774, Loss: 0.14521898329257965\n","Batch 430/774, Loss: 0.06495607644319534\n","Batch 440/774, Loss: 0.015449931845068932\n","Batch 450/774, Loss: 0.1792508065700531\n","Batch 460/774, Loss: 0.03333563357591629\n","Batch 470/774, Loss: 0.2870984673500061\n","Batch 480/774, Loss: 0.23859667778015137\n","Batch 490/774, Loss: 0.1921795904636383\n","Batch 500/774, Loss: 0.05342431366443634\n","Batch 510/774, Loss: 0.2124839574098587\n","Batch 520/774, Loss: 0.25759780406951904\n","Batch 530/774, Loss: 0.08434087783098221\n","Batch 540/774, Loss: 0.07580599933862686\n","Batch 550/774, Loss: 0.027940740808844566\n","Batch 560/774, Loss: 0.1275612711906433\n","Batch 570/774, Loss: 0.017105495557188988\n","Batch 580/774, Loss: 0.15438522398471832\n","Batch 590/774, Loss: 0.06479581445455551\n","Batch 600/774, Loss: 0.07764486968517303\n","Batch 610/774, Loss: 0.0209293682128191\n","Batch 620/774, Loss: 0.08991917967796326\n","Batch 630/774, Loss: 0.46794113516807556\n","Batch 640/774, Loss: 0.07059337943792343\n","Batch 650/774, Loss: 0.02315756492316723\n","Batch 660/774, Loss: 0.3710879683494568\n","Batch 670/774, Loss: 0.09150395542383194\n","Batch 680/774, Loss: 0.13282527029514313\n","Batch 690/774, Loss: 0.0715673491358757\n","Batch 700/774, Loss: 0.031612515449523926\n","Batch 710/774, Loss: 0.18359710276126862\n","Batch 720/774, Loss: 0.07095129787921906\n","Batch 730/774, Loss: 0.11536514014005661\n","Batch 740/774, Loss: 0.1557825803756714\n","Batch 750/774, Loss: 0.2019738405942917\n","Batch 760/774, Loss: 0.5453580617904663\n","Batch 770/774, Loss: 0.11765222996473312\n","Training epoch completed in: 1m 6s\n","Train loss 0.13582067353830213 accuracy 0.9442514341116588\n","Batch 0/194, test Loss: 0.32021787762641907\n","Batch 10/194, test Loss: 0.7340435981750488\n","Batch 20/194, test Loss: 0.31795740127563477\n","Batch 30/194, test Loss: 0.15974733233451843\n","Batch 40/194, test Loss: 0.7617865204811096\n","Batch 50/194, test Loss: 0.5328997373580933\n","Batch 60/194, test Loss: 0.3034563958644867\n","Batch 70/194, test Loss: 0.5346533060073853\n","Batch 80/194, test Loss: 0.32056736946105957\n","Batch 90/194, test Loss: 0.2420133650302887\n","Batch 100/194, test Loss: 0.21385490894317627\n","Batch 110/194, test Loss: 0.24823881685733795\n","Batch 120/194, test Loss: 0.43960756063461304\n","Batch 130/194, test Loss: 0.8788405060768127\n","Batch 140/194, test Loss: 0.4421783685684204\n","Batch 150/194, test Loss: 0.48443010449409485\n","Batch 160/194, test Loss: 0.3198601305484772\n","Batch 170/194, test Loss: 0.24995696544647217\n","Batch 180/194, test Loss: 0.37426427006721497\n","Batch 190/194, test Loss: 0.5793594717979431\n","test completed in: 0m 5s\n","test loss 0.5094172185978171 accuracy 0.824232633279483\n","Test Precision: 0.8299881511045603\n","Test Recall: 0.824232633279483\n","Test F1 Score: 0.8234770562586246\n","Test Accuracy: 0.824232633279483\n","Starting epoch 9/10\n","Batch 0/774, Loss: 0.17341963946819305\n","Batch 10/774, Loss: 0.04126410558819771\n","Batch 20/774, Loss: 0.07167141139507294\n","Batch 30/774, Loss: 0.16144844889640808\n","Batch 40/774, Loss: 0.20288051664829254\n","Batch 50/774, Loss: 0.15432102978229523\n","Batch 60/774, Loss: 0.026996782049536705\n","Batch 70/774, Loss: 0.05485238507390022\n","Batch 80/774, Loss: 0.10020319372415543\n","Batch 90/774, Loss: 0.16902588307857513\n","Batch 100/774, Loss: 0.18503959476947784\n","Batch 110/774, Loss: 0.012819473631680012\n","Batch 120/774, Loss: 0.07308737188577652\n","Batch 130/774, Loss: 0.19984936714172363\n","Batch 140/774, Loss: 0.046560756862163544\n","Batch 150/774, Loss: 0.20623153448104858\n","Batch 160/774, Loss: 0.015926122665405273\n","Batch 170/774, Loss: 0.0901482030749321\n","Batch 180/774, Loss: 0.26771095395088196\n","Batch 190/774, Loss: 0.1306503415107727\n","Batch 200/774, Loss: 0.021686794236302376\n","Batch 210/774, Loss: 0.011351014487445354\n","Batch 220/774, Loss: 0.04787243530154228\n","Batch 230/774, Loss: 0.020652055740356445\n","Batch 240/774, Loss: 0.20297181606292725\n","Batch 250/774, Loss: 0.047063324600458145\n","Batch 260/774, Loss: 0.0700894296169281\n","Batch 270/774, Loss: 0.00397274736315012\n","Batch 280/774, Loss: 0.019486868754029274\n","Batch 290/774, Loss: 0.02225825935602188\n","Batch 300/774, Loss: 0.044158533215522766\n","Batch 310/774, Loss: 0.22315508127212524\n","Batch 320/774, Loss: 0.06399448215961456\n","Batch 330/774, Loss: 0.33352646231651306\n","Batch 340/774, Loss: 0.07838598638772964\n","Batch 350/774, Loss: 0.07672648131847382\n","Batch 360/774, Loss: 0.06728515028953552\n","Batch 370/774, Loss: 0.22469143569469452\n","Batch 380/774, Loss: 0.1434992104768753\n","Batch 390/774, Loss: 0.3198849558830261\n","Batch 400/774, Loss: 0.1155286505818367\n","Batch 410/774, Loss: 0.03142426162958145\n","Batch 420/774, Loss: 0.17849454283714294\n","Batch 430/774, Loss: 0.05269594490528107\n","Batch 440/774, Loss: 0.005099512171000242\n","Batch 450/774, Loss: 0.2060590237379074\n","Batch 460/774, Loss: 0.02491421066224575\n","Batch 470/774, Loss: 0.19734105467796326\n","Batch 480/774, Loss: 0.06388936191797256\n","Batch 490/774, Loss: 0.09234929829835892\n","Batch 500/774, Loss: 0.03313311189413071\n","Batch 510/774, Loss: 0.18706010282039642\n","Batch 520/774, Loss: 0.15313920378684998\n","Batch 530/774, Loss: 0.07955680787563324\n","Batch 540/774, Loss: 0.07678347080945969\n","Batch 550/774, Loss: 0.062181662768125534\n","Batch 560/774, Loss: 0.03927744925022125\n","Batch 570/774, Loss: 0.018617667257785797\n","Batch 580/774, Loss: 0.013532241806387901\n","Batch 590/774, Loss: 0.08041010051965714\n","Batch 600/774, Loss: 0.0038239709101617336\n","Batch 610/774, Loss: 0.06833034008741379\n","Batch 620/774, Loss: 0.04913350194692612\n","Batch 630/774, Loss: 0.4480600357055664\n","Batch 640/774, Loss: 0.07216216623783112\n","Batch 650/774, Loss: 0.0819510817527771\n","Batch 660/774, Loss: 0.3904569447040558\n","Batch 670/774, Loss: 0.013690661638975143\n","Batch 680/774, Loss: 0.06728103011846542\n","Batch 690/774, Loss: 0.014334374107420444\n","Batch 700/774, Loss: 0.04014861583709717\n","Batch 710/774, Loss: 0.19614845514297485\n","Batch 720/774, Loss: 0.03988589718937874\n","Batch 730/774, Loss: 0.09414611011743546\n","Batch 740/774, Loss: 0.1434478759765625\n","Batch 750/774, Loss: 0.15902021527290344\n","Batch 760/774, Loss: 0.5497628450393677\n","Batch 770/774, Loss: 0.12937574088573456\n","Training epoch completed in: 1m 6s\n","Train loss 0.10903143453789581 accuracy 0.9560475074735396\n","Batch 0/194, test Loss: 0.4339238405227661\n","Batch 10/194, test Loss: 0.7449902892112732\n","Batch 20/194, test Loss: 0.33971866965293884\n","Batch 30/194, test Loss: 0.28704309463500977\n","Batch 40/194, test Loss: 0.7947778701782227\n","Batch 50/194, test Loss: 0.3177485764026642\n","Batch 60/194, test Loss: 0.34819263219833374\n","Batch 70/194, test Loss: 0.7146953344345093\n","Batch 80/194, test Loss: 0.5720640420913696\n","Batch 90/194, test Loss: 0.17032243311405182\n","Batch 100/194, test Loss: 0.08190593868494034\n","Batch 110/194, test Loss: 0.2913076877593994\n","Batch 120/194, test Loss: 0.44768205285072327\n","Batch 130/194, test Loss: 0.9845132231712341\n","Batch 140/194, test Loss: 0.5447207689285278\n","Batch 150/194, test Loss: 0.532153308391571\n","Batch 160/194, test Loss: 0.28362295031547546\n","Batch 170/194, test Loss: 0.23741641640663147\n","Batch 180/194, test Loss: 0.40771228075027466\n","Batch 190/194, test Loss: 0.6425972580909729\n","test completed in: 0m 5s\n","test loss 0.5410244096137737 accuracy 0.8410339256865913\n","Test Precision: 0.8413952807377406\n","Test Recall: 0.8410339256865913\n","Test F1 Score: 0.8409949179125422\n","Test Accuracy: 0.8410339256865913\n","Starting epoch 10/10\n","Batch 0/774, Loss: 0.164714977145195\n","Batch 10/774, Loss: 0.010588052682578564\n","Batch 20/774, Loss: 0.016481295228004456\n","Batch 30/774, Loss: 0.15258024632930756\n","Batch 40/774, Loss: 0.13858504593372345\n","Batch 50/774, Loss: 0.18086475133895874\n","Batch 60/774, Loss: 0.023820163682103157\n","Batch 70/774, Loss: 0.04952283576130867\n","Batch 80/774, Loss: 0.10441922396421432\n","Batch 90/774, Loss: 0.09741702675819397\n","Batch 100/774, Loss: 0.07493046671152115\n","Batch 110/774, Loss: 0.025442618876695633\n","Batch 120/774, Loss: 0.016966288909316063\n","Batch 130/774, Loss: 0.12181375920772552\n","Batch 140/774, Loss: 0.033280570060014725\n","Batch 150/774, Loss: 0.08103802055120468\n","Batch 160/774, Loss: 0.009667051024734974\n","Batch 170/774, Loss: 0.10605711489915848\n","Batch 180/774, Loss: 0.05419251322746277\n","Batch 190/774, Loss: 0.22556310892105103\n","Batch 200/774, Loss: 0.019273387268185616\n","Batch 210/774, Loss: 0.00955126155167818\n","Batch 220/774, Loss: 0.01349965576082468\n","Batch 230/774, Loss: 0.02294250577688217\n","Batch 240/774, Loss: 0.045592550188302994\n","Batch 250/774, Loss: 0.024206701666116714\n","Batch 260/774, Loss: 0.11171267926692963\n","Batch 270/774, Loss: 0.13834181427955627\n","Batch 280/774, Loss: 0.04865312576293945\n","Batch 290/774, Loss: 0.013408972881734371\n","Batch 300/774, Loss: 0.032853759825229645\n","Batch 310/774, Loss: 0.05592013895511627\n","Batch 320/774, Loss: 0.04325668141245842\n","Batch 330/774, Loss: 0.7681581974029541\n","Batch 340/774, Loss: 0.13422073423862457\n","Batch 350/774, Loss: 0.01908924989402294\n","Batch 360/774, Loss: 0.00618682662025094\n","Batch 370/774, Loss: 0.20507055521011353\n","Batch 380/774, Loss: 0.10735095292329788\n","Batch 390/774, Loss: 0.058732323348522186\n","Batch 400/774, Loss: 0.03693811595439911\n","Batch 410/774, Loss: 0.012419430539011955\n","Batch 420/774, Loss: 0.05563797429203987\n","Batch 430/774, Loss: 0.06864564120769501\n","Batch 440/774, Loss: 0.003459175117313862\n","Batch 450/774, Loss: 0.1234799325466156\n","Batch 460/774, Loss: 0.014094898477196693\n","Batch 470/774, Loss: 0.02026595175266266\n","Batch 480/774, Loss: 0.16818024218082428\n","Batch 490/774, Loss: 0.06827692687511444\n","Batch 500/774, Loss: 0.028356749564409256\n","Batch 510/774, Loss: 0.24902786314487457\n","Batch 520/774, Loss: 0.033319082111120224\n","Batch 530/774, Loss: 0.07847190648317337\n","Batch 540/774, Loss: 0.07654136419296265\n","Batch 550/774, Loss: 0.0015372132183983922\n","Batch 560/774, Loss: 0.006456123664975166\n","Batch 570/774, Loss: 0.00932532362639904\n","Batch 580/774, Loss: 0.009824970737099648\n","Batch 590/774, Loss: 0.07045385986566544\n","Batch 600/774, Loss: 0.004789438098669052\n","Batch 610/774, Loss: 0.13109728693962097\n","Batch 620/774, Loss: 0.14137908816337585\n","Batch 630/774, Loss: 0.0461769700050354\n","Batch 640/774, Loss: 0.1313467025756836\n","Batch 650/774, Loss: 0.07021268457174301\n","Batch 660/774, Loss: 0.292457640171051\n","Batch 670/774, Loss: 0.04994893819093704\n","Batch 680/774, Loss: 0.07724403589963913\n","Batch 690/774, Loss: 0.0037797291297465563\n","Batch 700/774, Loss: 0.01160240825265646\n","Batch 710/774, Loss: 0.19484283030033112\n","Batch 720/774, Loss: 0.02679438143968582\n","Batch 730/774, Loss: 0.02339472435414791\n","Batch 740/774, Loss: 0.025077009573578835\n","Batch 750/774, Loss: 0.12491712719202042\n","Batch 760/774, Loss: 0.564172089099884\n","Batch 770/774, Loss: 0.1906384974718094\n","Training epoch completed in: 1m 6s\n","Train loss 0.09748803753630217 accuracy 0.9604104387169751\n","Batch 0/194, test Loss: 0.6107328534126282\n","Batch 10/194, test Loss: 0.7304388284683228\n","Batch 20/194, test Loss: 0.4060029685497284\n","Batch 30/194, test Loss: 0.23527921736240387\n","Batch 40/194, test Loss: 0.6238185167312622\n","Batch 50/194, test Loss: 0.09570310264825821\n","Batch 60/194, test Loss: 0.37234583497047424\n","Batch 70/194, test Loss: 0.7070991396903992\n","Batch 80/194, test Loss: 0.5075924396514893\n","Batch 90/194, test Loss: 0.18200507760047913\n","Batch 100/194, test Loss: 0.038375549018383026\n","Batch 110/194, test Loss: 0.3787286579608917\n","Batch 120/194, test Loss: 0.3155500292778015\n","Batch 130/194, test Loss: 0.837462842464447\n","Batch 140/194, test Loss: 0.6416434049606323\n","Batch 150/194, test Loss: 0.4807751774787903\n","Batch 160/194, test Loss: 0.13471384346485138\n","Batch 170/194, test Loss: 0.25825756788253784\n","Batch 180/194, test Loss: 0.4204047918319702\n","Batch 190/194, test Loss: 0.8638461232185364\n","test completed in: 0m 5s\n","test loss 0.5450116631853366 accuracy 0.8523424878836834\n","Test Precision: 0.8529592428659901\n","Test Recall: 0.8523424878836834\n","Test F1 Score: 0.8522743221359198\n","Test Accuracy: 0.8523424878836834\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"lndzKrdwGmj-"}}]}
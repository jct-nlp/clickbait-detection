{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50924,"status":"ok","timestamp":1722588432986,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"},"user_tz":-180},"id":"X26qiUbJHNLn","outputId":"7c2e8cc2-abef-4f7f-e7f2-26550e62ab8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install torch torchvision transformers pandas"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0dd73f0c8af344569826bdcde78fbd2f","fc56914b7035402d86a603cbc30c0af6","09f7bd6dc12745dab21a572619b874e2","2cfadcdc5ad3453daa56b9f738ac4bbe","5101d88c69c24db5aa8e2f9b58fe6de6","e40c92fd88ce44e8891be6490fa4aee9","864fc74cda9d4f5185ec29d4eaf81996","01cc5a3a8398448dbf381894e0986307","2be32e1e8a954db28552f5b3d99dbf2a","4bf21c216e8848ee974171f464350737","fc370844760344ab834631ec218224a0","4af1e744003d475a804263b11d1a708a","122ddd425c1146cfaff67845b94c5c3a","a6a29cc34b10467cb38103e7c5ed0f05","487f30f971d84d6b80d8545c4c7dc852","5ffb3e25686f4c6fac85bd8fe42ff271","6f3fa9a41af44b3e952b70eb954fbe71","6d32ff223dd7407d83472e58e059fc27","fdb5240f468246399f2f517d6a761dba","e8c95404c9484b18a743697695c7628e","d09658827e964695a86ca2abef0489ac","b0ceb1bb2ed645f2aac523f77e324905","222947e57ba4416d87a264f33d0d97af","e352b9b7ade84e779286fadd3bb0ce5c","26d6ee6c451c4a39ab7aad67f98396db","d00275346da94cc4b3bc3816dd8c6093","62476dfd7f354ea2824ffaa05220b3ae","0f65128142654982878223bd34d5a919","68a9bd4a3be44c7ab6c18f85a20143b5","a9d66cfd879c4fae80f9032461806782","d0d274de0cd443d294e3fb8aece21d3b","c9f174bdf4814e908d1d9193485db22b","9767ad1fabfc4c9682df01e74c65376f","ac9a4ac0495140509b317e5fe5ee7708","4f62289b6b754b7dac4d60097697a1e8","b0a457ffdc3e4ca4a1e10c18b0108f8f","b22cb4b6e31c4d38925b19994535a8e3","37b93f6b10334bd8b46e1beb1df8ffb9","15eb13ff74924ebfbb7efc80c4ba9918","fe11fef1f7e74bd383d697de45893ceb","ad467105bda446fd9344af2cc7e68293","c8f97ce534e24975be888d0be7211201","07157af1237d45d99026c91e9165546e","742dffc797ed40f98a6448c0edaf2b58","2fc03ac6f5be4584b8e047de96d0587f","fa8a3615c6bf48a88595ab03acb504a8","c9618a9d333c48be9832057b9278b5a4","4eafa1ec18f54783bd0df6f18e2086a6","8ac17c6fe9a64b6f975f767f3097bd42","e0e1a97c23e544d6878240b1b23a6fd4","0ec4d9fa8d874800ac30becef575d140","e9ac93159169447d9e281d11d7430907","cbcd1f4d58f84cee985917c2fc4c9aba","9ea4f6d9c51947f38fa17dbe5b94a1d2","7fd607c4e66e4170819371768edccf6c"]},"id":"pPSyyuKOEnOS","executionInfo":{"status":"ok","timestamp":1722589959712,"user_tz":-180,"elapsed":1483638,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"}},"outputId":"f3629e55-1b19-4d98-cc15-c4ab1e86bdba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting main process\n","Loading the dataset.\n","Dataset loaded successfully.\n","Cleaning the dataset.\n","Dataset cleaned.\n","Splitting the dataset into training and test sets.\n","Dataset split into training and test sets.\n","Using device: cuda\n","Loading the tokenizer and model.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dd73f0c8af344569826bdcde78fbd2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4af1e744003d475a804263b11d1a708a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"222947e57ba4416d87a264f33d0d97af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac9a4ac0495140509b317e5fe5ee7708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fc03ac6f5be4584b8e047de96d0587f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model and tokenizer loaded and moved to device.\n","Creating data loaders.\n","Data loaders created.\n","Optimizer, scheduler, and loss function defined.\n","Starting epoch 1/10\n","Batch 0/833, Loss: 0.5698196291923523\n","Batch 10/833, Loss: 0.3742077946662903\n","Batch 20/833, Loss: 0.3329060673713684\n","Batch 30/833, Loss: 0.3081354796886444\n","Batch 40/833, Loss: 0.1973564326763153\n","Batch 50/833, Loss: 0.22523877024650574\n","Batch 60/833, Loss: 0.3335338234901428\n","Batch 70/833, Loss: 0.049282725900411606\n","Batch 80/833, Loss: 0.1103522926568985\n","Batch 90/833, Loss: 0.04980083927512169\n","Batch 100/833, Loss: 0.026369042694568634\n","Batch 110/833, Loss: 0.15667112171649933\n","Batch 120/833, Loss: 0.04068193584680557\n","Batch 130/833, Loss: 0.22014476358890533\n","Batch 140/833, Loss: 0.13823702931404114\n","Batch 150/833, Loss: 0.29426923394203186\n","Batch 160/833, Loss: 0.21577489376068115\n","Batch 170/833, Loss: 0.07433363795280457\n","Batch 180/833, Loss: 0.15754176676273346\n","Batch 190/833, Loss: 0.2123333364725113\n","Batch 200/833, Loss: 0.24499428272247314\n","Batch 210/833, Loss: 0.1673174798488617\n","Batch 220/833, Loss: 0.045113883912563324\n","Batch 230/833, Loss: 0.08056723326444626\n","Batch 240/833, Loss: 0.007911771535873413\n","Batch 250/833, Loss: 0.04805224761366844\n","Batch 260/833, Loss: 0.08562779426574707\n","Batch 270/833, Loss: 0.11231710016727448\n","Batch 280/833, Loss: 0.24899300932884216\n","Batch 290/833, Loss: 0.2885345220565796\n","Batch 300/833, Loss: 0.17609363794326782\n","Batch 310/833, Loss: 0.051075685769319534\n","Batch 320/833, Loss: 0.03936869651079178\n","Batch 330/833, Loss: 0.1467636227607727\n","Batch 340/833, Loss: 0.30908825993537903\n","Batch 350/833, Loss: 0.39349737763404846\n","Batch 360/833, Loss: 0.014507866464555264\n","Batch 370/833, Loss: 0.09208323806524277\n","Batch 380/833, Loss: 0.0990896075963974\n","Batch 390/833, Loss: 0.00738149881362915\n","Batch 400/833, Loss: 0.007593545131385326\n","Batch 410/833, Loss: 0.03773125261068344\n","Batch 420/833, Loss: 0.08293161541223526\n","Batch 430/833, Loss: 0.0352606400847435\n","Batch 440/833, Loss: 0.08763871341943741\n","Batch 450/833, Loss: 0.02612350881099701\n","Batch 460/833, Loss: 0.022455774247646332\n","Batch 470/833, Loss: 0.020328622311353683\n","Batch 480/833, Loss: 0.3517163097858429\n","Batch 490/833, Loss: 0.25270405411720276\n","Batch 500/833, Loss: 0.1273808628320694\n","Batch 510/833, Loss: 0.10113202035427094\n","Batch 520/833, Loss: 0.5455418229103088\n","Batch 530/833, Loss: 0.1566462367773056\n","Batch 540/833, Loss: 0.06966795027256012\n","Batch 550/833, Loss: 0.24433587491512299\n","Batch 560/833, Loss: 0.0922231674194336\n","Batch 570/833, Loss: 0.016325397416949272\n","Batch 580/833, Loss: 0.10691079497337341\n","Batch 590/833, Loss: 0.053765445947647095\n","Batch 600/833, Loss: 0.022174397483468056\n","Batch 610/833, Loss: 0.11122220009565353\n","Batch 620/833, Loss: 0.06667910516262054\n","Batch 630/833, Loss: 0.05671873688697815\n","Batch 640/833, Loss: 0.10885698348283768\n","Batch 650/833, Loss: 0.16699804365634918\n","Batch 660/833, Loss: 0.06561123579740524\n","Batch 670/833, Loss: 0.011615999974310398\n","Batch 680/833, Loss: 0.012728281319141388\n","Batch 690/833, Loss: 0.04727298393845558\n","Batch 700/833, Loss: 0.01510128565132618\n","Batch 710/833, Loss: 0.013158933259546757\n","Batch 720/833, Loss: 0.05680524930357933\n","Batch 730/833, Loss: 0.018705595284700394\n","Batch 740/833, Loss: 0.0174273531883955\n","Batch 750/833, Loss: 0.2364753782749176\n","Batch 760/833, Loss: 0.010743820108473301\n","Batch 770/833, Loss: 0.021408844739198685\n","Batch 780/833, Loss: 0.0557803213596344\n","Batch 790/833, Loss: 0.03369893133640289\n","Batch 800/833, Loss: 0.0196379367262125\n","Batch 810/833, Loss: 0.005119415000081062\n","Batch 820/833, Loss: 0.012715780176222324\n","Batch 830/833, Loss: 0.005328209139406681\n","Training epoch completed in: 2m 15s\n","Train loss 0.12795942321493506 accuracy 0.9531672170519364\n","Batch 0/209, Test Loss: 0.03558714687824249\n","Batch 10/209, Test Loss: 0.5403084754943848\n","Batch 20/209, Test Loss: 0.013201577588915825\n","Batch 30/209, Test Loss: 0.053189005702733994\n","Batch 40/209, Test Loss: 0.013984497636556625\n","Batch 50/209, Test Loss: 0.010317306034266949\n","Batch 60/209, Test Loss: 0.1408235728740692\n","Batch 70/209, Test Loss: 0.12203190475702286\n","Batch 80/209, Test Loss: 0.05897092819213867\n","Batch 90/209, Test Loss: 0.006991249043494463\n","Batch 100/209, Test Loss: 0.008618221618235111\n","Batch 110/209, Test Loss: 0.01592215709388256\n","Batch 120/209, Test Loss: 0.03315410017967224\n","Batch 130/209, Test Loss: 0.004069852642714977\n","Batch 140/209, Test Loss: 0.010303661227226257\n","Batch 150/209, Test Loss: 0.0065405163913965225\n","Batch 160/209, Test Loss: 0.04835335537791252\n","Batch 170/209, Test Loss: 0.11534612625837326\n","Batch 180/209, Test Loss: 0.2646458148956299\n","Batch 190/209, Test Loss: 0.004944914020597935\n","Batch 200/209, Test Loss: 0.0361303985118866\n","Test evaluation completed in: 0m 11s\n","Test loss 0.0754532626959268 accuracy 0.9726890756302521\n","Test Precision: 0.9725906155471092\n","Test Recall: 0.9726890756302521\n","Test F1 Score: 0.9725711508424594\n","Starting epoch 2/10\n","Batch 0/833, Loss: 0.013681129552423954\n","Batch 10/833, Loss: 0.10441917181015015\n","Batch 20/833, Loss: 0.034104689955711365\n","Batch 30/833, Loss: 0.10154496133327484\n","Batch 40/833, Loss: 0.039994627237319946\n","Batch 50/833, Loss: 0.013987994752824306\n","Batch 60/833, Loss: 0.019685151055455208\n","Batch 70/833, Loss: 0.02884088270366192\n","Batch 80/833, Loss: 0.009803048335015774\n","Batch 90/833, Loss: 0.014422574080526829\n","Batch 100/833, Loss: 0.004537195898592472\n","Batch 110/833, Loss: 0.04355322942137718\n","Batch 120/833, Loss: 0.09832771122455597\n","Batch 130/833, Loss: 0.04184488579630852\n","Batch 140/833, Loss: 0.018346033990383148\n","Batch 150/833, Loss: 0.02823738195002079\n","Batch 160/833, Loss: 0.0221236739307642\n","Batch 170/833, Loss: 0.003966344054788351\n","Batch 180/833, Loss: 0.021414753049612045\n","Batch 190/833, Loss: 0.013202576898038387\n","Batch 200/833, Loss: 0.2470867782831192\n","Batch 210/833, Loss: 0.029923144727945328\n","Batch 220/833, Loss: 0.003487485693767667\n","Batch 230/833, Loss: 0.00755402073264122\n","Batch 240/833, Loss: 0.001907069468870759\n","Batch 250/833, Loss: 0.006437378469854593\n","Batch 260/833, Loss: 0.05414922162890434\n","Batch 270/833, Loss: 0.006768407765775919\n","Batch 280/833, Loss: 0.04418134689331055\n","Batch 290/833, Loss: 0.045177899301052094\n","Batch 300/833, Loss: 0.002680125879123807\n","Batch 310/833, Loss: 0.005030300933867693\n","Batch 320/833, Loss: 0.027415594086050987\n","Batch 330/833, Loss: 0.028551578521728516\n","Batch 340/833, Loss: 0.0584431029856205\n","Batch 350/833, Loss: 0.0040459223091602325\n","Batch 360/833, Loss: 0.0021098051220178604\n","Batch 370/833, Loss: 0.10849989205598831\n","Batch 380/833, Loss: 0.021994303911924362\n","Batch 390/833, Loss: 0.0012762409169226885\n","Batch 400/833, Loss: 0.005702924448996782\n","Batch 410/833, Loss: 0.008799275383353233\n","Batch 420/833, Loss: 0.010077877901494503\n","Batch 430/833, Loss: 0.000878390681464225\n","Batch 440/833, Loss: 0.0054022022522985935\n","Batch 450/833, Loss: 0.03039056994020939\n","Batch 460/833, Loss: 0.009083756245672703\n","Batch 470/833, Loss: 0.006232758983969688\n","Batch 480/833, Loss: 0.3308073878288269\n","Batch 490/833, Loss: 0.018146740272641182\n","Batch 500/833, Loss: 0.016417235136032104\n","Batch 510/833, Loss: 0.023761015385389328\n","Batch 520/833, Loss: 0.08351754397153854\n","Batch 530/833, Loss: 0.049584805965423584\n","Batch 540/833, Loss: 0.0021072884555906057\n","Batch 550/833, Loss: 0.05303400382399559\n","Batch 560/833, Loss: 0.1152309849858284\n","Batch 570/833, Loss: 0.0046910447999835014\n","Batch 580/833, Loss: 0.0153689319267869\n","Batch 590/833, Loss: 0.0025330549106001854\n","Batch 600/833, Loss: 0.0016323176678270102\n","Batch 610/833, Loss: 0.001511972863227129\n","Batch 620/833, Loss: 0.013011655770242214\n","Batch 630/833, Loss: 0.006978488527238369\n","Batch 640/833, Loss: 0.023404881358146667\n","Batch 650/833, Loss: 0.013171273283660412\n","Batch 660/833, Loss: 0.010949982330203056\n","Batch 670/833, Loss: 0.006470074877142906\n","Batch 680/833, Loss: 0.0011058577802032232\n","Batch 690/833, Loss: 0.016896836459636688\n","Batch 700/833, Loss: 0.0008736734744161367\n","Batch 710/833, Loss: 0.0036333203315734863\n","Batch 720/833, Loss: 0.0024241332430392504\n","Batch 730/833, Loss: 0.004676009528338909\n","Batch 740/833, Loss: 0.018877485767006874\n","Batch 750/833, Loss: 0.015367378480732441\n","Batch 760/833, Loss: 0.007743589114397764\n","Batch 770/833, Loss: 0.002262934111058712\n","Batch 780/833, Loss: 0.13654935359954834\n","Batch 790/833, Loss: 0.004567766562104225\n","Batch 800/833, Loss: 0.051678914576768875\n","Batch 810/833, Loss: 0.005639444570988417\n","Batch 820/833, Loss: 0.003607067046687007\n","Batch 830/833, Loss: 0.003997510299086571\n","Training epoch completed in: 2m 16s\n","Train loss 0.048859349768782104 accuracy 0.9845391774241969\n","Batch 0/209, Test Loss: 0.02131643332540989\n","Batch 10/209, Test Loss: 0.8035531044006348\n","Batch 20/209, Test Loss: 0.10663969814777374\n","Batch 30/209, Test Loss: 0.006675247102975845\n","Batch 40/209, Test Loss: 0.003094651736319065\n","Batch 50/209, Test Loss: 0.010092208161950111\n","Batch 60/209, Test Loss: 0.3254568576812744\n","Batch 70/209, Test Loss: 0.07159513235092163\n","Batch 80/209, Test Loss: 0.017464974895119667\n","Batch 90/209, Test Loss: 0.0025910947006195784\n","Batch 100/209, Test Loss: 0.005998346488922834\n","Batch 110/209, Test Loss: 0.04132319986820221\n","Batch 120/209, Test Loss: 0.10171275585889816\n","Batch 130/209, Test Loss: 0.0051429904997348785\n","Batch 140/209, Test Loss: 0.13568618893623352\n","Batch 150/209, Test Loss: 0.0034585800021886826\n","Batch 160/209, Test Loss: 0.030356626957654953\n","Batch 170/209, Test Loss: 0.046194709837436676\n","Batch 180/209, Test Loss: 0.3498046100139618\n","Batch 190/209, Test Loss: 0.006982846185564995\n","Batch 200/209, Test Loss: 0.11557047069072723\n","Test evaluation completed in: 0m 11s\n","Test loss 0.08156415930577068 accuracy 0.9693877551020408\n","Test Precision: 0.9694018191957677\n","Test Recall: 0.9693877551020408\n","Test F1 Score: 0.96911668521927\n","Starting epoch 3/10\n","Batch 0/833, Loss: 0.009668540209531784\n","Batch 10/833, Loss: 0.026886416599154472\n","Batch 20/833, Loss: 0.038292136043310165\n","Batch 30/833, Loss: 0.009192360565066338\n","Batch 40/833, Loss: 0.1822655349969864\n","Batch 50/833, Loss: 0.010537131689488888\n","Batch 60/833, Loss: 0.0035778817255049944\n","Batch 70/833, Loss: 0.009665330871939659\n","Batch 80/833, Loss: 0.05780094116926193\n","Batch 90/833, Loss: 0.0063545238226652145\n","Batch 100/833, Loss: 0.0020366606768220663\n","Batch 110/833, Loss: 0.0026263915933668613\n","Batch 120/833, Loss: 0.005222491919994354\n","Batch 130/833, Loss: 0.005107543896883726\n","Batch 140/833, Loss: 0.0019201518734917045\n","Batch 150/833, Loss: 0.004350820556282997\n","Batch 160/833, Loss: 0.0017116208327934146\n","Batch 170/833, Loss: 0.002632956253364682\n","Batch 180/833, Loss: 0.0504298210144043\n","Batch 190/833, Loss: 0.029026975855231285\n","Batch 200/833, Loss: 0.0011686771176755428\n","Batch 210/833, Loss: 0.10424775630235672\n","Batch 220/833, Loss: 0.0007543580722995102\n","Batch 230/833, Loss: 0.00380048924125731\n","Batch 240/833, Loss: 0.0018140143947675824\n","Batch 250/833, Loss: 0.008585253730416298\n","Batch 260/833, Loss: 0.005440636537969112\n","Batch 270/833, Loss: 0.0019894230645149946\n","Batch 280/833, Loss: 0.0068617756478488445\n","Batch 290/833, Loss: 0.0020624089520424604\n","Batch 300/833, Loss: 0.00207105022855103\n","Batch 310/833, Loss: 0.005351989064365625\n","Batch 320/833, Loss: 0.0027311553712934256\n","Batch 330/833, Loss: 0.010985097847878933\n","Batch 340/833, Loss: 0.14242972433567047\n","Batch 350/833, Loss: 0.0012112046824768186\n","Batch 360/833, Loss: 0.021258076652884483\n","Batch 370/833, Loss: 0.0015853327931836247\n","Batch 380/833, Loss: 0.0035613751970231533\n","Batch 390/833, Loss: 0.0003586775274015963\n","Batch 400/833, Loss: 0.000687880557961762\n","Batch 410/833, Loss: 0.000918207224458456\n","Batch 420/833, Loss: 0.0004163901903666556\n","Batch 430/833, Loss: 0.0010991031304001808\n","Batch 440/833, Loss: 0.003320581279695034\n","Batch 450/833, Loss: 0.0014505634317174554\n","Batch 460/833, Loss: 0.005863826721906662\n","Batch 470/833, Loss: 0.002878631930798292\n","Batch 480/833, Loss: 0.15030977129936218\n","Batch 490/833, Loss: 0.05836252123117447\n","Batch 500/833, Loss: 0.006000257097184658\n","Batch 510/833, Loss: 0.055477917194366455\n","Batch 520/833, Loss: 0.0033349688164889812\n","Batch 530/833, Loss: 0.03769839555025101\n","Batch 540/833, Loss: 0.04673583433032036\n","Batch 550/833, Loss: 0.06393156945705414\n","Batch 560/833, Loss: 0.01889611780643463\n","Batch 570/833, Loss: 0.0002584890753496438\n","Batch 580/833, Loss: 0.0004984379047527909\n","Batch 590/833, Loss: 0.0009663535165600479\n","Batch 600/833, Loss: 0.0009834034135565162\n","Batch 610/833, Loss: 0.0002961357240565121\n","Batch 620/833, Loss: 0.0009022387093864381\n","Batch 630/833, Loss: 0.21817968785762787\n","Batch 640/833, Loss: 0.007602682337164879\n","Batch 650/833, Loss: 0.006353861652314663\n","Batch 660/833, Loss: 0.0012458798009902239\n","Batch 670/833, Loss: 0.002865373156964779\n","Batch 680/833, Loss: 0.004424929153174162\n","Batch 690/833, Loss: 0.001789973583072424\n","Batch 700/833, Loss: 0.0006955799181014299\n","Batch 710/833, Loss: 0.010021203197538853\n","Batch 720/833, Loss: 0.0004498015623539686\n","Batch 730/833, Loss: 0.0027068841736763716\n","Batch 740/833, Loss: 0.002979820827022195\n","Batch 750/833, Loss: 0.00956758763641119\n","Batch 760/833, Loss: 0.0029931666795164347\n","Batch 770/833, Loss: 0.008576595224440098\n","Batch 780/833, Loss: 0.0020592177752405405\n","Batch 790/833, Loss: 0.032100751996040344\n","Batch 800/833, Loss: 0.001208083238452673\n","Batch 810/833, Loss: 0.0006207695114426315\n","Batch 820/833, Loss: 0.0006802115822210908\n","Batch 830/833, Loss: 0.0006891349330544472\n","Training epoch completed in: 2m 16s\n","Train loss 0.024895347906185045 accuracy 0.9924947463224257\n","Batch 0/209, Test Loss: 0.05890517681837082\n","Batch 10/209, Test Loss: 0.666403591632843\n","Batch 20/209, Test Loss: 0.11414850503206253\n","Batch 30/209, Test Loss: 0.031055472791194916\n","Batch 40/209, Test Loss: 0.000597278238274157\n","Batch 50/209, Test Loss: 0.0008344858651980758\n","Batch 60/209, Test Loss: 0.3538590669631958\n","Batch 70/209, Test Loss: 0.1269943118095398\n","Batch 80/209, Test Loss: 0.002231825375929475\n","Batch 90/209, Test Loss: 0.0009677883354015648\n","Batch 100/209, Test Loss: 0.0005774153396487236\n","Batch 110/209, Test Loss: 0.004626816604286432\n","Batch 120/209, Test Loss: 0.01304764486849308\n","Batch 130/209, Test Loss: 0.0005250826943665743\n","Batch 140/209, Test Loss: 0.003039574483409524\n","Batch 150/209, Test Loss: 0.001754573080688715\n","Batch 160/209, Test Loss: 0.0033603606279939413\n","Batch 170/209, Test Loss: 0.006384542677551508\n","Batch 180/209, Test Loss: 0.4854860007762909\n","Batch 190/209, Test Loss: 0.0007095340988598764\n","Batch 200/209, Test Loss: 0.016965355724096298\n","Test evaluation completed in: 0m 11s\n","Test loss 0.09557428699813651 accuracy 0.9762905162064826\n","Test Precision: 0.9762684443119912\n","Test Recall: 0.9762905162064826\n","Test F1 Score: 0.9762784998755712\n","Starting epoch 4/10\n","Batch 0/833, Loss: 0.005388112738728523\n","Batch 10/833, Loss: 0.006605465896427631\n","Batch 20/833, Loss: 0.011742126196622849\n","Batch 30/833, Loss: 0.004793686792254448\n","Batch 40/833, Loss: 0.0013092581648379564\n","Batch 50/833, Loss: 0.008084232918918133\n","Batch 60/833, Loss: 0.12310099601745605\n","Batch 70/833, Loss: 0.0026062263641506433\n","Batch 80/833, Loss: 0.010278821922838688\n","Batch 90/833, Loss: 0.0021779288072139025\n","Batch 100/833, Loss: 0.001350986654870212\n","Batch 110/833, Loss: 0.002668278058990836\n","Batch 120/833, Loss: 0.0034708937164396048\n","Batch 130/833, Loss: 0.0029973958153277636\n","Batch 140/833, Loss: 0.0013936907052993774\n","Batch 150/833, Loss: 0.001336319255642593\n","Batch 160/833, Loss: 0.001292751869186759\n","Batch 170/833, Loss: 0.008190810680389404\n","Batch 180/833, Loss: 0.0008460006793029606\n","Batch 190/833, Loss: 0.0013019463513046503\n","Batch 200/833, Loss: 0.0014939584070816636\n","Batch 210/833, Loss: 0.0013050306588411331\n","Batch 220/833, Loss: 0.0004473140579648316\n","Batch 230/833, Loss: 0.0011572552612051368\n","Batch 240/833, Loss: 0.0005904489662498236\n","Batch 250/833, Loss: 0.0007546601700596511\n","Batch 260/833, Loss: 0.0010880534537136555\n","Batch 270/833, Loss: 0.0009403683361597359\n","Batch 280/833, Loss: 0.0011236759601160884\n","Batch 290/833, Loss: 0.0022681797854602337\n","Batch 300/833, Loss: 0.004997187294065952\n","Batch 310/833, Loss: 0.0012502986937761307\n","Batch 320/833, Loss: 0.13657213747501373\n","Batch 330/833, Loss: 0.01519414409995079\n","Batch 340/833, Loss: 0.0024822084233164787\n","Batch 350/833, Loss: 0.0013001584447920322\n","Batch 360/833, Loss: 0.020946288481354713\n","Batch 370/833, Loss: 0.0010725065367296338\n","Batch 380/833, Loss: 0.0013742689043283463\n","Batch 390/833, Loss: 0.0006691575981676579\n","Batch 400/833, Loss: 0.0009480941807851195\n","Batch 410/833, Loss: 0.0010925017995759845\n","Batch 420/833, Loss: 0.00041473988676443696\n","Batch 430/833, Loss: 0.0006563550559803843\n","Batch 440/833, Loss: 0.0005449711461551487\n","Batch 450/833, Loss: 0.000587770133279264\n","Batch 460/833, Loss: 0.0030995390843600035\n","Batch 470/833, Loss: 0.00028727040626108646\n","Batch 480/833, Loss: 0.021089179441332817\n","Batch 490/833, Loss: 0.0019655635114759207\n","Batch 500/833, Loss: 0.0007638889946974814\n","Batch 510/833, Loss: 0.19537687301635742\n","Batch 520/833, Loss: 0.007733069825917482\n","Batch 530/833, Loss: 0.0006495993584394455\n","Batch 540/833, Loss: 0.0008361818036064506\n","Batch 550/833, Loss: 0.0009375764057040215\n","Batch 560/833, Loss: 0.0036140326410531998\n","Batch 570/833, Loss: 0.0002831147867254913\n","Batch 580/833, Loss: 0.00041310483356937766\n","Batch 590/833, Loss: 0.0003354411164764315\n","Batch 600/833, Loss: 0.0007873901049606502\n","Batch 610/833, Loss: 0.0008209811057895422\n","Batch 620/833, Loss: 0.0019147212151437998\n","Batch 630/833, Loss: 0.0025313301011919975\n","Batch 640/833, Loss: 0.0009647472761571407\n","Batch 650/833, Loss: 0.0006419920828193426\n","Batch 660/833, Loss: 0.0016350098885595798\n","Batch 670/833, Loss: 0.0004615750804077834\n","Batch 680/833, Loss: 0.0008086676243692636\n","Batch 690/833, Loss: 0.001124831847846508\n","Batch 700/833, Loss: 0.0008108061738312244\n","Batch 710/833, Loss: 0.0005865986458957195\n","Batch 720/833, Loss: 0.00046923960326239467\n","Batch 730/833, Loss: 0.0011404405813664198\n","Batch 740/833, Loss: 0.0015997493173927069\n","Batch 750/833, Loss: 0.0005011110915802419\n","Batch 760/833, Loss: 0.10825438052415848\n","Batch 770/833, Loss: 0.0065628234297037125\n","Batch 780/833, Loss: 0.01081750076264143\n","Batch 790/833, Loss: 0.0015772538026794791\n","Batch 800/833, Loss: 0.0004999585798941553\n","Batch 810/833, Loss: 0.00039415041101165116\n","Batch 820/833, Loss: 0.03242476284503937\n","Batch 830/833, Loss: 0.0006838439730927348\n","Training epoch completed in: 2m 16s\n","Train loss 0.015120098477844115 accuracy 0.9958721104773341\n","Batch 0/209, Test Loss: 0.030129780992865562\n","Batch 10/209, Test Loss: 0.7503350377082825\n","Batch 20/209, Test Loss: 0.11973489075899124\n","Batch 30/209, Test Loss: 0.015778925269842148\n","Batch 40/209, Test Loss: 0.00021159044990781695\n","Batch 50/209, Test Loss: 0.0006513348780572414\n","Batch 60/209, Test Loss: 0.36131513118743896\n","Batch 70/209, Test Loss: 0.3296205997467041\n","Batch 80/209, Test Loss: 0.0004353310796432197\n","Batch 90/209, Test Loss: 0.012377072125673294\n","Batch 100/209, Test Loss: 0.000518171873409301\n","Batch 110/209, Test Loss: 0.004563010763376951\n","Batch 120/209, Test Loss: 0.03552462160587311\n","Batch 130/209, Test Loss: 0.0002209586964454502\n","Batch 140/209, Test Loss: 0.0014907526783645153\n","Batch 150/209, Test Loss: 0.0009159791516140103\n","Batch 160/209, Test Loss: 0.12252490222454071\n","Batch 170/209, Test Loss: 0.003981396555900574\n","Batch 180/209, Test Loss: 0.577856719493866\n","Batch 190/209, Test Loss: 0.004544809460639954\n","Batch 200/209, Test Loss: 0.04664596915245056\n","Test evaluation completed in: 0m 11s\n","Test loss 0.09942899535660205 accuracy 0.9765906362545017\n","Test Precision: 0.9766399900755969\n","Test Recall: 0.9765906362545018\n","Test F1 Score: 0.9764179040904089\n","Starting epoch 5/10\n","Batch 0/833, Loss: 0.0006261804373934865\n","Batch 10/833, Loss: 0.0019058688776567578\n","Batch 20/833, Loss: 0.003221178660169244\n","Batch 30/833, Loss: 0.0013291483046486974\n","Batch 40/833, Loss: 0.0011608493514358997\n","Batch 50/833, Loss: 0.07639792561531067\n","Batch 60/833, Loss: 0.0014153996016830206\n","Batch 70/833, Loss: 0.002074678661301732\n","Batch 80/833, Loss: 0.005314103327691555\n","Batch 90/833, Loss: 0.0016414497513324022\n","Batch 100/833, Loss: 0.0011112645734101534\n","Batch 110/833, Loss: 0.001323309144936502\n","Batch 120/833, Loss: 0.001108174561522901\n","Batch 130/833, Loss: 0.0013938713818788528\n","Batch 140/833, Loss: 0.0009827857138589025\n","Batch 150/833, Loss: 0.0005392449675127864\n","Batch 160/833, Loss: 0.005435006693005562\n","Batch 170/833, Loss: 0.0005491943447850645\n","Batch 180/833, Loss: 0.0008201503078453243\n","Batch 190/833, Loss: 0.0014110855991020799\n","Batch 200/833, Loss: 0.00046992048737592995\n","Batch 210/833, Loss: 0.0012544402852654457\n","Batch 220/833, Loss: 0.00044997292570769787\n","Batch 230/833, Loss: 0.12863561511039734\n","Batch 240/833, Loss: 0.000810775498393923\n","Batch 250/833, Loss: 0.000994339119642973\n","Batch 260/833, Loss: 0.006766416598111391\n","Batch 270/833, Loss: 0.00660467566922307\n","Batch 280/833, Loss: 0.0500614233314991\n","Batch 290/833, Loss: 0.0028051803819835186\n","Batch 300/833, Loss: 0.0009160703048110008\n","Batch 310/833, Loss: 0.0004393892886582762\n","Batch 320/833, Loss: 0.0005441862158477306\n","Batch 330/833, Loss: 0.0004571926547214389\n","Batch 340/833, Loss: 0.0026482734829187393\n","Batch 350/833, Loss: 0.0004470849235076457\n","Batch 360/833, Loss: 0.0015068160137161613\n","Batch 370/833, Loss: 0.0006061320891603827\n","Batch 380/833, Loss: 0.0004497737099882215\n","Batch 390/833, Loss: 0.00035321590257808566\n","Batch 400/833, Loss: 0.0005634887493215501\n","Batch 410/833, Loss: 0.0004468663828447461\n","Batch 420/833, Loss: 0.0002907837915699929\n","Batch 430/833, Loss: 0.0006301030516624451\n","Batch 440/833, Loss: 0.0012085196794942021\n","Batch 450/833, Loss: 0.0003578749019652605\n","Batch 460/833, Loss: 0.0003586534585338086\n","Batch 470/833, Loss: 0.0004174448549747467\n","Batch 480/833, Loss: 0.012733080424368382\n","Batch 490/833, Loss: 0.0006551890983246267\n","Batch 500/833, Loss: 0.0004969249130226672\n","Batch 510/833, Loss: 0.000631470640655607\n","Batch 520/833, Loss: 0.006442483514547348\n","Batch 530/833, Loss: 0.00035880276118405163\n","Batch 540/833, Loss: 0.0005677714943885803\n","Batch 550/833, Loss: 0.0005302451900206506\n","Batch 560/833, Loss: 0.011255691759288311\n","Batch 570/833, Loss: 0.00018019934941548854\n","Batch 580/833, Loss: 0.00021715932234656066\n","Batch 590/833, Loss: 0.00017386258696205914\n","Batch 600/833, Loss: 0.0001675983367022127\n","Batch 610/833, Loss: 0.0001423507637809962\n","Batch 620/833, Loss: 0.0001554442133056\n","Batch 630/833, Loss: 0.00027211266569793224\n","Batch 640/833, Loss: 0.0005411222809925675\n","Batch 650/833, Loss: 0.00013423328346107155\n","Batch 660/833, Loss: 0.0037422068417072296\n","Batch 670/833, Loss: 0.0003565903170965612\n","Batch 680/833, Loss: 0.00046042422764003277\n","Batch 690/833, Loss: 0.00038655634853057563\n","Batch 700/833, Loss: 0.00029129214817658067\n","Batch 710/833, Loss: 0.0007933024899102747\n","Batch 720/833, Loss: 0.00017404789105057716\n","Batch 730/833, Loss: 0.00021624348300974816\n","Batch 740/833, Loss: 0.0003333997156005353\n","Batch 750/833, Loss: 0.0003237918426748365\n","Batch 760/833, Loss: 0.0003245484840590507\n","Batch 770/833, Loss: 0.00035610454506240785\n","Batch 780/833, Loss: 0.00020936699002049863\n","Batch 790/833, Loss: 0.04263937473297119\n","Batch 800/833, Loss: 0.0003613478038460016\n","Batch 810/833, Loss: 0.000152479755342938\n","Batch 820/833, Loss: 0.00021658297919202596\n","Batch 830/833, Loss: 0.0001721001317491755\n","Training epoch completed in: 2m 16s\n","Train loss 0.008883605903062197 accuracy 0.997373161212849\n","Batch 0/209, Test Loss: 0.19557559490203857\n","Batch 10/209, Test Loss: 0.8831984400749207\n","Batch 20/209, Test Loss: 0.27702078223228455\n","Batch 30/209, Test Loss: 0.0002850327000487596\n","Batch 40/209, Test Loss: 0.00010619340901030228\n","Batch 50/209, Test Loss: 0.0001821701298467815\n","Batch 60/209, Test Loss: 0.5407291054725647\n","Batch 70/209, Test Loss: 0.38120147585868835\n","Batch 80/209, Test Loss: 0.0002922380226664245\n","Batch 90/209, Test Loss: 0.00012854360102210194\n","Batch 100/209, Test Loss: 0.00010429065150674433\n","Batch 110/209, Test Loss: 0.0004857612366322428\n","Batch 120/209, Test Loss: 0.002238453133031726\n","Batch 130/209, Test Loss: 7.775071571813896e-05\n","Batch 140/209, Test Loss: 0.0001370449608657509\n","Batch 150/209, Test Loss: 0.00013704503362532705\n","Batch 160/209, Test Loss: 0.010353902354836464\n","Batch 170/209, Test Loss: 0.013181613758206367\n","Batch 180/209, Test Loss: 0.6148137450218201\n","Batch 190/209, Test Loss: 0.00015328828885685652\n","Batch 200/209, Test Loss: 0.07243095338344574\n","Test evaluation completed in: 0m 11s\n","Test loss 0.1179634381016913 accuracy 0.976890756302521\n","Test Precision: 0.9768211201886914\n","Test Recall: 0.976890756302521\n","Test F1 Score: 0.9768072749045743\n","Starting epoch 6/10\n","Batch 0/833, Loss: 0.00021054779062978923\n","Batch 10/833, Loss: 0.0007941228104755282\n","Batch 20/833, Loss: 0.000979176489636302\n","Batch 30/833, Loss: 0.0002604372857604176\n","Batch 40/833, Loss: 0.00027755185146816075\n","Batch 50/833, Loss: 0.000441605196101591\n","Batch 60/833, Loss: 0.000538103049620986\n","Batch 70/833, Loss: 0.000803140108473599\n","Batch 80/833, Loss: 0.0015813616337254643\n","Batch 90/833, Loss: 0.001775427837856114\n","Batch 100/833, Loss: 0.0008924100548028946\n","Batch 110/833, Loss: 0.0009872869122773409\n","Batch 120/833, Loss: 0.011990487575531006\n","Batch 130/833, Loss: 0.5282661318778992\n","Batch 140/833, Loss: 0.0008018990629352629\n","Batch 150/833, Loss: 0.0009053356479853392\n","Batch 160/833, Loss: 0.0007561755483038723\n","Batch 170/833, Loss: 0.0009229119750671089\n","Batch 180/833, Loss: 0.000902767525985837\n","Batch 190/833, Loss: 0.007050991989672184\n","Batch 200/833, Loss: 0.0004501437651924789\n","Batch 210/833, Loss: 0.0008107195608317852\n","Batch 220/833, Loss: 0.00044733972754329443\n","Batch 230/833, Loss: 0.00045032717753201723\n","Batch 240/833, Loss: 0.0003918549045920372\n","Batch 250/833, Loss: 0.0004900167696177959\n","Batch 260/833, Loss: 0.0006000746507197618\n","Batch 270/833, Loss: 0.0004309504001867026\n","Batch 280/833, Loss: 0.0002984327438753098\n","Batch 290/833, Loss: 0.00046942324843257666\n","Batch 300/833, Loss: 0.0003780292463488877\n","Batch 310/833, Loss: 0.0002915354853030294\n","Batch 320/833, Loss: 0.0003655302571132779\n","Batch 330/833, Loss: 0.0007073970627970994\n","Batch 340/833, Loss: 0.000592312659136951\n","Batch 350/833, Loss: 0.0010317628039047122\n","Batch 360/833, Loss: 0.0010927377734333277\n","Batch 370/833, Loss: 0.0009128096862696111\n","Batch 380/833, Loss: 0.0016637537628412247\n","Batch 390/833, Loss: 0.0011850750306621194\n","Batch 400/833, Loss: 0.007507301867008209\n","Batch 410/833, Loss: 0.0007876202580519021\n","Batch 420/833, Loss: 0.00063746003434062\n","Batch 430/833, Loss: 0.0007042211946099997\n","Batch 440/833, Loss: 0.0004996413481421769\n","Batch 450/833, Loss: 0.0005378885543905199\n","Batch 460/833, Loss: 0.0010634391801431775\n","Batch 470/833, Loss: 0.00040199648356065154\n","Batch 480/833, Loss: 0.00045671279076486826\n","Batch 490/833, Loss: 0.0017711521359160542\n","Batch 500/833, Loss: 0.00039310529245994985\n","Batch 510/833, Loss: 0.0014276172732934356\n","Batch 520/833, Loss: 0.0006551764672622085\n","Batch 530/833, Loss: 0.014581610448658466\n","Batch 540/833, Loss: 0.0005331942229531705\n","Batch 550/833, Loss: 0.034103382378816605\n","Batch 560/833, Loss: 0.004808699246495962\n","Batch 570/833, Loss: 0.00033412923221476376\n","Batch 580/833, Loss: 0.0002753756707534194\n","Batch 590/833, Loss: 0.00034430588129907846\n","Batch 600/833, Loss: 0.0016389715019613504\n","Batch 610/833, Loss: 0.0002879723033402115\n","Batch 620/833, Loss: 0.0003191371797583997\n","Batch 630/833, Loss: 0.00044834052096121013\n","Batch 640/833, Loss: 0.0030773086473345757\n","Batch 650/833, Loss: 0.0004295716353226453\n","Batch 660/833, Loss: 0.0005570256616920233\n","Batch 670/833, Loss: 0.0002691709960345179\n","Batch 680/833, Loss: 0.00046172624570317566\n","Batch 690/833, Loss: 0.002128932625055313\n","Batch 700/833, Loss: 0.00034111650893464684\n","Batch 710/833, Loss: 0.0006200454663485289\n","Batch 720/833, Loss: 0.00028594129253178835\n","Batch 730/833, Loss: 0.0006857920670881867\n","Batch 740/833, Loss: 0.0002886757720261812\n","Batch 750/833, Loss: 0.059600941836833954\n","Batch 760/833, Loss: 0.0010923779336735606\n","Batch 770/833, Loss: 0.000723819830454886\n","Batch 780/833, Loss: 0.006307907868176699\n","Batch 790/833, Loss: 0.0007760292501188815\n","Batch 800/833, Loss: 0.0006570236873812973\n","Batch 810/833, Loss: 0.00048452988266944885\n","Batch 820/833, Loss: 0.00037003131001256406\n","Batch 830/833, Loss: 0.0003477674617897719\n","Training epoch completed in: 2m 16s\n","Train loss 0.007474486033732588 accuracy 0.9983488441909336\n","Batch 0/209, Test Loss: 0.11599569022655487\n","Batch 10/209, Test Loss: 1.173427700996399\n","Batch 20/209, Test Loss: 0.21996799111366272\n","Batch 30/209, Test Loss: 0.003778928890824318\n","Batch 40/209, Test Loss: 0.0002231193211628124\n","Batch 50/209, Test Loss: 0.00047951441956683993\n","Batch 60/209, Test Loss: 0.7624713182449341\n","Batch 70/209, Test Loss: 0.5283507108688354\n","Batch 80/209, Test Loss: 0.00021341345563996583\n","Batch 90/209, Test Loss: 0.0010000239126384258\n","Batch 100/209, Test Loss: 0.00022006378276273608\n","Batch 110/209, Test Loss: 0.0007141660898923874\n","Batch 120/209, Test Loss: 0.007531726732850075\n","Batch 130/209, Test Loss: 0.00017756836314219981\n","Batch 140/209, Test Loss: 0.001328837824985385\n","Batch 150/209, Test Loss: 0.00026370721752755344\n","Batch 160/209, Test Loss: 0.1249065026640892\n","Batch 170/209, Test Loss: 0.0023480779491364956\n","Batch 180/209, Test Loss: 0.5434097647666931\n","Batch 190/209, Test Loss: 0.02340000867843628\n","Batch 200/209, Test Loss: 0.00308538437820971\n","Test evaluation completed in: 0m 11s\n","Test loss 0.12500835777734937 accuracy 0.9771908763505401\n","Test Precision: 0.9771647844784713\n","Test Recall: 0.9771908763505402\n","Test F1 Score: 0.9770639248451686\n","Starting epoch 7/10\n","Batch 0/833, Loss: 0.00031149742426350713\n","Batch 10/833, Loss: 0.00024789583403617144\n","Batch 20/833, Loss: 0.00039653878775425255\n","Batch 30/833, Loss: 0.0002448420855216682\n","Batch 40/833, Loss: 0.00024703392409719527\n","Batch 50/833, Loss: 0.0003680487861856818\n","Batch 60/833, Loss: 0.00034709915053099394\n","Batch 70/833, Loss: 0.0004024712834507227\n","Batch 80/833, Loss: 0.0005626834463328123\n","Batch 90/833, Loss: 0.0004953539464622736\n","Batch 100/833, Loss: 0.0004140164819546044\n","Batch 110/833, Loss: 0.0007455051527358592\n","Batch 120/833, Loss: 0.0004993047914467752\n","Batch 130/833, Loss: 0.0005125809693709016\n","Batch 140/833, Loss: 0.2805297076702118\n","Batch 150/833, Loss: 0.0005302925710566342\n","Batch 160/833, Loss: 0.0007035391754470766\n","Batch 170/833, Loss: 0.002307479502633214\n","Batch 180/833, Loss: 0.0006962408078834414\n","Batch 190/833, Loss: 0.004842634312808514\n","Batch 200/833, Loss: 0.000390003522625193\n","Batch 210/833, Loss: 0.0042403400875627995\n","Batch 220/833, Loss: 0.0004542226670309901\n","Batch 230/833, Loss: 0.000557161052711308\n","Batch 240/833, Loss: 0.0003507800865918398\n","Batch 250/833, Loss: 0.000509253703057766\n","Batch 260/833, Loss: 0.00037211101152934134\n","Batch 270/833, Loss: 0.00046410609502345324\n","Batch 280/833, Loss: 0.00029053783509880304\n","Batch 290/833, Loss: 0.0005587696796283126\n","Batch 300/833, Loss: 0.0005369107821024954\n","Batch 310/833, Loss: 0.00032194724190048873\n","Batch 320/833, Loss: 0.00024650554405525327\n","Batch 330/833, Loss: 0.00046693571493960917\n","Batch 340/833, Loss: 0.0027691044379025698\n","Batch 350/833, Loss: 0.0003260956145823002\n","Batch 360/833, Loss: 0.00029182169237174094\n","Batch 370/833, Loss: 0.00029108751914463937\n","Batch 380/833, Loss: 0.00028132658917456865\n","Batch 390/833, Loss: 0.0002464811550453305\n","Batch 400/833, Loss: 0.00047812997945584357\n","Batch 410/833, Loss: 0.00028743199072778225\n","Batch 420/833, Loss: 0.0003331101033836603\n","Batch 430/833, Loss: 0.00024686241522431374\n","Batch 440/833, Loss: 0.0003173902223352343\n","Batch 450/833, Loss: 0.0002134640672011301\n","Batch 460/833, Loss: 0.00044114989577792585\n","Batch 470/833, Loss: 0.00018249920685775578\n","Batch 480/833, Loss: 0.00029497273499146104\n","Batch 490/833, Loss: 0.015487182885408401\n","Batch 500/833, Loss: 0.0002599068684503436\n","Batch 510/833, Loss: 0.0005487200687639415\n","Batch 520/833, Loss: 0.0003178933111485094\n","Batch 530/833, Loss: 0.00023593298101332039\n","Batch 540/833, Loss: 0.0001828325039241463\n","Batch 550/833, Loss: 0.00021561312314588577\n","Batch 560/833, Loss: 0.00024811754701659083\n","Batch 570/833, Loss: 0.00017551184282638133\n","Batch 580/833, Loss: 0.00019853873527608812\n","Batch 590/833, Loss: 0.0003155591257382184\n","Batch 600/833, Loss: 0.0002580047002993524\n","Batch 610/833, Loss: 0.00022859885939396918\n","Batch 620/833, Loss: 0.0001650669873924926\n","Batch 630/833, Loss: 0.00022778355923946947\n","Batch 640/833, Loss: 0.0002197782159782946\n","Batch 650/833, Loss: 0.00017797543841879815\n","Batch 660/833, Loss: 0.00018473363888915628\n","Batch 670/833, Loss: 0.00016673326899763197\n","Batch 680/833, Loss: 0.00975942611694336\n","Batch 690/833, Loss: 0.06577025353908539\n","Batch 700/833, Loss: 0.0006252534803934395\n","Batch 710/833, Loss: 0.0002157977141905576\n","Batch 720/833, Loss: 0.0001854851288953796\n","Batch 730/833, Loss: 0.00027508323546499014\n","Batch 740/833, Loss: 0.0002543468144722283\n","Batch 750/833, Loss: 0.00022440605971496552\n","Batch 760/833, Loss: 0.0001930455182446167\n","Batch 770/833, Loss: 0.00019365761545486748\n","Batch 780/833, Loss: 0.0003850408538710326\n","Batch 790/833, Loss: 0.00024723808746784925\n","Batch 800/833, Loss: 0.0003652013838291168\n","Batch 810/833, Loss: 0.00017621018923819065\n","Batch 820/833, Loss: 0.00020737167506013066\n","Batch 830/833, Loss: 0.0002188117359764874\n","Training epoch completed in: 2m 16s\n","Train loss 0.005641946365463123 accuracy 0.9984238967277094\n","Batch 0/209, Test Loss: 0.16182927787303925\n","Batch 10/209, Test Loss: 1.4575998783111572\n","Batch 20/209, Test Loss: 0.19205230474472046\n","Batch 30/209, Test Loss: 0.0005309508414939046\n","Batch 40/209, Test Loss: 0.00013473430590238422\n","Batch 50/209, Test Loss: 0.0001405507791787386\n","Batch 60/209, Test Loss: 1.0381911993026733\n","Batch 70/209, Test Loss: 0.5112017393112183\n","Batch 80/209, Test Loss: 0.00014032129547558725\n","Batch 90/209, Test Loss: 0.0001394260034430772\n","Batch 100/209, Test Loss: 0.00014233987894840539\n","Batch 110/209, Test Loss: 0.00016359543951693922\n","Batch 120/209, Test Loss: 0.02967303991317749\n","Batch 130/209, Test Loss: 0.00015162462659645826\n","Batch 140/209, Test Loss: 0.00012569788668770343\n","Batch 150/209, Test Loss: 0.00014048883167561144\n","Batch 160/209, Test Loss: 0.0012684622779488564\n","Batch 170/209, Test Loss: 0.02139321342110634\n","Batch 180/209, Test Loss: 0.5493476390838623\n","Batch 190/209, Test Loss: 0.0012922800378873944\n","Batch 200/209, Test Loss: 0.00030069128843024373\n","Test evaluation completed in: 0m 11s\n","Test loss 0.1310179347034728 accuracy 0.9756902761104441\n","Test Precision: 0.9756807076572126\n","Test Recall: 0.9756902761104442\n","Test F1 Score: 0.9755330448273494\n","Starting epoch 8/10\n","Batch 0/833, Loss: 0.0003552304988261312\n","Batch 10/833, Loss: 0.0047927252016961575\n","Batch 20/833, Loss: 0.00045941973803564906\n","Batch 30/833, Loss: 0.00021021881548222154\n","Batch 40/833, Loss: 0.0001999560190597549\n","Batch 50/833, Loss: 0.0003743103879969567\n","Batch 60/833, Loss: 0.0003050028462894261\n","Batch 70/833, Loss: 0.0005758146871812642\n","Batch 80/833, Loss: 0.0005355838220566511\n","Batch 90/833, Loss: 0.0005183902685530484\n","Batch 100/833, Loss: 0.0004468844854272902\n","Batch 110/833, Loss: 0.0010464047081768513\n","Batch 120/833, Loss: 0.0005637001013383269\n","Batch 130/833, Loss: 0.0007697638939134777\n","Batch 140/833, Loss: 0.0004215073131490499\n","Batch 150/833, Loss: 0.0004907631664536893\n","Batch 160/833, Loss: 0.00040987745160236955\n","Batch 170/833, Loss: 0.08885158598423004\n","Batch 180/833, Loss: 0.00036011997144669294\n","Batch 190/833, Loss: 0.0007566485437564552\n","Batch 200/833, Loss: 0.0002921802515629679\n","Batch 210/833, Loss: 0.0005647459765896201\n","Batch 220/833, Loss: 0.00030803371919319034\n","Batch 230/833, Loss: 0.00027545372722670436\n","Batch 240/833, Loss: 0.00024523813044652343\n","Batch 250/833, Loss: 0.0003349116013851017\n","Batch 260/833, Loss: 0.0012715653283521533\n","Batch 270/833, Loss: 0.00029016242478974164\n","Batch 280/833, Loss: 0.0002449389430694282\n","Batch 290/833, Loss: 0.0003227845299988985\n","Batch 300/833, Loss: 0.000268740754108876\n","Batch 310/833, Loss: 0.0002847450377885252\n","Batch 320/833, Loss: 0.0001897153997560963\n","Batch 330/833, Loss: 0.000317600613925606\n","Batch 340/833, Loss: 0.0006053015822544694\n","Batch 350/833, Loss: 0.0003103420021943748\n","Batch 360/833, Loss: 0.00023283936025109142\n","Batch 370/833, Loss: 0.00020862117526121438\n","Batch 380/833, Loss: 0.00035228912020102143\n","Batch 390/833, Loss: 0.0001755767734721303\n","Batch 400/833, Loss: 0.00022718672698829323\n","Batch 410/833, Loss: 0.00017258385196328163\n","Batch 420/833, Loss: 0.0001887771359179169\n","Batch 430/833, Loss: 0.0002126314939232543\n","Batch 440/833, Loss: 0.00024294991453643888\n","Batch 450/833, Loss: 0.0001897916226880625\n","Batch 460/833, Loss: 0.0004210078332107514\n","Batch 470/833, Loss: 0.00017112343630287796\n","Batch 480/833, Loss: 0.002814249601215124\n","Batch 490/833, Loss: 0.00034076362499035895\n","Batch 500/833, Loss: 0.0001908482372527942\n","Batch 510/833, Loss: 0.0006078424630686641\n","Batch 520/833, Loss: 0.004063205327838659\n","Batch 530/833, Loss: 0.00020950382167939097\n","Batch 540/833, Loss: 0.00017703686899039894\n","Batch 550/833, Loss: 0.00027136827702634037\n","Batch 560/833, Loss: 0.00034493260318413377\n","Batch 570/833, Loss: 0.00017567217582836747\n","Batch 580/833, Loss: 0.00024032947840169072\n","Batch 590/833, Loss: 0.00017169801867567003\n","Batch 600/833, Loss: 0.0004097915953025222\n","Batch 610/833, Loss: 0.0001841044140746817\n","Batch 620/833, Loss: 0.00016327192133758217\n","Batch 630/833, Loss: 0.00017251026292797178\n","Batch 640/833, Loss: 0.0001819156459532678\n","Batch 650/833, Loss: 0.00036460914998315275\n","Batch 660/833, Loss: 0.00014478372759185731\n","Batch 670/833, Loss: 0.00012592652637977153\n","Batch 680/833, Loss: 0.00014363983063958585\n","Batch 690/833, Loss: 0.00017240199667867273\n","Batch 700/833, Loss: 0.00016607518773525953\n","Batch 710/833, Loss: 0.0002632542164064944\n","Batch 720/833, Loss: 0.00014994238154031336\n","Batch 730/833, Loss: 0.00021341914543882012\n","Batch 740/833, Loss: 0.00016027460515033454\n","Batch 750/833, Loss: 0.0001522613747511059\n","Batch 760/833, Loss: 0.000169840466696769\n","Batch 770/833, Loss: 0.00015216536121442914\n","Batch 780/833, Loss: 0.00022598254145123065\n","Batch 790/833, Loss: 0.00018352489860262722\n","Batch 800/833, Loss: 0.00015181093476712704\n","Batch 810/833, Loss: 0.00013505303650163114\n","Batch 820/833, Loss: 0.0002544174203649163\n","Batch 830/833, Loss: 0.0001387775846524164\n","Training epoch completed in: 2m 16s\n","Train loss 0.0035743600278206745 accuracy 0.999099369558691\n","Batch 0/209, Test Loss: 0.2774830162525177\n","Batch 10/209, Test Loss: 1.1881201267242432\n","Batch 20/209, Test Loss: 0.4225076735019684\n","Batch 30/209, Test Loss: 0.0004067063855472952\n","Batch 40/209, Test Loss: 0.00011207169154658914\n","Batch 50/209, Test Loss: 0.00010362367902416736\n","Batch 60/209, Test Loss: 0.9889851808547974\n","Batch 70/209, Test Loss: 0.5850546360015869\n","Batch 80/209, Test Loss: 0.00010753569222288206\n","Batch 90/209, Test Loss: 0.00010357938299421221\n","Batch 100/209, Test Loss: 0.00010631391342030838\n","Batch 110/209, Test Loss: 0.00013209780445322394\n","Batch 120/209, Test Loss: 0.017999371513724327\n","Batch 130/209, Test Loss: 0.0001182453052024357\n","Batch 140/209, Test Loss: 9.619700722396374e-05\n","Batch 150/209, Test Loss: 0.00010909784032264724\n","Batch 160/209, Test Loss: 0.0025082125794142485\n","Batch 170/209, Test Loss: 0.008285514079034328\n","Batch 180/209, Test Loss: 0.5730193853378296\n","Batch 190/209, Test Loss: 0.0002877767838072032\n","Batch 200/209, Test Loss: 0.00023200578289106488\n","Test evaluation completed in: 0m 11s\n","Test loss 0.13000475156902025 accuracy 0.976890756302521\n","Test Precision: 0.976855346575309\n","Test Recall: 0.976890756302521\n","Test F1 Score: 0.9767662787576544\n","Starting epoch 9/10\n","Batch 0/833, Loss: 0.00013254299119580537\n","Batch 10/833, Loss: 0.00016061094356700778\n","Batch 20/833, Loss: 0.0004339333681855351\n","Batch 30/833, Loss: 0.0002189822553191334\n","Batch 40/833, Loss: 0.00015401955170091242\n","Batch 50/833, Loss: 0.00014072982594370842\n","Batch 60/833, Loss: 0.00018555177666712552\n","Batch 70/833, Loss: 0.00018118711886927485\n","Batch 80/833, Loss: 0.0003046528436243534\n","Batch 90/833, Loss: 0.0003356008674018085\n","Batch 100/833, Loss: 0.000233455968555063\n","Batch 110/833, Loss: 0.00029879072098992765\n","Batch 120/833, Loss: 0.00027391838375478983\n","Batch 130/833, Loss: 0.00024388106248807162\n","Batch 140/833, Loss: 0.00028191597084514797\n","Batch 150/833, Loss: 0.0003044698969461024\n","Batch 160/833, Loss: 0.00025524231023155153\n","Batch 170/833, Loss: 0.00030655201408080757\n","Batch 180/833, Loss: 0.00032379417098127306\n","Batch 190/833, Loss: 0.00030624886858277023\n","Batch 200/833, Loss: 0.0002135657996404916\n","Batch 210/833, Loss: 0.00029295191052369773\n","Batch 220/833, Loss: 0.00026936159702017903\n","Batch 230/833, Loss: 0.00029314079438336194\n","Batch 240/833, Loss: 0.00014958663086872548\n","Batch 250/833, Loss: 0.000323992659104988\n","Batch 260/833, Loss: 0.00018648071272764355\n","Batch 270/833, Loss: 0.00021025219757575542\n","Batch 280/833, Loss: 0.00016843221965245903\n","Batch 290/833, Loss: 0.0002895419020205736\n","Batch 300/833, Loss: 0.00022562913363799453\n","Batch 310/833, Loss: 0.00022503074433188885\n","Batch 320/833, Loss: 0.0001873601577244699\n","Batch 330/833, Loss: 0.00024594555725343525\n","Batch 340/833, Loss: 0.00020943394338246435\n","Batch 350/833, Loss: 0.00015288750000763685\n","Batch 360/833, Loss: 0.0003834444214589894\n","Batch 370/833, Loss: 0.00016895306180231273\n","Batch 380/833, Loss: 0.00017171794024761766\n","Batch 390/833, Loss: 0.00016400098684243858\n","Batch 400/833, Loss: 0.000203074116143398\n","Batch 410/833, Loss: 0.00019643595442175865\n","Batch 420/833, Loss: 0.00017731374828144908\n","Batch 430/833, Loss: 0.0002515512751415372\n","Batch 440/833, Loss: 0.00016701772983651608\n","Batch 450/833, Loss: 0.0001668391632847488\n","Batch 460/833, Loss: 0.00021648778056260198\n","Batch 470/833, Loss: 0.00014048411685507745\n","Batch 480/833, Loss: 0.0004466651298571378\n","Batch 490/833, Loss: 0.0002069246838800609\n","Batch 500/833, Loss: 0.00014758175530005246\n","Batch 510/833, Loss: 0.00015339173842221498\n","Batch 520/833, Loss: 0.00044747404172085226\n","Batch 530/833, Loss: 0.00021539165754802525\n","Batch 540/833, Loss: 0.00016318743291776627\n","Batch 550/833, Loss: 0.0006766798906028271\n","Batch 560/833, Loss: 0.0003006101178470999\n","Batch 570/833, Loss: 0.00016541760123800486\n","Batch 580/833, Loss: 0.0001423890353180468\n","Batch 590/833, Loss: 0.00013509012933354825\n","Batch 600/833, Loss: 0.0010612619807943702\n","Batch 610/833, Loss: 0.00016053834406193346\n","Batch 620/833, Loss: 0.00015537600847892463\n","Batch 630/833, Loss: 0.0001537579228170216\n","Batch 640/833, Loss: 0.00018078007269650698\n","Batch 650/833, Loss: 0.00015323108527809381\n","Batch 660/833, Loss: 0.0001521451777080074\n","Batch 670/833, Loss: 0.0001274253154406324\n","Batch 680/833, Loss: 0.00024058166309259832\n","Batch 690/833, Loss: 0.0001477235637139529\n","Batch 700/833, Loss: 0.0001387034571962431\n","Batch 710/833, Loss: 0.0001700342691037804\n","Batch 720/833, Loss: 0.00020180635328870267\n","Batch 730/833, Loss: 0.0007186034927144647\n","Batch 740/833, Loss: 0.00020914652850478888\n","Batch 750/833, Loss: 0.00015565945068374276\n","Batch 760/833, Loss: 0.00018249366257805377\n","Batch 770/833, Loss: 0.00012362551933620125\n","Batch 780/833, Loss: 0.0001586855942150578\n","Batch 790/833, Loss: 0.00013670789485331625\n","Batch 800/833, Loss: 0.00033337323111481965\n","Batch 810/833, Loss: 0.0001432254648534581\n","Batch 820/833, Loss: 0.00017302889318671077\n","Batch 830/833, Loss: 0.00011898444063263014\n","Training epoch completed in: 2m 16s\n","Train loss 0.002169622407405705 accuracy 0.9996247373161213\n","Batch 0/209, Test Loss: 0.324311226606369\n","Batch 10/209, Test Loss: 1.1817444562911987\n","Batch 20/209, Test Loss: 0.508356511592865\n","Batch 30/209, Test Loss: 0.01536900456994772\n","Batch 40/209, Test Loss: 0.00011629620712483302\n","Batch 50/209, Test Loss: 9.751503239385784e-05\n","Batch 60/209, Test Loss: 1.0409635305404663\n","Batch 70/209, Test Loss: 0.5838946104049683\n","Batch 80/209, Test Loss: 0.00011453099432401359\n","Batch 90/209, Test Loss: 0.00010880906484089792\n","Batch 100/209, Test Loss: 0.00010933091107290238\n","Batch 110/209, Test Loss: 0.00012078607687726617\n","Batch 120/209, Test Loss: 0.01115155965089798\n","Batch 130/209, Test Loss: 0.00011201253801118582\n","Batch 140/209, Test Loss: 0.00010140423546545208\n","Batch 150/209, Test Loss: 0.00011087089660577476\n","Batch 160/209, Test Loss: 0.0013656191295012832\n","Batch 170/209, Test Loss: 0.006681487895548344\n","Batch 180/209, Test Loss: 0.5727906823158264\n","Batch 190/209, Test Loss: 0.00013517779007088393\n","Batch 200/209, Test Loss: 0.00016226322622969747\n","Test evaluation completed in: 0m 11s\n","Test loss 0.1386353529270966 accuracy 0.978391356542617\n","Test Precision: 0.9783471465690003\n","Test Recall: 0.978391356542617\n","Test F1 Score: 0.9782942238441651\n","Starting epoch 10/10\n","Batch 0/833, Loss: 0.00014905392890796065\n","Batch 10/833, Loss: 0.00016435766883660108\n","Batch 20/833, Loss: 0.0004831546393688768\n","Batch 30/833, Loss: 0.000219768364331685\n","Batch 40/833, Loss: 0.0001647114404477179\n","Batch 50/833, Loss: 0.0001398360909661278\n","Batch 60/833, Loss: 0.00017524352006148547\n","Batch 70/833, Loss: 0.00020525464788079262\n","Batch 80/833, Loss: 0.00021276692859828472\n","Batch 90/833, Loss: 0.0002791434235405177\n","Batch 100/833, Loss: 0.0001755531266098842\n","Batch 110/833, Loss: 0.0002692126727197319\n","Batch 120/833, Loss: 0.0001975815393961966\n","Batch 130/833, Loss: 0.0001863835786934942\n","Batch 140/833, Loss: 0.00019953973242081702\n","Batch 150/833, Loss: 0.00019198021618649364\n","Batch 160/833, Loss: 0.00021793117048218846\n","Batch 170/833, Loss: 0.00020852411398664117\n","Batch 180/833, Loss: 0.00018920734873972833\n","Batch 190/833, Loss: 0.000232654667343013\n","Batch 200/833, Loss: 0.0001735207042656839\n","Batch 210/833, Loss: 0.0004005774389952421\n","Batch 220/833, Loss: 0.00025509283295832574\n","Batch 230/833, Loss: 0.00017662832397036254\n","Batch 240/833, Loss: 0.00014977843966335058\n","Batch 250/833, Loss: 0.00019903000793419778\n","Batch 260/833, Loss: 0.0001536530180601403\n","Batch 270/833, Loss: 0.00018342153634876013\n","Batch 280/833, Loss: 0.00021623019711114466\n","Batch 290/833, Loss: 0.0002941284910775721\n","Batch 300/833, Loss: 0.00017418885545339435\n","Batch 310/833, Loss: 0.00017913844203576446\n","Batch 320/833, Loss: 0.00017220235895365477\n","Batch 330/833, Loss: 0.000161251678946428\n","Batch 340/833, Loss: 0.0002578109269961715\n","Batch 350/833, Loss: 0.00014544607256539166\n","Batch 360/833, Loss: 0.00017651461530476809\n","Batch 370/833, Loss: 0.00017286985530517995\n","Batch 380/833, Loss: 0.00018679808999877423\n","Batch 390/833, Loss: 0.0001522152597317472\n","Batch 400/833, Loss: 0.00015715553308837116\n","Batch 410/833, Loss: 0.0002546454779803753\n","Batch 420/833, Loss: 0.00015000511484686285\n","Batch 430/833, Loss: 0.00019763177260756493\n","Batch 440/833, Loss: 0.0001542258105473593\n","Batch 450/833, Loss: 0.00017033227777574211\n","Batch 460/833, Loss: 0.00024264046805910766\n","Batch 470/833, Loss: 0.00014595655375160277\n","Batch 480/833, Loss: 0.00017956637020688504\n","Batch 490/833, Loss: 0.00018376095977146178\n","Batch 500/833, Loss: 0.00016413300181739032\n","Batch 510/833, Loss: 0.0002163705648854375\n","Batch 520/833, Loss: 0.0005164359463378787\n","Batch 530/833, Loss: 0.00025527889374643564\n","Batch 540/833, Loss: 0.0001333393156528473\n","Batch 550/833, Loss: 0.00017837095947470516\n","Batch 560/833, Loss: 0.00032348872628062963\n","Batch 570/833, Loss: 0.0001564916456118226\n","Batch 580/833, Loss: 0.00018220454512629658\n","Batch 590/833, Loss: 0.0001502942614024505\n","Batch 600/833, Loss: 0.0005671605467796326\n","Batch 610/833, Loss: 0.00017532569472678006\n","Batch 620/833, Loss: 0.00016158082871697843\n","Batch 630/833, Loss: 0.00018592992273624986\n","Batch 640/833, Loss: 0.00018108944641426206\n","Batch 650/833, Loss: 0.00021327637659851462\n","Batch 660/833, Loss: 0.00013441979535855353\n","Batch 670/833, Loss: 0.00015025623724795878\n","Batch 680/833, Loss: 0.0002082960563711822\n","Batch 690/833, Loss: 0.00018162406922783703\n","Batch 700/833, Loss: 0.00014520715922117233\n","Batch 710/833, Loss: 0.00026720232563093305\n","Batch 720/833, Loss: 0.00019307294860482216\n","Batch 730/833, Loss: 0.00015167890524026006\n","Batch 740/833, Loss: 0.00020798700279556215\n","Batch 750/833, Loss: 0.00016337650595232844\n","Batch 760/833, Loss: 0.00017432370805181563\n","Batch 770/833, Loss: 0.00017118007235694677\n","Batch 780/833, Loss: 0.00023319340834859759\n","Batch 790/833, Loss: 0.0001826164807425812\n","Batch 800/833, Loss: 0.0002460600808262825\n","Batch 810/833, Loss: 0.00015992653788998723\n","Batch 820/833, Loss: 0.00017750170081853867\n","Batch 830/833, Loss: 0.0001571993198012933\n","Training epoch completed in: 2m 16s\n","Train loss 0.0019779575171507 accuracy 0.9996997898528971\n","Batch 0/209, Test Loss: 0.19293178617954254\n","Batch 10/209, Test Loss: 1.3417948484420776\n","Batch 20/209, Test Loss: 0.6723764538764954\n","Batch 30/209, Test Loss: 0.09953223913908005\n","Batch 40/209, Test Loss: 0.00014373328303918242\n","Batch 50/209, Test Loss: 0.00011415006883908063\n","Batch 60/209, Test Loss: 1.087358832359314\n","Batch 70/209, Test Loss: 0.5619611740112305\n","Batch 80/209, Test Loss: 0.0001442249777028337\n","Batch 90/209, Test Loss: 0.00013592532195616513\n","Batch 100/209, Test Loss: 0.00013655901420861483\n","Batch 110/209, Test Loss: 0.00017768022371456027\n","Batch 120/209, Test Loss: 0.0530543252825737\n","Batch 130/209, Test Loss: 0.0001400304026901722\n","Batch 140/209, Test Loss: 0.00012842382420785725\n","Batch 150/209, Test Loss: 0.00012997201702091843\n","Batch 160/209, Test Loss: 0.008262590505182743\n","Batch 170/209, Test Loss: 0.0019107998814433813\n","Batch 180/209, Test Loss: 0.5586509704589844\n","Batch 190/209, Test Loss: 0.0005861577228643\n","Batch 200/209, Test Loss: 0.000690164917614311\n","Test evaluation completed in: 0m 11s\n","Test loss 0.14870453588023272 accuracy 0.9756902761104441\n","Test Precision: 0.9757361514058124\n","Test Recall: 0.9756902761104442\n","Test F1 Score: 0.9755064453762288\n"]}],"source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split\n","import time\n","import traceback\n","\n","class ClickbaitDataset(Dataset):\n","    def __init__(self, titles, labels, tokenizer, max_len):\n","        self.titles = titles\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.titles)\n","\n","    def __getitem__(self, idx):\n","        title = self.titles[idx]\n","        label = self.labels[idx]\n","\n","        # Ensure the label is a valid integer\n","        try:\n","            label = int(label)\n","        except ValueError:\n","            raise ValueError(f\"Label {label} at index {idx} is not a valid integer.\")\n","\n","        encoding = self.tokenizer.encode_plus(\n","            title,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'title_text': title,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","def create_data_loader(titles, labels, tokenizer, max_len, batch_size):\n","    ds = ClickbaitDataset(\n","        titles=titles,\n","        labels=labels,\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","\n","    return DataLoader(ds, batch_size=batch_size, num_workers=4, pin_memory=True)\n","\n","def train_epoch(\n","    model,\n","    data_loader,\n","    loss_fn,\n","    optimizer,\n","    device,\n","    scheduler,\n","    n_examples\n","):\n","    model.train()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    for batch_idx, d in enumerate(data_loader):\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        labels = d[\"labels\"].to(device)\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        _, preds = torch.max(outputs.logits, dim=1)\n","        loss = loss_fn(outputs.logits, labels)\n","\n","        correct_predictions += torch.sum(preds == labels)\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        if batch_idx % 10 == 0:\n","            print(f'Batch {batch_idx}/{len(data_loader)}, Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Training epoch completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model.eval()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    all_labels = []\n","    all_preds = []\n","\n","    with torch.no_grad():\n","        for batch_idx, d in enumerate(data_loader):\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            labels = d[\"labels\"].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            loss = loss_fn(outputs.logits, labels)\n","\n","            correct_predictions += torch.sum(preds == labels)\n","            losses.append(loss.item())\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","\n","            if batch_idx % 10 == 0:\n","                print(f'Batch {batch_idx}/{len(data_loader)}, Test Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Test evaluation completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses), all_labels, all_preds\n","\n","def clean_dataset(df):\n","    # Remove rows where labels are NaN\n","    df = df.dropna(subset=['label'])\n","    # Convert labels to integers, and filter out rows where this conversion fails\n","    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n","    df = df.dropna(subset=['label'])\n","    df['label'] = df['label'].astype(int)\n","    return df\n","\n","def main():\n","    print(\"Starting main process\")\n","\n","    try:\n","        print('Loading the dataset.')\n","        df = pd.read_csv('/content/drive/MyDrive/clickbait/chinese.csv')\n","        print('Dataset loaded successfully.')\n","\n","        print('Cleaning the dataset.')\n","        df = clean_dataset(df)\n","        print('Dataset cleaned.')\n","\n","        print('Splitting the dataset into training and test sets.')\n","        df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n","        print('Dataset split into training and test sets.')\n","\n","        RANDOM_SEED = 42\n","        MAX_LEN = 128\n","        BATCH_SIZE = 16\n","        EPOCHS = 10\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f'Using device: {device}')\n","\n","        print('Loading the tokenizer and model.')\n","        tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n","        model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=2)\n","        model = model.to(device)\n","        print('Model and tokenizer loaded and moved to device.')\n","\n","        print('Creating data loaders.')\n","        train_titles = df_train['title'].tolist()\n","        train_labels = df_train['label'].tolist()\n","        test_titles = df_test['title'].tolist()\n","        test_labels = df_test['label'].tolist()\n","\n","        train_data_loader = create_data_loader(train_titles, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        test_data_loader = create_data_loader(test_titles, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        print('Data loaders created.')\n","\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","        total_steps = len(train_data_loader) * EPOCHS\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","        loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","        print('Optimizer, scheduler, and loss function defined.')\n","\n","        for epoch in range(EPOCHS):\n","            print(f'Starting epoch {epoch + 1}/{EPOCHS}')\n","\n","            train_acc, train_loss = train_epoch(\n","                model,\n","                train_data_loader,\n","                loss_fn,\n","                optimizer,\n","                device,\n","                scheduler,\n","                len(df_train)\n","            )\n","            print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","            test_acc, test_loss, all_labels, all_preds = eval_model(\n","                model,\n","                test_data_loader,\n","                loss_fn,\n","                device,\n","                len(df_test)\n","            )\n","            print(f'Test loss {test_loss} accuracy {test_acc}')\n","\n","            # Calculate additional metrics\n","            precision = precision_score(all_labels, all_preds, average='weighted')\n","            recall = recall_score(all_labels, all_preds, average='weighted')\n","            f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","            print(f'Test Precision: {precision}')\n","            print(f'Test Recall: {recall}')\n","            print(f'Test F1 Score: {f1}')\n","\n","    except Exception as e:\n","        print(f'An error occurred: {e}')\n","        traceback.print_exc()\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55793,"status":"ok","timestamp":1722588338189,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"},"user_tz":-180},"id":"Ic7ZMvG1IbEx","outputId":"32b6f1dc-7bf5-477a-a452-d97b0a582142"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1ZDoY_cCvGd99UeEdfZDeQTAR73hRmiF8","authorship_tag":"ABX9TyNEILal6o1AJnAamtTb+dUd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0dd73f0c8af344569826bdcde78fbd2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc56914b7035402d86a603cbc30c0af6","IPY_MODEL_09f7bd6dc12745dab21a572619b874e2","IPY_MODEL_2cfadcdc5ad3453daa56b9f738ac4bbe"],"layout":"IPY_MODEL_5101d88c69c24db5aa8e2f9b58fe6de6"}},"fc56914b7035402d86a603cbc30c0af6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e40c92fd88ce44e8891be6490fa4aee9","placeholder":"​","style":"IPY_MODEL_864fc74cda9d4f5185ec29d4eaf81996","value":"tokenizer_config.json: 100%"}},"09f7bd6dc12745dab21a572619b874e2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_01cc5a3a8398448dbf381894e0986307","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2be32e1e8a954db28552f5b3d99dbf2a","value":49}},"2cfadcdc5ad3453daa56b9f738ac4bbe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bf21c216e8848ee974171f464350737","placeholder":"​","style":"IPY_MODEL_fc370844760344ab834631ec218224a0","value":" 49.0/49.0 [00:00&lt;00:00, 4.06kB/s]"}},"5101d88c69c24db5aa8e2f9b58fe6de6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e40c92fd88ce44e8891be6490fa4aee9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"864fc74cda9d4f5185ec29d4eaf81996":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01cc5a3a8398448dbf381894e0986307":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2be32e1e8a954db28552f5b3d99dbf2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4bf21c216e8848ee974171f464350737":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc370844760344ab834631ec218224a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4af1e744003d475a804263b11d1a708a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_122ddd425c1146cfaff67845b94c5c3a","IPY_MODEL_a6a29cc34b10467cb38103e7c5ed0f05","IPY_MODEL_487f30f971d84d6b80d8545c4c7dc852"],"layout":"IPY_MODEL_5ffb3e25686f4c6fac85bd8fe42ff271"}},"122ddd425c1146cfaff67845b94c5c3a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f3fa9a41af44b3e952b70eb954fbe71","placeholder":"​","style":"IPY_MODEL_6d32ff223dd7407d83472e58e059fc27","value":"vocab.txt: 100%"}},"a6a29cc34b10467cb38103e7c5ed0f05":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdb5240f468246399f2f517d6a761dba","max":109540,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e8c95404c9484b18a743697695c7628e","value":109540}},"487f30f971d84d6b80d8545c4c7dc852":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d09658827e964695a86ca2abef0489ac","placeholder":"​","style":"IPY_MODEL_b0ceb1bb2ed645f2aac523f77e324905","value":" 110k/110k [00:00&lt;00:00, 7.54MB/s]"}},"5ffb3e25686f4c6fac85bd8fe42ff271":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f3fa9a41af44b3e952b70eb954fbe71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d32ff223dd7407d83472e58e059fc27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdb5240f468246399f2f517d6a761dba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8c95404c9484b18a743697695c7628e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d09658827e964695a86ca2abef0489ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0ceb1bb2ed645f2aac523f77e324905":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"222947e57ba4416d87a264f33d0d97af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e352b9b7ade84e779286fadd3bb0ce5c","IPY_MODEL_26d6ee6c451c4a39ab7aad67f98396db","IPY_MODEL_d00275346da94cc4b3bc3816dd8c6093"],"layout":"IPY_MODEL_62476dfd7f354ea2824ffaa05220b3ae"}},"e352b9b7ade84e779286fadd3bb0ce5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f65128142654982878223bd34d5a919","placeholder":"​","style":"IPY_MODEL_68a9bd4a3be44c7ab6c18f85a20143b5","value":"tokenizer.json: 100%"}},"26d6ee6c451c4a39ab7aad67f98396db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9d66cfd879c4fae80f9032461806782","max":268943,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0d274de0cd443d294e3fb8aece21d3b","value":268943}},"d00275346da94cc4b3bc3816dd8c6093":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9f174bdf4814e908d1d9193485db22b","placeholder":"​","style":"IPY_MODEL_9767ad1fabfc4c9682df01e74c65376f","value":" 269k/269k [00:00&lt;00:00, 1.23MB/s]"}},"62476dfd7f354ea2824ffaa05220b3ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f65128142654982878223bd34d5a919":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68a9bd4a3be44c7ab6c18f85a20143b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9d66cfd879c4fae80f9032461806782":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0d274de0cd443d294e3fb8aece21d3b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9f174bdf4814e908d1d9193485db22b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9767ad1fabfc4c9682df01e74c65376f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac9a4ac0495140509b317e5fe5ee7708":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f62289b6b754b7dac4d60097697a1e8","IPY_MODEL_b0a457ffdc3e4ca4a1e10c18b0108f8f","IPY_MODEL_b22cb4b6e31c4d38925b19994535a8e3"],"layout":"IPY_MODEL_37b93f6b10334bd8b46e1beb1df8ffb9"}},"4f62289b6b754b7dac4d60097697a1e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15eb13ff74924ebfbb7efc80c4ba9918","placeholder":"​","style":"IPY_MODEL_fe11fef1f7e74bd383d697de45893ceb","value":"config.json: 100%"}},"b0a457ffdc3e4ca4a1e10c18b0108f8f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad467105bda446fd9344af2cc7e68293","max":624,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c8f97ce534e24975be888d0be7211201","value":624}},"b22cb4b6e31c4d38925b19994535a8e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07157af1237d45d99026c91e9165546e","placeholder":"​","style":"IPY_MODEL_742dffc797ed40f98a6448c0edaf2b58","value":" 624/624 [00:00&lt;00:00, 49.2kB/s]"}},"37b93f6b10334bd8b46e1beb1df8ffb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15eb13ff74924ebfbb7efc80c4ba9918":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe11fef1f7e74bd383d697de45893ceb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad467105bda446fd9344af2cc7e68293":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8f97ce534e24975be888d0be7211201":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"07157af1237d45d99026c91e9165546e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"742dffc797ed40f98a6448c0edaf2b58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2fc03ac6f5be4584b8e047de96d0587f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa8a3615c6bf48a88595ab03acb504a8","IPY_MODEL_c9618a9d333c48be9832057b9278b5a4","IPY_MODEL_4eafa1ec18f54783bd0df6f18e2086a6"],"layout":"IPY_MODEL_8ac17c6fe9a64b6f975f767f3097bd42"}},"fa8a3615c6bf48a88595ab03acb504a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0e1a97c23e544d6878240b1b23a6fd4","placeholder":"​","style":"IPY_MODEL_0ec4d9fa8d874800ac30becef575d140","value":"model.safetensors: 100%"}},"c9618a9d333c48be9832057b9278b5a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9ac93159169447d9e281d11d7430907","max":411553788,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cbcd1f4d58f84cee985917c2fc4c9aba","value":411553788}},"4eafa1ec18f54783bd0df6f18e2086a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ea4f6d9c51947f38fa17dbe5b94a1d2","placeholder":"​","style":"IPY_MODEL_7fd607c4e66e4170819371768edccf6c","value":" 412M/412M [00:01&lt;00:00, 430MB/s]"}},"8ac17c6fe9a64b6f975f767f3097bd42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0e1a97c23e544d6878240b1b23a6fd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ec4d9fa8d874800ac30becef575d140":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e9ac93159169447d9e281d11d7430907":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbcd1f4d58f84cee985917c2fc4c9aba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ea4f6d9c51947f38fa17dbe5b94a1d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fd607c4e66e4170819371768edccf6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
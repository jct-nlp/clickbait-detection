{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"vPRssl5D_GRy","executionInfo":{"status":"error","timestamp":1723573584564,"user_tz":-180,"elapsed":725,"user":{"displayName":"","userId":""}},"outputId":"57cd91d5-76a3-4f84-e1f6-ec61f09ec59b"},"id":"vPRssl5D_GRy","execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'TBE_EPHEM_CREDS_ADDR'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   metadata_server_addr = (\n\u001b[0;32m--> 128\u001b[0;31m       \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TBE_EPHEM_CREDS_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m       \u001b[0;32melse\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TBE_CREDS_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'TBE_EPHEM_CREDS_ADDR'"]}]},{"cell_type":"code","id":"gMiWbNRKfRZro1tVCaNhFTwH","metadata":{"tags":[],"id":"gMiWbNRKfRZro1tVCaNhFTwH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723810792151,"user_tz":-180,"elapsed":44409,"user":{"displayName":"","userId":""}},"outputId":"83e94a49-9aa5-4be0-e704-ebf7fbe78657"},"source":["!pip install torch torchvision transformers pandas"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split\n","import time\n","import traceback\n","\n","class ClickbaitDataset(Dataset):\n","    def __init__(self, titles, labels, tokenizer, max_len):\n","        self.titles = titles\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.titles)\n","\n","    def __getitem__(self, idx):\n","        title = self.titles[idx]\n","        label = self.labels[idx]\n","\n","        # Ensure the label is a valid integer\n","        try:\n","            label = int(label)\n","        except ValueError:\n","            raise ValueError(f\"Label {label} at index {idx} is not a valid integer.\")\n","\n","        encoding = self.tokenizer.encode_plus(\n","            title,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'title_text': title,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","def create_data_loader(titles, labels, tokenizer, max_len, batch_size):\n","    ds = ClickbaitDataset(\n","        titles=titles,\n","        labels=labels,\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","\n","    return DataLoader(ds, batch_size=batch_size, num_workers=4, pin_memory=True)\n","\n","def train_epoch(\n","    model,\n","    data_loader,\n","    loss_fn,\n","    optimizer,\n","    device,\n","    scheduler,\n","    n_examples\n","):\n","    model.train()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    for batch_idx, d in enumerate(data_loader):\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        labels = d[\"labels\"].to(device)\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        _, preds = torch.max(outputs.logits, dim=1)\n","        loss = loss_fn(outputs.logits, labels)\n","\n","        correct_predictions += torch.sum(preds == labels)\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        if batch_idx % 10 == 0:\n","            print(f'Batch {batch_idx}/{len(data_loader)}, Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Training epoch completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model.eval()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    all_labels = []\n","    all_preds = []\n","\n","    with torch.no_grad():\n","        for batch_idx, d in enumerate(data_loader):\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            labels = d[\"labels\"].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            loss = loss_fn(outputs.logits, labels)\n","\n","            correct_predictions += torch.sum(preds == labels)\n","            losses.append(loss.item())\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","\n","            if batch_idx % 10 == 0:\n","                print(f'Batch {batch_idx}/{len(data_loader)}, Test Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Test evaluation completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses), all_labels, all_preds\n","\n","def clean_dataset(df):\n","    # Remove rows where labels are NaN\n","    df = df.dropna(subset=['label'])\n","    # Convert labels to integers, and filter out rows where this conversion fails\n","    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n","    df = df.dropna(subset=['label'])\n","    df['label'] = df['label'].astype(int)\n","    return df\n","\n","def main():\n","    print(\"Starting main process\")\n","\n","    try:\n","        print('Loading the dataset.')\n","        df_train = pd.read_csv('/content/bangla_train.csv')  # Use pd.read_excel for XLSX files\n","        print('Training dataset loaded successfully.')\n","\n","        print('Cleaning the training dataset.')\n","        df_train = clean_dataset(df_train)\n","        print('Training dataset cleaned.')\n","\n","        print('Loading the test dataset.')\n","        df_test = pd.read_csv('/content/bangla_test.csv')  # Use pd.read_excel for XLSX files\n","        print('Test dataset loaded successfully.')\n","\n","        RANDOM_SEED = 42\n","        MAX_LEN = 128\n","        BATCH_SIZE = 16\n","        EPOCHS = 10\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f'Using device: {device}')\n","\n","        print('Loading the tokenizer and model.')\n","        tokenizer = BertTokenizer.from_pretrained('sagorsarker/bangla-bert-base')\n","        model = BertForSequenceClassification.from_pretrained('sagorsarker/bangla-bert-base', num_labels=2)\n","        model = model.to(device)\n","        print('Model and tokenizer loaded and moved to device.')\n","\n","        print('Creating data loaders.')\n","        train_titles = df_train['title'].tolist()\n","        train_labels = df_train['label'].tolist()\n","        test_titles = df_test['title'].tolist()\n","        test_labels = df_test['label'].tolist()\n","\n","        train_data_loader = create_data_loader(train_titles, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        test_data_loader = create_data_loader(test_titles, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        print('Data loaders created.')\n","\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","        total_steps = len(train_data_loader) * EPOCHS\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","        loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","        print('Optimizer, scheduler, and loss function defined.')\n","\n","        for epoch in range(EPOCHS):\n","            print(f'Starting epoch {epoch + 1}/{EPOCHS}')\n","\n","            train_acc, train_loss = train_epoch(\n","                model,\n","                train_data_loader,\n","                loss_fn,\n","                optimizer,\n","                device,\n","                scheduler,\n","                len(df_train)\n","            )\n","            print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","            test_acc, test_loss, all_labels, all_preds = eval_model(\n","                model,\n","                test_data_loader,\n","                loss_fn,\n","                device,\n","                len(df_test)\n","            )\n","            print(f'Test loss {test_loss} accuracy {test_acc}')\n","\n","            # Calculate additional metrics\n","            precision = precision_score(all_labels, all_preds, average='weighted')\n","            recall = recall_score(all_labels, all_preds, average='weighted')\n","            f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","            print(f'Test Precision: {precision}')\n","            print(f'Test Recall: {recall}')\n","            print(f'Test F1 Score: {f1}')\n","\n","    except Exception as e:\n","        print(f'An error occurred: {e}')\n","        traceback.print_exc()\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYViO0Wa_IzA","outputId":"2b4ce65b-d66d-4fab-8756-b970f75beb44","executionInfo":{"status":"ok","timestamp":1723814656707,"user_tz":-180,"elapsed":1091112,"user":{"displayName":"","userId":""}}},"id":"qYViO0Wa_IzA","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting main process\n","Loading the dataset.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-56cc049870c2>:158: DtypeWarning: Columns (5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df_train = pd.read_csv('/content/bangla_train.csv')  # Use pd.read_excel for XLSX files\n","<ipython-input-5-56cc049870c2>:148: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['label'] = pd.to_numeric(df['label'], errors='coerce')\n"]},{"output_type":"stream","name":"stdout","text":["Training dataset loaded successfully.\n","Cleaning the training dataset.\n","Training dataset cleaned.\n","Loading the test dataset.\n","Test dataset loaded successfully.\n","Using device: cuda\n","Loading the tokenizer and model.\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model and tokenizer loaded and moved to device.\n","Creating data loaders.\n","Data loaders created.\n","Optimizer, scheduler, and loss function defined.\n","Starting epoch 1/10\n","Batch 0/677, Loss: 0.9987808465957642\n","Batch 10/677, Loss: 0.6476734280586243\n","Batch 20/677, Loss: 0.5750921368598938\n","Batch 30/677, Loss: 0.6597618460655212\n","Batch 40/677, Loss: 0.6294072866439819\n","Batch 50/677, Loss: 0.535251796245575\n","Batch 60/677, Loss: 0.42236781120300293\n","Batch 70/677, Loss: 0.3186330497264862\n","Batch 80/677, Loss: 0.4654485583305359\n","Batch 90/677, Loss: 0.45308491587638855\n","Batch 100/677, Loss: 0.2870646119117737\n","Batch 110/677, Loss: 0.2723543643951416\n","Batch 120/677, Loss: 0.5982267260551453\n","Batch 130/677, Loss: 0.5895584225654602\n","Batch 140/677, Loss: 0.4520467221736908\n","Batch 150/677, Loss: 0.4688686728477478\n","Batch 160/677, Loss: 0.555928647518158\n","Batch 170/677, Loss: 0.5776329636573792\n","Batch 180/677, Loss: 0.5739564299583435\n","Batch 190/677, Loss: 0.39260953664779663\n","Batch 200/677, Loss: 0.28276458382606506\n","Batch 210/677, Loss: 0.4039182662963867\n","Batch 220/677, Loss: 0.5802696347236633\n","Batch 230/677, Loss: 0.42052215337753296\n","Batch 240/677, Loss: 0.7567657232284546\n","Batch 250/677, Loss: 0.3811890780925751\n","Batch 260/677, Loss: 0.43813055753707886\n","Batch 270/677, Loss: 0.29066377878189087\n","Batch 280/677, Loss: 0.42157280445098877\n","Batch 290/677, Loss: 0.6245588660240173\n","Batch 300/677, Loss: 0.3885176479816437\n","Batch 310/677, Loss: 0.5801261067390442\n","Batch 320/677, Loss: 0.35459843277931213\n","Batch 330/677, Loss: 0.33566969633102417\n","Batch 340/677, Loss: 0.5494583249092102\n","Batch 350/677, Loss: 0.5474799275398254\n","Batch 360/677, Loss: 0.5460465550422668\n","Batch 370/677, Loss: 0.7130351066589355\n","Batch 380/677, Loss: 0.49387598037719727\n","Batch 390/677, Loss: 0.4531983435153961\n","Batch 400/677, Loss: 0.5098602771759033\n","Batch 410/677, Loss: 0.46507227420806885\n","Batch 420/677, Loss: 0.39308398962020874\n","Batch 430/677, Loss: 0.5233776569366455\n","Batch 440/677, Loss: 0.6133630871772766\n","Batch 450/677, Loss: 0.7504847049713135\n","Batch 460/677, Loss: 0.19020897150039673\n","Batch 470/677, Loss: 0.5397248864173889\n","Batch 480/677, Loss: 0.26375219225883484\n","Batch 490/677, Loss: 0.5418691635131836\n","Batch 500/677, Loss: 0.4506004750728607\n","Batch 510/677, Loss: 0.30862170457839966\n","Batch 520/677, Loss: 0.42976483702659607\n","Batch 530/677, Loss: 0.45800283551216125\n","Batch 540/677, Loss: 0.3238498866558075\n","Batch 550/677, Loss: 0.5050351023674011\n","Batch 560/677, Loss: 0.64794921875\n","Batch 570/677, Loss: 0.32297560572624207\n","Batch 580/677, Loss: 0.5153412818908691\n","Batch 590/677, Loss: 0.2765112519264221\n","Batch 600/677, Loss: 0.3227132260799408\n","Batch 610/677, Loss: 0.6131044030189514\n","Batch 620/677, Loss: 0.4679400622844696\n","Batch 630/677, Loss: 0.36612468957901\n","Batch 640/677, Loss: 0.5355278253555298\n","Batch 650/677, Loss: 0.488150030374527\n","Batch 660/677, Loss: 0.4776913523674011\n","Batch 670/677, Loss: 0.3390232026576996\n","Training epoch completed in: 4m 15s\n","Train loss 0.5013744658302169 accuracy 0.7599704579025112\n","Batch 0/189, Test Loss: 0.5518556833267212\n","Batch 10/189, Test Loss: 0.4370415210723877\n","Batch 20/189, Test Loss: 0.3055275082588196\n","Batch 30/189, Test Loss: 0.47074127197265625\n","Batch 40/189, Test Loss: 0.3021382689476013\n","Batch 50/189, Test Loss: 0.4396001696586609\n","Batch 60/189, Test Loss: 0.35975292325019836\n","Batch 70/189, Test Loss: 0.5067340135574341\n","Batch 80/189, Test Loss: 0.40615010261535645\n","Batch 90/189, Test Loss: 0.3422735035419464\n","Batch 100/189, Test Loss: 0.3986087739467621\n","Batch 110/189, Test Loss: 0.3686106204986572\n","Batch 120/189, Test Loss: 0.38452041149139404\n","Batch 130/189, Test Loss: 0.3605079650878906\n","Batch 140/189, Test Loss: 0.6566872596740723\n","Batch 150/189, Test Loss: 0.714898407459259\n","Batch 160/189, Test Loss: 0.6110910177230835\n","Batch 170/189, Test Loss: 0.5732355117797852\n","Batch 180/189, Test Loss: 0.5955886244773865\n","Test evaluation completed in: 0m 24s\n","Test loss 0.4659779659970097 accuracy 0.7781467950846895\n","Test Precision: 0.7804681714773628\n","Test Recall: 0.7781467950846894\n","Test F1 Score: 0.7791371305089781\n","Starting epoch 2/10\n","Batch 0/677, Loss: 0.311000257730484\n","Batch 10/677, Loss: 0.27234435081481934\n","Batch 20/677, Loss: 0.5623796582221985\n","Batch 30/677, Loss: 0.5694915056228638\n","Batch 40/677, Loss: 0.33318930864334106\n","Batch 50/677, Loss: 0.2523472309112549\n","Batch 60/677, Loss: 0.3426697850227356\n","Batch 70/677, Loss: 0.19929949939250946\n","Batch 80/677, Loss: 0.2648935317993164\n","Batch 90/677, Loss: 0.18613770604133606\n","Batch 100/677, Loss: 0.24541229009628296\n","Batch 110/677, Loss: 0.1999814510345459\n","Batch 120/677, Loss: 0.5164075493812561\n","Batch 130/677, Loss: 0.4972703754901886\n","Batch 140/677, Loss: 0.3280501067638397\n","Batch 150/677, Loss: 0.232129767537117\n","Batch 160/677, Loss: 0.45306968688964844\n","Batch 170/677, Loss: 0.4296876788139343\n","Batch 180/677, Loss: 0.4371900260448456\n","Batch 190/677, Loss: 0.2178114652633667\n","Batch 200/677, Loss: 0.17977871000766754\n","Batch 210/677, Loss: 0.2565443813800812\n","Batch 220/677, Loss: 0.5008379817008972\n","Batch 230/677, Loss: 0.35273435711860657\n","Batch 240/677, Loss: 0.606187105178833\n","Batch 250/677, Loss: 0.258297860622406\n","Batch 260/677, Loss: 0.2946021258831024\n","Batch 270/677, Loss: 0.14649944007396698\n","Batch 280/677, Loss: 0.36427751183509827\n","Batch 290/677, Loss: 0.42065486311912537\n","Batch 300/677, Loss: 0.335953027009964\n","Batch 310/677, Loss: 0.378537118434906\n","Batch 320/677, Loss: 0.12091109156608582\n","Batch 330/677, Loss: 0.18146827816963196\n","Batch 340/677, Loss: 0.3979680836200714\n","Batch 350/677, Loss: 0.35106161236763\n","Batch 360/677, Loss: 0.2955743372440338\n","Batch 370/677, Loss: 0.3557153642177582\n","Batch 380/677, Loss: 0.302282452583313\n","Batch 390/677, Loss: 0.20727229118347168\n","Batch 400/677, Loss: 0.2950616478919983\n","Batch 410/677, Loss: 0.2264951914548874\n","Batch 420/677, Loss: 0.47482264041900635\n","Batch 430/677, Loss: 0.4279410243034363\n","Batch 440/677, Loss: 0.2935243546962738\n","Batch 450/677, Loss: 0.5623055696487427\n","Batch 460/677, Loss: 0.11865220218896866\n","Batch 470/677, Loss: 0.28849804401397705\n","Batch 480/677, Loss: 0.11518625915050507\n","Batch 490/677, Loss: 0.5007593631744385\n","Batch 500/677, Loss: 0.3799232840538025\n","Batch 510/677, Loss: 0.09724914282560349\n","Batch 520/677, Loss: 0.367463618516922\n","Batch 530/677, Loss: 0.39391225576400757\n","Batch 540/677, Loss: 0.20326346158981323\n","Batch 550/677, Loss: 0.3453502953052521\n","Batch 560/677, Loss: 0.43051624298095703\n","Batch 570/677, Loss: 0.1803283393383026\n","Batch 580/677, Loss: 0.30065837502479553\n","Batch 590/677, Loss: 0.24580344557762146\n","Batch 600/677, Loss: 0.30071723461151123\n","Batch 610/677, Loss: 0.36708182096481323\n","Batch 620/677, Loss: 0.3109169602394104\n","Batch 630/677, Loss: 0.26441383361816406\n","Batch 640/677, Loss: 0.3560180366039276\n","Batch 650/677, Loss: 0.22769084572792053\n","Batch 660/677, Loss: 0.45719704031944275\n","Batch 670/677, Loss: 0.10093498229980469\n","Training epoch completed in: 4m 15s\n","Train loss 0.35009909382136745 accuracy 0.8486890694239292\n","Batch 0/189, Test Loss: 0.5998401641845703\n","Batch 10/189, Test Loss: 0.32886093854904175\n","Batch 20/189, Test Loss: 0.13769549131393433\n","Batch 30/189, Test Loss: 0.40377241373062134\n","Batch 40/189, Test Loss: 0.20981024205684662\n","Batch 50/189, Test Loss: 0.4744642376899719\n","Batch 60/189, Test Loss: 0.35135284066200256\n","Batch 70/189, Test Loss: 0.6069625616073608\n","Batch 80/189, Test Loss: 0.2734459340572357\n","Batch 90/189, Test Loss: 0.14975516498088837\n","Batch 100/189, Test Loss: 0.36152932047843933\n","Batch 110/189, Test Loss: 0.2981523275375366\n","Batch 120/189, Test Loss: 0.3691410422325134\n","Batch 130/189, Test Loss: 0.5598427653312683\n","Batch 140/189, Test Loss: 0.9991160035133362\n","Batch 150/189, Test Loss: 0.9414177536964417\n","Batch 160/189, Test Loss: 0.6605015397071838\n","Batch 170/189, Test Loss: 0.9780116081237793\n","Batch 180/189, Test Loss: 0.7126978039741516\n","Test evaluation completed in: 0m 24s\n","Test loss 0.4887242779608757 accuracy 0.7847891066091001\n","Test Precision: 0.7824383625034084\n","Test Recall: 0.7847891066091\n","Test F1 Score: 0.7832845299295661\n","Starting epoch 3/10\n","Batch 0/677, Loss: 0.17851532995700836\n","Batch 10/677, Loss: 0.17129454016685486\n","Batch 20/677, Loss: 0.3016853928565979\n","Batch 30/677, Loss: 0.31573355197906494\n","Batch 40/677, Loss: 0.25005975365638733\n","Batch 50/677, Loss: 0.1413186937570572\n","Batch 60/677, Loss: 0.16128259897232056\n","Batch 70/677, Loss: 0.13941122591495514\n","Batch 80/677, Loss: 0.07473479211330414\n","Batch 90/677, Loss: 0.12095007300376892\n","Batch 100/677, Loss: 0.11957461386919022\n","Batch 110/677, Loss: 0.2244100421667099\n","Batch 120/677, Loss: 0.3351985812187195\n","Batch 130/677, Loss: 0.4110228419303894\n","Batch 140/677, Loss: 0.08731422573328018\n","Batch 150/677, Loss: 0.07853744924068451\n","Batch 160/677, Loss: 0.2632884681224823\n","Batch 170/677, Loss: 0.3188738226890564\n","Batch 180/677, Loss: 0.12079380452632904\n","Batch 190/677, Loss: 0.045856717973947525\n","Batch 200/677, Loss: 0.07387477159500122\n","Batch 210/677, Loss: 0.13952089846134186\n","Batch 220/677, Loss: 0.32556283473968506\n","Batch 230/677, Loss: 0.38792455196380615\n","Batch 240/677, Loss: 0.2648436427116394\n","Batch 250/677, Loss: 0.07955987751483917\n","Batch 260/677, Loss: 0.32891589403152466\n","Batch 270/677, Loss: 0.13318045437335968\n","Batch 280/677, Loss: 0.1381845474243164\n","Batch 290/677, Loss: 0.26819908618927\n","Batch 300/677, Loss: 0.31010738015174866\n","Batch 310/677, Loss: 0.10241907089948654\n","Batch 320/677, Loss: 0.0477113351225853\n","Batch 330/677, Loss: 0.06486749649047852\n","Batch 340/677, Loss: 0.06940454989671707\n","Batch 350/677, Loss: 0.32605481147766113\n","Batch 360/677, Loss: 0.3966512084007263\n","Batch 370/677, Loss: 0.1572638899087906\n","Batch 380/677, Loss: 0.16419412195682526\n","Batch 390/677, Loss: 0.06507472693920135\n","Batch 400/677, Loss: 0.06974121928215027\n","Batch 410/677, Loss: 0.1668350249528885\n","Batch 420/677, Loss: 0.16510605812072754\n","Batch 430/677, Loss: 0.19709272682666779\n","Batch 440/677, Loss: 0.2716761529445648\n","Batch 450/677, Loss: 0.26257920265197754\n","Batch 460/677, Loss: 0.027057500556111336\n","Batch 470/677, Loss: 0.13817746937274933\n","Batch 480/677, Loss: 0.10162588953971863\n","Batch 490/677, Loss: 0.1353672444820404\n","Batch 500/677, Loss: 0.09872974455356598\n","Batch 510/677, Loss: 0.021113775670528412\n","Batch 520/677, Loss: 0.3360655903816223\n","Batch 530/677, Loss: 0.3012964427471161\n","Batch 540/677, Loss: 0.10700752586126328\n","Batch 550/677, Loss: 0.09000899642705917\n","Batch 560/677, Loss: 0.13673368096351624\n","Batch 570/677, Loss: 0.31423693895339966\n","Batch 580/677, Loss: 0.19028007984161377\n","Batch 590/677, Loss: 0.08902386575937271\n","Batch 600/677, Loss: 0.19507403671741486\n","Batch 610/677, Loss: 0.16433997452259064\n","Batch 620/677, Loss: 0.15274018049240112\n","Batch 630/677, Loss: 0.08542518317699432\n","Batch 640/677, Loss: 0.08998069167137146\n","Batch 650/677, Loss: 0.04994962364435196\n","Batch 660/677, Loss: 0.3684163987636566\n","Batch 670/677, Loss: 0.12686027586460114\n","Training epoch completed in: 4m 15s\n","Train loss 0.19558300064983977 accuracy 0.9270679468242246\n","Batch 0/189, Test Loss: 0.531476616859436\n","Batch 10/189, Test Loss: 0.24761317670345306\n","Batch 20/189, Test Loss: 0.08010438084602356\n","Batch 30/189, Test Loss: 0.14638736844062805\n","Batch 40/189, Test Loss: 0.16990409791469574\n","Batch 50/189, Test Loss: 0.40244805812835693\n","Batch 60/189, Test Loss: 0.3235618770122528\n","Batch 70/189, Test Loss: 0.6911556124687195\n","Batch 80/189, Test Loss: 0.30667781829833984\n","Batch 90/189, Test Loss: 0.09360390156507492\n","Batch 100/189, Test Loss: 0.2398841232061386\n","Batch 110/189, Test Loss: 0.31432753801345825\n","Batch 120/189, Test Loss: 0.18149185180664062\n","Batch 130/189, Test Loss: 1.4694788455963135\n","Batch 140/189, Test Loss: 1.979066014289856\n","Batch 150/189, Test Loss: 1.9409881830215454\n","Batch 160/189, Test Loss: 1.2126870155334473\n","Batch 170/189, Test Loss: 2.1750078201293945\n","Batch 180/189, Test Loss: 1.617541790008545\n","Test evaluation completed in: 0m 24s\n","Test loss 0.7570021470821409 accuracy 0.7671869810694122\n","Test Precision: 0.7628574885198407\n","Test Recall: 0.7671869810694122\n","Test F1 Score: 0.7566490325481583\n","Starting epoch 4/10\n","Batch 0/677, Loss: 0.04613252356648445\n","Batch 10/677, Loss: 0.01991482824087143\n","Batch 20/677, Loss: 0.10873472690582275\n","Batch 30/677, Loss: 0.1112334206700325\n","Batch 40/677, Loss: 0.016746755689382553\n","Batch 50/677, Loss: 0.08849852532148361\n","Batch 60/677, Loss: 0.035228580236434937\n","Batch 70/677, Loss: 0.03425534442067146\n","Batch 80/677, Loss: 0.1529630571603775\n","Batch 90/677, Loss: 0.00514196464791894\n","Batch 100/677, Loss: 0.03939047455787659\n","Batch 110/677, Loss: 0.035285159945487976\n","Batch 120/677, Loss: 0.2065768986940384\n","Batch 130/677, Loss: 0.15102382004261017\n","Batch 140/677, Loss: 0.06634829938411713\n","Batch 150/677, Loss: 0.010173280723392963\n","Batch 160/677, Loss: 0.25233155488967896\n","Batch 170/677, Loss: 0.080596923828125\n","Batch 180/677, Loss: 0.288894385099411\n","Batch 190/677, Loss: 0.024631978943943977\n","Batch 200/677, Loss: 0.14162959158420563\n","Batch 210/677, Loss: 0.011758172884583473\n","Batch 220/677, Loss: 0.1128331646323204\n","Batch 230/677, Loss: 0.05373023450374603\n","Batch 240/677, Loss: 0.13487890362739563\n","Batch 250/677, Loss: 0.06107813119888306\n","Batch 260/677, Loss: 0.13627742230892181\n","Batch 270/677, Loss: 0.038317933678627014\n","Batch 280/677, Loss: 0.03895292058587074\n","Batch 290/677, Loss: 0.009194058366119862\n","Batch 300/677, Loss: 0.09231264144182205\n","Batch 310/677, Loss: 0.0843224748969078\n","Batch 320/677, Loss: 0.09085556864738464\n","Batch 330/677, Loss: 0.11002136766910553\n","Batch 340/677, Loss: 0.02908335253596306\n","Batch 350/677, Loss: 0.09018553048372269\n","Batch 360/677, Loss: 0.06402716785669327\n","Batch 370/677, Loss: 0.16027404367923737\n","Batch 380/677, Loss: 0.038194671273231506\n","Batch 390/677, Loss: 0.1070537120103836\n","Batch 400/677, Loss: 0.035986628383398056\n","Batch 410/677, Loss: 0.04511723294854164\n","Batch 420/677, Loss: 0.11342579871416092\n","Batch 430/677, Loss: 0.032916609197854996\n","Batch 440/677, Loss: 0.029418308287858963\n","Batch 450/677, Loss: 0.12199968099594116\n","Batch 460/677, Loss: 0.007042136043310165\n","Batch 470/677, Loss: 0.006032241508364677\n","Batch 480/677, Loss: 0.004854228347539902\n","Batch 490/677, Loss: 0.04794282093644142\n","Batch 500/677, Loss: 0.2897668182849884\n","Batch 510/677, Loss: 0.07179977744817734\n","Batch 520/677, Loss: 0.1730349063873291\n","Batch 530/677, Loss: 0.38628289103507996\n","Batch 540/677, Loss: 0.04029753431677818\n","Batch 550/677, Loss: 0.02492256835103035\n","Batch 560/677, Loss: 0.06737516075372696\n","Batch 570/677, Loss: 0.05849798768758774\n","Batch 580/677, Loss: 0.009380392730236053\n","Batch 590/677, Loss: 0.05528075248003006\n","Batch 600/677, Loss: 0.047236423939466476\n","Batch 610/677, Loss: 0.28410738706588745\n","Batch 620/677, Loss: 0.1409294158220291\n","Batch 630/677, Loss: 0.050060831010341644\n","Batch 640/677, Loss: 0.0685742124915123\n","Batch 650/677, Loss: 0.0499148927628994\n","Batch 660/677, Loss: 0.36290642619132996\n","Batch 670/677, Loss: 0.0322454608976841\n","Training epoch completed in: 4m 15s\n","Train loss 0.10506074009648905 accuracy 0.9621491875923192\n","Batch 0/189, Test Loss: 0.6809827089309692\n","Batch 10/189, Test Loss: 0.4198004901409149\n","Batch 20/189, Test Loss: 0.05366690829396248\n","Batch 30/189, Test Loss: 0.059828996658325195\n","Batch 40/189, Test Loss: 0.17604289948940277\n","Batch 50/189, Test Loss: 0.4145912230014801\n","Batch 60/189, Test Loss: 0.29821285605430603\n","Batch 70/189, Test Loss: 0.7186087369918823\n","Batch 80/189, Test Loss: 0.4183656871318817\n","Batch 90/189, Test Loss: 0.16631540656089783\n","Batch 100/189, Test Loss: 0.17738190293312073\n","Batch 110/189, Test Loss: 0.33093997836112976\n","Batch 120/189, Test Loss: 0.2897789180278778\n","Batch 130/189, Test Loss: 1.265019178390503\n","Batch 140/189, Test Loss: 2.666128396987915\n","Batch 150/189, Test Loss: 1.9213600158691406\n","Batch 160/189, Test Loss: 1.4031002521514893\n","Batch 170/189, Test Loss: 2.304553508758545\n","Batch 180/189, Test Loss: 1.8061412572860718\n","Test evaluation completed in: 0m 24s\n","Test loss 0.8392303921861288 accuracy 0.7754898704749253\n","Test Precision: 0.7722372575954571\n","Test Recall: 0.7754898704749252\n","Test F1 Score: 0.7652812897234713\n","Starting epoch 5/10\n","Batch 0/677, Loss: 0.07594315707683563\n","Batch 10/677, Loss: 0.004273445811122656\n","Batch 20/677, Loss: 0.0163325946778059\n","Batch 30/677, Loss: 0.03222331404685974\n","Batch 40/677, Loss: 0.007570709567517042\n","Batch 50/677, Loss: 0.023890502750873566\n","Batch 60/677, Loss: 0.1827409267425537\n","Batch 70/677, Loss: 0.014177948236465454\n","Batch 80/677, Loss: 0.04026853293180466\n","Batch 90/677, Loss: 0.010232501663267612\n","Batch 100/677, Loss: 0.011259377934038639\n","Batch 110/677, Loss: 0.038352422416210175\n","Batch 120/677, Loss: 0.03725387901067734\n","Batch 130/677, Loss: 0.03264027088880539\n","Batch 140/677, Loss: 0.008729812689125538\n","Batch 150/677, Loss: 0.001253408961929381\n","Batch 160/677, Loss: 0.021023649722337723\n","Batch 170/677, Loss: 0.04746953025460243\n","Batch 180/677, Loss: 0.012117302045226097\n","Batch 190/677, Loss: 0.025315413251519203\n","Batch 200/677, Loss: 0.04878324270248413\n","Batch 210/677, Loss: 0.014637074433267117\n","Batch 220/677, Loss: 0.005360710434615612\n","Batch 230/677, Loss: 0.057557057589292526\n","Batch 240/677, Loss: 0.036661092191934586\n","Batch 250/677, Loss: 0.011478327214717865\n","Batch 260/677, Loss: 0.04549749940633774\n","Batch 270/677, Loss: 0.0024686118122190237\n","Batch 280/677, Loss: 0.13666778802871704\n","Batch 290/677, Loss: 0.0038507413119077682\n","Batch 300/677, Loss: 0.03757699951529503\n","Batch 310/677, Loss: 0.007730306591838598\n","Batch 320/677, Loss: 0.004466394428163767\n","Batch 330/677, Loss: 0.0019591236487030983\n","Batch 340/677, Loss: 0.4104835093021393\n","Batch 350/677, Loss: 0.0032381941564381123\n","Batch 360/677, Loss: 0.07173530757427216\n","Batch 370/677, Loss: 0.004693378694355488\n","Batch 380/677, Loss: 0.05568990483880043\n","Batch 390/677, Loss: 0.0763658806681633\n","Batch 400/677, Loss: 0.0014749574474990368\n","Batch 410/677, Loss: 0.0017209436045959592\n","Batch 420/677, Loss: 0.16097107529640198\n","Batch 430/677, Loss: 0.34558022022247314\n","Batch 440/677, Loss: 0.04405589401721954\n","Batch 450/677, Loss: 0.05028712376952171\n","Batch 460/677, Loss: 0.013485818170011044\n","Batch 470/677, Loss: 0.0040880050510168076\n","Batch 480/677, Loss: 0.0056743379682302475\n","Batch 490/677, Loss: 0.026289282366633415\n","Batch 500/677, Loss: 0.016445929184556007\n","Batch 510/677, Loss: 0.0008321109344251454\n","Batch 520/677, Loss: 0.013725229538977146\n","Batch 530/677, Loss: 0.5663945078849792\n","Batch 540/677, Loss: 0.024500630795955658\n","Batch 550/677, Loss: 0.05600345879793167\n","Batch 560/677, Loss: 0.06744326651096344\n","Batch 570/677, Loss: 0.21194685995578766\n","Batch 580/677, Loss: 0.007314363960176706\n","Batch 590/677, Loss: 0.00594851141795516\n","Batch 600/677, Loss: 0.03618022799491882\n","Batch 610/677, Loss: 0.01921902969479561\n","Batch 620/677, Loss: 0.03714640066027641\n","Batch 630/677, Loss: 0.006284171715378761\n","Batch 640/677, Loss: 0.029270697385072708\n","Batch 650/677, Loss: 0.0464593805372715\n","Batch 660/677, Loss: 0.1499490588903427\n","Batch 670/677, Loss: 0.014313039369881153\n","Training epoch completed in: 4m 15s\n","Train loss 0.05325548666860548 accuracy 0.9822747415066471\n","Batch 0/189, Test Loss: 0.9976086020469666\n","Batch 10/189, Test Loss: 0.5962285399436951\n","Batch 20/189, Test Loss: 0.15578295290470123\n","Batch 30/189, Test Loss: 0.4501323401927948\n","Batch 40/189, Test Loss: 0.5394002795219421\n","Batch 50/189, Test Loss: 1.0171644687652588\n","Batch 60/189, Test Loss: 0.6889854669570923\n","Batch 70/189, Test Loss: 1.3200925588607788\n","Batch 80/189, Test Loss: 0.3766947388648987\n","Batch 90/189, Test Loss: 0.38704439997673035\n","Batch 100/189, Test Loss: 0.3825094699859619\n","Batch 110/189, Test Loss: 0.5983747839927673\n","Batch 120/189, Test Loss: 0.729518711566925\n","Batch 130/189, Test Loss: 0.8951367735862732\n","Batch 140/189, Test Loss: 1.8469372987747192\n","Batch 150/189, Test Loss: 1.335601806640625\n","Batch 160/189, Test Loss: 0.6678224802017212\n","Batch 170/189, Test Loss: 1.9919672012329102\n","Batch 180/189, Test Loss: 1.5850509405136108\n","Test evaluation completed in: 0m 24s\n","Test loss 0.878942816225053 accuracy 0.7774825639322485\n","Test Precision: 0.7790221289717608\n","Test Recall: 0.7774825639322485\n","Test F1 Score: 0.7781729859692499\n","Starting epoch 6/10\n","Batch 0/677, Loss: 0.0361097976565361\n","Batch 10/677, Loss: 0.010002754628658295\n","Batch 20/677, Loss: 0.0027280012145638466\n","Batch 30/677, Loss: 0.02478882111608982\n","Batch 40/677, Loss: 0.006731729954481125\n","Batch 50/677, Loss: 0.018855256959795952\n","Batch 60/677, Loss: 0.0038264135364443064\n","Batch 70/677, Loss: 0.002681947313249111\n","Batch 80/677, Loss: 0.007336101960390806\n","Batch 90/677, Loss: 0.1038578450679779\n","Batch 100/677, Loss: 0.0047241137363016605\n","Batch 110/677, Loss: 0.0036818874068558216\n","Batch 120/677, Loss: 0.07855144143104553\n","Batch 130/677, Loss: 0.003296077251434326\n","Batch 140/677, Loss: 0.003617457114160061\n","Batch 150/677, Loss: 0.00191052770242095\n","Batch 160/677, Loss: 0.16044841706752777\n","Batch 170/677, Loss: 0.07006947696208954\n","Batch 180/677, Loss: 0.027007022872567177\n","Batch 190/677, Loss: 0.060027409344911575\n","Batch 200/677, Loss: 0.0023755664005875587\n","Batch 210/677, Loss: 0.0034927111119031906\n","Batch 220/677, Loss: 0.006862345151603222\n","Batch 230/677, Loss: 0.0070165726356208324\n","Batch 240/677, Loss: 0.005562696605920792\n","Batch 250/677, Loss: 0.002665210049599409\n","Batch 260/677, Loss: 0.12983791530132294\n","Batch 270/677, Loss: 0.029248226433992386\n","Batch 280/677, Loss: 0.000962806458119303\n","Batch 290/677, Loss: 0.1338554471731186\n","Batch 300/677, Loss: 0.047362133860588074\n","Batch 310/677, Loss: 0.002771355677396059\n","Batch 320/677, Loss: 0.00887446291744709\n","Batch 330/677, Loss: 0.0012816591188311577\n","Batch 340/677, Loss: 0.0859607458114624\n","Batch 350/677, Loss: 0.04801188409328461\n","Batch 360/677, Loss: 0.0036962972953915596\n","Batch 370/677, Loss: 0.0223690215498209\n","Batch 380/677, Loss: 0.008569643832743168\n","Batch 390/677, Loss: 0.0032582504209131002\n","Batch 400/677, Loss: 0.0011300996411591768\n","Batch 410/677, Loss: 0.11406958103179932\n","Batch 420/677, Loss: 0.017240308225154877\n","Batch 430/677, Loss: 0.011749797500669956\n","Batch 440/677, Loss: 0.0016651783371344209\n","Batch 450/677, Loss: 0.012640324421226978\n","Batch 460/677, Loss: 0.0015839793486520648\n","Batch 470/677, Loss: 0.001920808688737452\n","Batch 480/677, Loss: 0.0007657348178327084\n","Batch 490/677, Loss: 0.3879898190498352\n","Batch 500/677, Loss: 0.035114992409944534\n","Batch 510/677, Loss: 0.000689164618961513\n","Batch 520/677, Loss: 0.010109375230967999\n","Batch 530/677, Loss: 0.33952566981315613\n","Batch 540/677, Loss: 0.005837057251483202\n","Batch 550/677, Loss: 0.026216622442007065\n","Batch 560/677, Loss: 0.005742424167692661\n","Batch 570/677, Loss: 0.04847400635480881\n","Batch 580/677, Loss: 0.0181259922683239\n","Batch 590/677, Loss: 0.002417222363874316\n","Batch 600/677, Loss: 0.037202317267656326\n","Batch 610/677, Loss: 0.0014776504831388593\n","Batch 620/677, Loss: 0.002548038028180599\n","Batch 630/677, Loss: 0.002143984194844961\n","Batch 640/677, Loss: 0.01745833456516266\n","Batch 650/677, Loss: 0.05083039775490761\n","Batch 660/677, Loss: 0.001635669614188373\n","Batch 670/677, Loss: 0.015282446518540382\n","Training epoch completed in: 4m 15s\n","Train loss 0.039351732345984734 accuracy 0.986152141802068\n","Batch 0/189, Test Loss: 1.0101776123046875\n","Batch 10/189, Test Loss: 0.6838881373405457\n","Batch 20/189, Test Loss: 0.09175489842891693\n","Batch 30/189, Test Loss: 0.09612061083316803\n","Batch 40/189, Test Loss: 0.5663018226623535\n","Batch 50/189, Test Loss: 0.9416378736495972\n","Batch 60/189, Test Loss: 0.7627683877944946\n","Batch 70/189, Test Loss: 1.1769286394119263\n","Batch 80/189, Test Loss: 0.37834927439689636\n","Batch 90/189, Test Loss: 0.24857163429260254\n","Batch 100/189, Test Loss: 0.25596365332603455\n","Batch 110/189, Test Loss: 0.7510263919830322\n","Batch 120/189, Test Loss: 0.7553842663764954\n","Batch 130/189, Test Loss: 1.3324426412582397\n","Batch 140/189, Test Loss: 1.7665143013000488\n","Batch 150/189, Test Loss: 1.751964807510376\n","Batch 160/189, Test Loss: 1.077832818031311\n","Batch 170/189, Test Loss: 2.3615989685058594\n","Batch 180/189, Test Loss: 2.1985960006713867\n","Test evaluation completed in: 0m 24s\n","Test loss 0.9509161627030975 accuracy 0.7798073729657922\n","Test Precision: 0.777482491375589\n","Test Recall: 0.7798073729657921\n","Test F1 Score: 0.7783505685936852\n","Starting epoch 7/10\n","Batch 0/677, Loss: 0.006509021855890751\n","Batch 10/677, Loss: 0.004674840718507767\n","Batch 20/677, Loss: 0.03211120143532753\n","Batch 30/677, Loss: 0.06041542440652847\n","Batch 40/677, Loss: 0.0024997685104608536\n","Batch 50/677, Loss: 0.035430312156677246\n","Batch 60/677, Loss: 0.0033041369169950485\n","Batch 70/677, Loss: 0.001109620789065957\n","Batch 80/677, Loss: 0.03761058673262596\n","Batch 90/677, Loss: 0.001154996221885085\n","Batch 100/677, Loss: 0.015538048930466175\n","Batch 110/677, Loss: 0.04222884029150009\n","Batch 120/677, Loss: 0.11689543724060059\n","Batch 130/677, Loss: 0.22430112957954407\n","Batch 140/677, Loss: 0.004155938513576984\n","Batch 150/677, Loss: 0.00033835406065918505\n","Batch 160/677, Loss: 0.03748646378517151\n","Batch 170/677, Loss: 0.035147588700056076\n","Batch 180/677, Loss: 0.0007093771127983928\n","Batch 190/677, Loss: 0.02293683961033821\n","Batch 200/677, Loss: 0.0012561860494315624\n","Batch 210/677, Loss: 0.004216376692056656\n","Batch 220/677, Loss: 0.002774276304990053\n","Batch 230/677, Loss: 0.006133677903562784\n","Batch 240/677, Loss: 0.023337963968515396\n","Batch 250/677, Loss: 0.005518725141882896\n","Batch 260/677, Loss: 0.02157686837017536\n","Batch 270/677, Loss: 0.0014629063662141562\n","Batch 280/677, Loss: 0.002031557960435748\n","Batch 290/677, Loss: 0.1695510447025299\n","Batch 300/677, Loss: 0.05618489161133766\n","Batch 310/677, Loss: 0.00218627555295825\n","Batch 320/677, Loss: 0.0008821708033792675\n","Batch 330/677, Loss: 0.0010032410500571132\n","Batch 340/677, Loss: 0.0025192308239638805\n","Batch 350/677, Loss: 0.009186644107103348\n","Batch 360/677, Loss: 0.0022704717703163624\n","Batch 370/677, Loss: 0.0047113606706261635\n","Batch 380/677, Loss: 0.007581590209156275\n","Batch 390/677, Loss: 0.0011561006540432572\n","Batch 400/677, Loss: 0.0006175661692395806\n","Batch 410/677, Loss: 0.001702087582089007\n","Batch 420/677, Loss: 0.004387013614177704\n","Batch 430/677, Loss: 0.00335329188965261\n","Batch 440/677, Loss: 0.2173735499382019\n","Batch 450/677, Loss: 0.01239755004644394\n","Batch 460/677, Loss: 0.0010257691610604525\n","Batch 470/677, Loss: 0.0013317916309460998\n","Batch 480/677, Loss: 0.00035607017343863845\n","Batch 490/677, Loss: 0.017300594598054886\n","Batch 500/677, Loss: 0.0014810002176091075\n","Batch 510/677, Loss: 0.0006567882956005633\n","Batch 520/677, Loss: 0.0006175478920340538\n","Batch 530/677, Loss: 0.35735446214675903\n","Batch 540/677, Loss: 0.00597540894523263\n","Batch 550/677, Loss: 0.0036346414126455784\n","Batch 560/677, Loss: 0.007838830351829529\n","Batch 570/677, Loss: 0.0009014052338898182\n","Batch 580/677, Loss: 0.0007017647149041295\n","Batch 590/677, Loss: 0.0006797629757784307\n","Batch 600/677, Loss: 0.0011104554869234562\n","Batch 610/677, Loss: 0.0005539150442928076\n","Batch 620/677, Loss: 0.00020319203031249344\n","Batch 630/677, Loss: 0.07683544605970383\n","Batch 640/677, Loss: 0.06976575404405594\n","Batch 650/677, Loss: 0.001806527259759605\n","Batch 660/677, Loss: 0.025114944204688072\n","Batch 670/677, Loss: 0.018844524398446083\n","Training epoch completed in: 4m 15s\n","Train loss 0.02012724774277307 accuracy 0.99390694239291\n","Batch 0/189, Test Loss: 0.9155969619750977\n","Batch 10/189, Test Loss: 0.8576138019561768\n","Batch 20/189, Test Loss: 0.08728303760290146\n","Batch 30/189, Test Loss: 0.1456340253353119\n","Batch 40/189, Test Loss: 0.2696704566478729\n","Batch 50/189, Test Loss: 0.9372752904891968\n","Batch 60/189, Test Loss: 0.6800314784049988\n","Batch 70/189, Test Loss: 0.9899307489395142\n","Batch 80/189, Test Loss: 0.4188951849937439\n","Batch 90/189, Test Loss: 0.30145135521888733\n","Batch 100/189, Test Loss: 0.35354796051979065\n","Batch 110/189, Test Loss: 0.7059517502784729\n","Batch 120/189, Test Loss: 0.8351194262504578\n","Batch 130/189, Test Loss: 1.4265319108963013\n","Batch 140/189, Test Loss: 2.670353889465332\n","Batch 150/189, Test Loss: 2.4829697608947754\n","Batch 160/189, Test Loss: 1.5788168907165527\n","Batch 170/189, Test Loss: 3.021960735321045\n","Batch 180/189, Test Loss: 2.4822704792022705\n","Test evaluation completed in: 0m 24s\n","Test loss 1.1354401685417908 accuracy 0.7904350714048489\n","Test Precision: 0.7867607393540922\n","Test Recall: 0.7904350714048489\n","Test F1 Score: 0.7871222517431313\n","Starting epoch 8/10\n","Batch 0/677, Loss: 0.001112084835767746\n","Batch 10/677, Loss: 0.00033501439611427486\n","Batch 20/677, Loss: 0.0017277022125199437\n","Batch 30/677, Loss: 0.0017303366912528872\n","Batch 40/677, Loss: 0.005439668893814087\n","Batch 50/677, Loss: 0.0018570870161056519\n","Batch 60/677, Loss: 0.0030498746782541275\n","Batch 70/677, Loss: 0.0020042662508785725\n","Batch 80/677, Loss: 0.10193590074777603\n","Batch 90/677, Loss: 0.0027238763868808746\n","Batch 100/677, Loss: 0.01418616995215416\n","Batch 110/677, Loss: 0.0008822183008305728\n","Batch 120/677, Loss: 0.00651067029684782\n","Batch 130/677, Loss: 0.00751288328319788\n","Batch 140/677, Loss: 0.00172383151948452\n","Batch 150/677, Loss: 0.05329423025250435\n","Batch 160/677, Loss: 0.008329465985298157\n","Batch 170/677, Loss: 0.006241448223590851\n","Batch 180/677, Loss: 0.022857336327433586\n","Batch 190/677, Loss: 0.02749740518629551\n","Batch 200/677, Loss: 0.02673923596739769\n","Batch 210/677, Loss: 0.000354118732502684\n","Batch 220/677, Loss: 0.007611689157783985\n","Batch 230/677, Loss: 0.003544668899849057\n","Batch 240/677, Loss: 0.010263637639582157\n","Batch 250/677, Loss: 0.020059986039996147\n","Batch 260/677, Loss: 0.008061196655035019\n","Batch 270/677, Loss: 0.1614900380373001\n","Batch 280/677, Loss: 0.0008068163879215717\n","Batch 290/677, Loss: 0.0012287143617868423\n","Batch 300/677, Loss: 0.016777625307440758\n","Batch 310/677, Loss: 0.2253299355506897\n","Batch 320/677, Loss: 0.005144532769918442\n","Batch 330/677, Loss: 0.002583435969427228\n","Batch 340/677, Loss: 0.00827871635556221\n","Batch 350/677, Loss: 0.042149998247623444\n","Batch 360/677, Loss: 0.005618710536509752\n","Batch 370/677, Loss: 0.0010818368755280972\n","Batch 380/677, Loss: 0.0007599117234349251\n","Batch 390/677, Loss: 0.0017533201025798917\n","Batch 400/677, Loss: 0.0008834903128445148\n","Batch 410/677, Loss: 0.006052133161574602\n","Batch 420/677, Loss: 0.0005731863202527165\n","Batch 430/677, Loss: 0.0019149528816342354\n","Batch 440/677, Loss: 0.0008318559266626835\n","Batch 450/677, Loss: 0.003376113250851631\n","Batch 460/677, Loss: 0.0006666153203696012\n","Batch 470/677, Loss: 0.06448077410459518\n","Batch 480/677, Loss: 0.018017807975411415\n","Batch 490/677, Loss: 0.0013952612644061446\n","Batch 500/677, Loss: 0.0010887936223298311\n","Batch 510/677, Loss: 0.00045749402488581836\n","Batch 520/677, Loss: 0.0004792381951119751\n","Batch 530/677, Loss: 0.20557668805122375\n","Batch 540/677, Loss: 0.004106003325432539\n","Batch 550/677, Loss: 0.01117042824625969\n","Batch 560/677, Loss: 0.0006448219064623117\n","Batch 570/677, Loss: 0.0020893821492791176\n","Batch 580/677, Loss: 0.00040183268720284104\n","Batch 590/677, Loss: 0.005810820963233709\n","Batch 600/677, Loss: 0.0023816637694835663\n","Batch 610/677, Loss: 0.002930364105850458\n","Batch 620/677, Loss: 0.00026739243185147643\n","Batch 630/677, Loss: 0.00026572178467176855\n","Batch 640/677, Loss: 0.0013393722474575043\n","Batch 650/677, Loss: 0.0003613651206251234\n","Batch 660/677, Loss: 0.010105528868734837\n","Batch 670/677, Loss: 0.0006791980122216046\n","Training epoch completed in: 4m 15s\n","Train loss 0.01627921945236773 accuracy 0.9944608567208273\n","Batch 0/189, Test Loss: 1.0176498889923096\n","Batch 10/189, Test Loss: 1.1538183689117432\n","Batch 20/189, Test Loss: 0.13039834797382355\n","Batch 30/189, Test Loss: 0.38071972131729126\n","Batch 40/189, Test Loss: 0.7183935642242432\n","Batch 50/189, Test Loss: 0.8402234315872192\n","Batch 60/189, Test Loss: 0.8193634748458862\n","Batch 70/189, Test Loss: 1.177775502204895\n","Batch 80/189, Test Loss: 0.4315008521080017\n","Batch 90/189, Test Loss: 0.4061163067817688\n","Batch 100/189, Test Loss: 0.3356655538082123\n","Batch 110/189, Test Loss: 0.8157176971435547\n","Batch 120/189, Test Loss: 1.085383653640747\n","Batch 130/189, Test Loss: 0.9967309832572937\n","Batch 140/189, Test Loss: 2.2775871753692627\n","Batch 150/189, Test Loss: 2.216886281967163\n","Batch 160/189, Test Loss: 1.5744174718856812\n","Batch 170/189, Test Loss: 2.723505735397339\n","Batch 180/189, Test Loss: 2.430056571960449\n","Test evaluation completed in: 0m 24s\n","Test loss 1.142746143904761 accuracy 0.7914314181335105\n","Test Precision: 0.7884902137268733\n","Test Recall: 0.7914314181335105\n","Test F1 Score: 0.7892710099934577\n","Starting epoch 9/10\n","Batch 0/677, Loss: 0.0004132277681492269\n","Batch 10/677, Loss: 0.0002862366090994328\n","Batch 20/677, Loss: 0.0002970601199194789\n","Batch 30/677, Loss: 0.004019070416688919\n","Batch 40/677, Loss: 0.0004553796024993062\n","Batch 50/677, Loss: 0.000718409544788301\n","Batch 60/677, Loss: 0.00277441693469882\n","Batch 70/677, Loss: 0.0005895665381103754\n","Batch 80/677, Loss: 0.001961177447810769\n","Batch 90/677, Loss: 0.00036888313479721546\n","Batch 100/677, Loss: 0.00025169120635837317\n","Batch 110/677, Loss: 0.001709438394755125\n","Batch 120/677, Loss: 0.0014166884357109666\n","Batch 130/677, Loss: 0.0039759669452905655\n","Batch 140/677, Loss: 0.00048665760550647974\n","Batch 150/677, Loss: 0.00016919519111979753\n","Batch 160/677, Loss: 0.007152658887207508\n","Batch 170/677, Loss: 0.0022147600539028645\n","Batch 180/677, Loss: 0.0003678930806927383\n","Batch 190/677, Loss: 0.00246134283952415\n","Batch 200/677, Loss: 0.00048440147656947374\n","Batch 210/677, Loss: 0.0041243755258619785\n","Batch 220/677, Loss: 0.013410693034529686\n","Batch 230/677, Loss: 0.005578204058110714\n","Batch 240/677, Loss: 0.1119530126452446\n","Batch 250/677, Loss: 0.03792495280504227\n","Batch 260/677, Loss: 0.0013684057630598545\n","Batch 270/677, Loss: 0.00023842300288379192\n","Batch 280/677, Loss: 0.001605164143256843\n","Batch 290/677, Loss: 0.0004762409662362188\n","Batch 300/677, Loss: 0.0008345022797584534\n","Batch 310/677, Loss: 0.000217496941331774\n","Batch 320/677, Loss: 0.007218771148473024\n","Batch 330/677, Loss: 0.0017480802489444613\n","Batch 340/677, Loss: 0.0070386542938649654\n","Batch 350/677, Loss: 0.0030735014006495476\n","Batch 360/677, Loss: 0.16706593334674835\n","Batch 370/677, Loss: 0.0015688387211412191\n","Batch 380/677, Loss: 0.001693072379566729\n","Batch 390/677, Loss: 0.020336126908659935\n","Batch 400/677, Loss: 0.0002524668234400451\n","Batch 410/677, Loss: 0.0007948522106744349\n","Batch 420/677, Loss: 0.0009905170882120728\n","Batch 430/677, Loss: 0.00031260846299119294\n","Batch 440/677, Loss: 0.0007375294808298349\n","Batch 450/677, Loss: 0.0009792833589017391\n","Batch 460/677, Loss: 0.0013562928652390838\n","Batch 470/677, Loss: 0.0005249393288977444\n","Batch 480/677, Loss: 0.00023193855304270983\n","Batch 490/677, Loss: 0.0037565166130661964\n","Batch 500/677, Loss: 0.0004137672367505729\n","Batch 510/677, Loss: 0.005054059438407421\n","Batch 520/677, Loss: 0.0004946010303683579\n","Batch 530/677, Loss: 0.2625173330307007\n","Batch 540/677, Loss: 0.0009241915540769696\n","Batch 550/677, Loss: 0.000803343253210187\n","Batch 560/677, Loss: 0.0015211417339742184\n","Batch 570/677, Loss: 0.00229619350284338\n","Batch 580/677, Loss: 0.0024138912558555603\n","Batch 590/677, Loss: 0.0005507892929017544\n","Batch 600/677, Loss: 0.003360396483913064\n","Batch 610/677, Loss: 0.0003726894501596689\n","Batch 620/677, Loss: 8.51402583066374e-05\n","Batch 630/677, Loss: 0.0002334080054424703\n","Batch 640/677, Loss: 0.00033736301702447236\n","Batch 650/677, Loss: 0.0003951971302740276\n","Batch 660/677, Loss: 0.0017785291420295835\n","Batch 670/677, Loss: 0.0033799682278186083\n","Training epoch completed in: 4m 15s\n","Train loss 0.009555249000199884 accuracy 0.9970457902511078\n","Batch 0/189, Test Loss: 1.1035133600234985\n","Batch 10/189, Test Loss: 1.2133561372756958\n","Batch 20/189, Test Loss: 0.08679565042257309\n","Batch 30/189, Test Loss: 0.4710441827774048\n","Batch 40/189, Test Loss: 0.6003831028938293\n","Batch 50/189, Test Loss: 0.6789766550064087\n","Batch 60/189, Test Loss: 0.9063799977302551\n","Batch 70/189, Test Loss: 1.3933758735656738\n","Batch 80/189, Test Loss: 0.4763824939727783\n","Batch 90/189, Test Loss: 0.552098274230957\n","Batch 100/189, Test Loss: 0.37679237127304077\n","Batch 110/189, Test Loss: 0.8983443379402161\n","Batch 120/189, Test Loss: 1.2688668966293335\n","Batch 130/189, Test Loss: 0.8681541681289673\n","Batch 140/189, Test Loss: 2.0628294944763184\n","Batch 150/189, Test Loss: 1.9772535562515259\n","Batch 160/189, Test Loss: 1.7190594673156738\n","Batch 170/189, Test Loss: 2.8605384826660156\n","Batch 180/189, Test Loss: 2.3876194953918457\n","Test evaluation completed in: 0m 24s\n","Test loss 1.1708903789825778 accuracy 0.7917635337097311\n","Test Precision: 0.7902012501327925\n","Test Recall: 0.791763533709731\n","Test F1 Score: 0.7908426053952338\n","Starting epoch 10/10\n","Batch 0/677, Loss: 0.0006265308475121856\n","Batch 10/677, Loss: 0.00026632638764567673\n","Batch 20/677, Loss: 0.0006278249202296138\n","Batch 30/677, Loss: 0.0009712307364679873\n","Batch 40/677, Loss: 0.0007407668745145202\n","Batch 50/677, Loss: 0.000793512852396816\n","Batch 60/677, Loss: 0.012322528287768364\n","Batch 70/677, Loss: 0.004033411853015423\n","Batch 80/677, Loss: 0.005819135811179876\n","Batch 90/677, Loss: 0.24807369709014893\n","Batch 100/677, Loss: 0.08368600159883499\n","Batch 110/677, Loss: 0.0006996093434281647\n","Batch 120/677, Loss: 0.0012569759273901582\n","Batch 130/677, Loss: 0.011172933503985405\n","Batch 140/677, Loss: 0.00023928812879603356\n","Batch 150/677, Loss: 0.00020994016085751355\n","Batch 160/677, Loss: 0.0023481908719986677\n","Batch 170/677, Loss: 0.0012928755022585392\n","Batch 180/677, Loss: 0.0014996121171861887\n","Batch 190/677, Loss: 0.0008822036325000226\n","Batch 200/677, Loss: 0.00013216644583735615\n","Batch 210/677, Loss: 0.00040105285006575286\n","Batch 220/677, Loss: 0.0005135556566528976\n","Batch 230/677, Loss: 0.0008127797627821565\n","Batch 240/677, Loss: 0.0098639614880085\n","Batch 250/677, Loss: 0.004300972446799278\n","Batch 260/677, Loss: 0.008722844533622265\n","Batch 270/677, Loss: 0.00039448399911634624\n","Batch 280/677, Loss: 0.00021431459754239768\n","Batch 290/677, Loss: 0.00042595493141561747\n","Batch 300/677, Loss: 0.0013665338046848774\n","Batch 310/677, Loss: 0.0001764794287737459\n","Batch 320/677, Loss: 0.0006266456912271678\n","Batch 330/677, Loss: 0.00015200243797153234\n","Batch 340/677, Loss: 0.0012899935245513916\n","Batch 350/677, Loss: 0.022470755502581596\n","Batch 360/677, Loss: 0.0005859773373231292\n","Batch 370/677, Loss: 0.001030329498462379\n","Batch 380/677, Loss: 0.015647176653146744\n","Batch 390/677, Loss: 0.0003158273466397077\n","Batch 400/677, Loss: 0.00033389832242392004\n","Batch 410/677, Loss: 0.000272538949502632\n","Batch 420/677, Loss: 0.00010606707655824721\n","Batch 430/677, Loss: 0.0007331885281018913\n","Batch 440/677, Loss: 0.0006029215874150395\n","Batch 450/677, Loss: 0.0009291386231780052\n","Batch 460/677, Loss: 0.006781681906431913\n","Batch 470/677, Loss: 0.00087045255349949\n","Batch 480/677, Loss: 0.00011100353731308132\n","Batch 490/677, Loss: 0.001818034565076232\n","Batch 500/677, Loss: 0.0009316428331658244\n","Batch 510/677, Loss: 0.001155151054263115\n","Batch 520/677, Loss: 0.0002711090783122927\n","Batch 530/677, Loss: 0.03377785533666611\n","Batch 540/677, Loss: 0.007494484540075064\n","Batch 550/677, Loss: 0.00043266790453344584\n","Batch 560/677, Loss: 0.0003694460610859096\n","Batch 570/677, Loss: 0.00028393027605488896\n","Batch 580/677, Loss: 0.0006553222192451358\n","Batch 590/677, Loss: 0.011737089604139328\n","Batch 600/677, Loss: 0.0006763339624740183\n","Batch 610/677, Loss: 0.00016806463827379048\n","Batch 620/677, Loss: 0.00011308198008919135\n","Batch 630/677, Loss: 0.00013792882964480668\n","Batch 640/677, Loss: 0.0008281930931843817\n","Batch 650/677, Loss: 0.0008438966469839215\n","Batch 660/677, Loss: 0.0008484518621116877\n","Batch 670/677, Loss: 0.0043512689881026745\n","Training epoch completed in: 4m 15s\n","Train loss 0.005229411322443881 accuracy 0.9981536189069424\n","Batch 0/189, Test Loss: 1.0763585567474365\n","Batch 10/189, Test Loss: 1.142059087753296\n","Batch 20/189, Test Loss: 0.10000132769346237\n","Batch 30/189, Test Loss: 0.37767738103866577\n","Batch 40/189, Test Loss: 0.4801556169986725\n","Batch 50/189, Test Loss: 0.7188974618911743\n","Batch 60/189, Test Loss: 0.8911565542221069\n","Batch 70/189, Test Loss: 1.4038842916488647\n","Batch 80/189, Test Loss: 0.49727603793144226\n","Batch 90/189, Test Loss: 0.5763413310050964\n","Batch 100/189, Test Loss: 0.3754003047943115\n","Batch 110/189, Test Loss: 0.9437783360481262\n","Batch 120/189, Test Loss: 1.2287968397140503\n","Batch 130/189, Test Loss: 0.9455115795135498\n","Batch 140/189, Test Loss: 2.121408700942993\n","Batch 150/189, Test Loss: 2.2483530044555664\n","Batch 160/189, Test Loss: 1.8389472961425781\n","Batch 170/189, Test Loss: 2.9769883155822754\n","Batch 180/189, Test Loss: 2.339346408843994\n","Test evaluation completed in: 0m 24s\n","Test loss 1.1990110755082843 accuracy 0.7944204583194953\n","Test Precision: 0.7923636482355133\n","Test Recall: 0.7944204583194951\n","Test F1 Score: 0.7931114343072918\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rtqyLyR2LXND"},"id":"rtqyLyR2LXND","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}
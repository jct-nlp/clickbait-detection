{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47296,"status":"ok","timestamp":1722593578498,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"},"user_tz":-180},"id":"X26qiUbJHNLn","outputId":"6ce9d01d-0a95-477e-b89d-87870ee90080"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install torch torchvision transformers pandas"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["02d092e1517f4b4fb52a33b4c388e3f4","f1a54def02b34b48816bbb417b314271","50be26be971144f789f1b8b787060cf2","aab2b9a582e64fcc86bd8b0236f7f2ee","fa88061446074d35a9c73cc8099ab03b","59eb56b7521241f797781e36231db5b5","f9b29fc7ba114407bfe016084d86df7a","79457734e68343e790050772a99f50f8","4e11e3ae44a548049f75a7af80517dd2","cf9484accf67414bb9b13eb18e129341","76e1397bd92e412a99bdac738ea477f8","beef9b9f41ea463b9f6e77e15a2a2801","2b0bc26a995840c0aca2d3bec1eb0119","541662e910c540b78499d921c303267e","2676a26d9b2b456c97b5b033d69565f6","21470fb9fb894ff79202686f97a0e20e","8774ceb09a244b438e9ab45375fdf6df","a0b95080001246e9ba30209dd1816eb4","76a76108e1cc43bbbf0bc2f534659b6a","6415f82c61384b709b3616597c4d5fc1","78bfe451044445d78cdc7687979d2035","7c21e291070248a4bd5c19db8797d960","ce38598984a24f639b8b791e17d6e125","82574b3f3e8b483485e6d8ce68a3bf8b","90eac4efc9924bdd932a7459531fbb4a","7c66ef25d7224fa2bacf2370ce38b437","377ff9020efb4b78a58c558e2b69bdb9","61c1437391cf419fad22ff93759c0aaa","68b90ca03e7f4785a72c6298bd42268d","bc41f7d64fc7481fbb8afa84beec07c7","8b676c00631f4578bef8d462f52c81cc","58102bf528b34305af1864c1aeaab771","e2b17b701f2f44a39f2fb5cc5979a638","472c244580824b2f83b019263019ecb4","27ad23feb6104e279004bc1733af3aac","2403f764d76247eeb9dd27a5baabf879","34fdb174596045b99419727e337354da","788adcf6ea844825a96b5a8f61a48967","d1b2b5855cde4a09b87ef4d494453c09","c25c7200b37243aba2444e5bf59b3e0d","9ce7c7ed427b483b884f9c59a2afd82a","52cf9cabe72f4046926e2fe69cdbf3a8","4958be39f6ab45469bd8d5e011342718","6c05597806de4edeb8ceee08c3d2c68e"]},"id":"pPSyyuKOEnOS","executionInfo":{"status":"ok","timestamp":1722600282057,"user_tz":-180,"elapsed":6695021,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"}},"outputId":"8ae9e0e0-e164-4e1b-e096-744870676324"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting main process\n","Loading the dataset.\n","Dataset loaded successfully.\n","Cleaning the dataset.\n","Dataset cleaned.\n","Splitting the dataset into training and test sets.\n","Dataset split into training and test sets.\n","Using device: cuda\n","Loading the tokenizer and model.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02d092e1517f4b4fb52a33b4c388e3f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/240k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beef9b9f41ea463b9f6e77e15a2a2801"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/456 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce38598984a24f639b8b791e17d6e125"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"472c244580824b2f83b019263019ecb4"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model and tokenizer loaded and moved to device.\n","Creating data loaders.\n","Data loaders created.\n","Optimizer, scheduler, and loss function defined.\n","Starting epoch 1/10\n","Batch 0/3470, Loss: 1.0827285051345825\n","Batch 10/3470, Loss: 0.5144497752189636\n","Batch 20/3470, Loss: 0.17353571951389313\n","Batch 30/3470, Loss: 0.4598327875137329\n","Batch 40/3470, Loss: 0.31629303097724915\n","Batch 50/3470, Loss: 0.23405306041240692\n","Batch 60/3470, Loss: 0.2039952129125595\n","Batch 70/3470, Loss: 0.5721286535263062\n","Batch 80/3470, Loss: 0.07955675572156906\n","Batch 90/3470, Loss: 0.17151899635791779\n","Batch 100/3470, Loss: 0.2519475221633911\n","Batch 110/3470, Loss: 0.07013112306594849\n","Batch 120/3470, Loss: 0.13710856437683105\n","Batch 130/3470, Loss: 0.10305342078208923\n","Batch 140/3470, Loss: 0.2123766988515854\n","Batch 150/3470, Loss: 0.01681072637438774\n","Batch 160/3470, Loss: 0.02795672044157982\n","Batch 170/3470, Loss: 0.021646304056048393\n","Batch 180/3470, Loss: 0.6213057041168213\n","Batch 190/3470, Loss: 0.23905104398727417\n","Batch 200/3470, Loss: 0.028719864785671234\n","Batch 210/3470, Loss: 0.06404643505811691\n","Batch 220/3470, Loss: 0.10115009546279907\n","Batch 230/3470, Loss: 0.007442461792379618\n","Batch 240/3470, Loss: 0.018295451998710632\n","Batch 250/3470, Loss: 0.19172392785549164\n","Batch 260/3470, Loss: 0.06288079172372818\n","Batch 270/3470, Loss: 0.03712531551718712\n","Batch 280/3470, Loss: 0.009832634590566158\n","Batch 290/3470, Loss: 0.12860523164272308\n","Batch 300/3470, Loss: 0.14491692185401917\n","Batch 310/3470, Loss: 0.25872689485549927\n","Batch 320/3470, Loss: 0.036300066858530045\n","Batch 330/3470, Loss: 0.007649668958038092\n","Batch 340/3470, Loss: 0.021914318203926086\n","Batch 350/3470, Loss: 0.07324446737766266\n","Batch 360/3470, Loss: 0.056446559727191925\n","Batch 370/3470, Loss: 0.1560896933078766\n","Batch 380/3470, Loss: 0.03868019953370094\n","Batch 390/3470, Loss: 0.2213691920042038\n","Batch 400/3470, Loss: 0.1867976039648056\n","Batch 410/3470, Loss: 0.02778765931725502\n","Batch 420/3470, Loss: 0.19786114990711212\n","Batch 430/3470, Loss: 0.009235910139977932\n","Batch 440/3470, Loss: 0.034621935337781906\n","Batch 450/3470, Loss: 0.10401275753974915\n","Batch 460/3470, Loss: 0.03491898998618126\n","Batch 470/3470, Loss: 0.06275022029876709\n","Batch 480/3470, Loss: 0.003283983562141657\n","Batch 490/3470, Loss: 0.3704172372817993\n","Batch 500/3470, Loss: 0.06488882750272751\n","Batch 510/3470, Loss: 0.06915413588285446\n","Batch 520/3470, Loss: 0.31354305148124695\n","Batch 530/3470, Loss: 0.3801020383834839\n","Batch 540/3470, Loss: 0.10886450111865997\n","Batch 550/3470, Loss: 0.08780043572187424\n","Batch 560/3470, Loss: 0.007208566647022963\n","Batch 570/3470, Loss: 0.0037089951802045107\n","Batch 580/3470, Loss: 0.06036969646811485\n","Batch 590/3470, Loss: 0.024912791326642036\n","Batch 600/3470, Loss: 0.0051366183906793594\n","Batch 610/3470, Loss: 0.012237702496349812\n","Batch 620/3470, Loss: 0.019498543813824654\n","Batch 630/3470, Loss: 0.05349263921380043\n","Batch 640/3470, Loss: 0.02787158079445362\n","Batch 650/3470, Loss: 0.004877092782407999\n","Batch 660/3470, Loss: 0.14378556609153748\n","Batch 670/3470, Loss: 0.010720820166170597\n","Batch 680/3470, Loss: 0.274455189704895\n","Batch 690/3470, Loss: 0.03385871648788452\n","Batch 700/3470, Loss: 0.003800516715273261\n","Batch 710/3470, Loss: 0.008564543910324574\n","Batch 720/3470, Loss: 0.005722951143980026\n","Batch 730/3470, Loss: 0.029631823301315308\n","Batch 740/3470, Loss: 0.11126555502414703\n","Batch 750/3470, Loss: 0.0032595968805253506\n","Batch 760/3470, Loss: 0.055452629923820496\n","Batch 770/3470, Loss: 0.020629236474633217\n","Batch 780/3470, Loss: 0.027853064239025116\n","Batch 790/3470, Loss: 0.05239034444093704\n","Batch 800/3470, Loss: 0.012270991690456867\n","Batch 810/3470, Loss: 0.021988574415445328\n","Batch 820/3470, Loss: 0.019901331514120102\n","Batch 830/3470, Loss: 0.010878543369472027\n","Batch 840/3470, Loss: 0.13064724206924438\n","Batch 850/3470, Loss: 0.01345446053892374\n","Batch 860/3470, Loss: 0.07778014242649078\n","Batch 870/3470, Loss: 0.6466407775878906\n","Batch 880/3470, Loss: 0.02575167454779148\n","Batch 890/3470, Loss: 0.01954350247979164\n","Batch 900/3470, Loss: 0.05666216090321541\n","Batch 910/3470, Loss: 0.49179065227508545\n","Batch 920/3470, Loss: 0.0073989988304674625\n","Batch 930/3470, Loss: 0.0199130866676569\n","Batch 940/3470, Loss: 0.003727362956851721\n","Batch 950/3470, Loss: 0.24514061212539673\n","Batch 960/3470, Loss: 0.004026645794510841\n","Batch 970/3470, Loss: 0.037146247923374176\n","Batch 980/3470, Loss: 0.01627671718597412\n","Batch 990/3470, Loss: 0.03136485442519188\n","Batch 1000/3470, Loss: 0.07515077292919159\n","Batch 1010/3470, Loss: 0.029051020741462708\n","Batch 1020/3470, Loss: 0.29126352071762085\n","Batch 1030/3470, Loss: 0.012891673482954502\n","Batch 1040/3470, Loss: 0.03875069320201874\n","Batch 1050/3470, Loss: 0.010653120465576649\n","Batch 1060/3470, Loss: 0.02621307782828808\n","Batch 1070/3470, Loss: 0.486579954624176\n","Batch 1080/3470, Loss: 0.011365219950675964\n","Batch 1090/3470, Loss: 0.010360836051404476\n","Batch 1100/3470, Loss: 0.015146877616643906\n","Batch 1110/3470, Loss: 0.128964364528656\n","Batch 1120/3470, Loss: 0.006175554357469082\n","Batch 1130/3470, Loss: 0.38282355666160583\n","Batch 1140/3470, Loss: 0.2604077160358429\n","Batch 1150/3470, Loss: 0.03138881176710129\n","Batch 1160/3470, Loss: 0.015803661197423935\n","Batch 1170/3470, Loss: 0.05400159955024719\n","Batch 1180/3470, Loss: 0.012961756438016891\n","Batch 1190/3470, Loss: 0.05140937492251396\n","Batch 1200/3470, Loss: 0.005037632770836353\n","Batch 1210/3470, Loss: 0.0018439398845657706\n","Batch 1220/3470, Loss: 0.01497532706707716\n","Batch 1230/3470, Loss: 0.00867868959903717\n","Batch 1240/3470, Loss: 0.13518026471138\n","Batch 1250/3470, Loss: 0.011278635822236538\n","Batch 1260/3470, Loss: 0.39918941259384155\n","Batch 1270/3470, Loss: 0.003708125092089176\n","Batch 1280/3470, Loss: 0.026305418461561203\n","Batch 1290/3470, Loss: 0.010740852914750576\n","Batch 1300/3470, Loss: 0.08580034971237183\n","Batch 1310/3470, Loss: 0.007623237557709217\n","Batch 1320/3470, Loss: 0.18061403930187225\n","Batch 1330/3470, Loss: 0.028498532250523567\n","Batch 1340/3470, Loss: 0.011771154589951038\n","Batch 1350/3470, Loss: 0.07695835828781128\n","Batch 1360/3470, Loss: 0.1382489800453186\n","Batch 1370/3470, Loss: 0.0670066848397255\n","Batch 1380/3470, Loss: 0.002091122092679143\n","Batch 1390/3470, Loss: 0.005083341617137194\n","Batch 1400/3470, Loss: 0.10455106943845749\n","Batch 1410/3470, Loss: 0.003059810260310769\n","Batch 1420/3470, Loss: 0.004114695359021425\n","Batch 1430/3470, Loss: 0.00606407830491662\n","Batch 1440/3470, Loss: 0.10493424534797668\n","Batch 1450/3470, Loss: 0.008747030049562454\n","Batch 1460/3470, Loss: 0.02417939342558384\n","Batch 1470/3470, Loss: 0.002040168270468712\n","Batch 1480/3470, Loss: 0.037832390516996384\n","Batch 1490/3470, Loss: 0.023522011935710907\n","Batch 1500/3470, Loss: 0.18362393975257874\n","Batch 1510/3470, Loss: 0.008828059770166874\n","Batch 1520/3470, Loss: 0.030358891934156418\n","Batch 1530/3470, Loss: 0.04059455543756485\n","Batch 1540/3470, Loss: 0.03538568690419197\n","Batch 1550/3470, Loss: 0.4307790696620941\n","Batch 1560/3470, Loss: 0.0474301241338253\n","Batch 1570/3470, Loss: 0.013225339353084564\n","Batch 1580/3470, Loss: 0.020259082317352295\n","Batch 1590/3470, Loss: 0.06838610768318176\n","Batch 1600/3470, Loss: 0.002034558681771159\n","Batch 1610/3470, Loss: 0.028861192986369133\n","Batch 1620/3470, Loss: 0.02189583145081997\n","Batch 1630/3470, Loss: 0.0014571615029126406\n","Batch 1640/3470, Loss: 0.005360788200050592\n","Batch 1650/3470, Loss: 0.0060286796651780605\n","Batch 1660/3470, Loss: 0.003694945713505149\n","Batch 1670/3470, Loss: 0.002049138071015477\n","Batch 1680/3470, Loss: 0.017015447840094566\n","Batch 1690/3470, Loss: 0.30597224831581116\n","Batch 1700/3470, Loss: 0.005834088660776615\n","Batch 1710/3470, Loss: 0.0027444520965218544\n","Batch 1720/3470, Loss: 0.0039254832081496716\n","Batch 1730/3470, Loss: 0.2939753830432892\n","Batch 1740/3470, Loss: 0.26385757327079773\n","Batch 1750/3470, Loss: 0.009761114604771137\n","Batch 1760/3470, Loss: 0.008299116045236588\n","Batch 1770/3470, Loss: 0.022972984239459038\n","Batch 1780/3470, Loss: 0.0911797285079956\n","Batch 1790/3470, Loss: 0.008510366082191467\n","Batch 1800/3470, Loss: 0.15437926352024078\n","Batch 1810/3470, Loss: 0.19421905279159546\n","Batch 1820/3470, Loss: 0.1607266664505005\n","Batch 1830/3470, Loss: 0.013047413900494576\n","Batch 1840/3470, Loss: 0.10581401735544205\n","Batch 1850/3470, Loss: 0.21473881602287292\n","Batch 1860/3470, Loss: 0.08749312907457352\n","Batch 1870/3470, Loss: 0.1440742462873459\n","Batch 1880/3470, Loss: 0.07085392624139786\n","Batch 1890/3470, Loss: 0.118509940803051\n","Batch 1900/3470, Loss: 0.004534194711595774\n","Batch 1910/3470, Loss: 0.13989850878715515\n","Batch 1920/3470, Loss: 0.15532101690769196\n","Batch 1930/3470, Loss: 0.002534089144319296\n","Batch 1940/3470, Loss: 0.055668655782938004\n","Batch 1950/3470, Loss: 0.003490041708573699\n","Batch 1960/3470, Loss: 0.020232852548360825\n","Batch 1970/3470, Loss: 0.14058417081832886\n","Batch 1980/3470, Loss: 0.028963664546608925\n","Batch 1990/3470, Loss: 0.28423500061035156\n","Batch 2000/3470, Loss: 0.006263940129429102\n","Batch 2010/3470, Loss: 0.0074503603391349316\n","Batch 2020/3470, Loss: 0.10888202488422394\n","Batch 2030/3470, Loss: 0.0713222548365593\n","Batch 2040/3470, Loss: 0.0030611346010118723\n","Batch 2050/3470, Loss: 0.065757617354393\n","Batch 2060/3470, Loss: 0.020764989778399467\n","Batch 2070/3470, Loss: 0.019736003130674362\n","Batch 2080/3470, Loss: 0.0042688013054430485\n","Batch 2090/3470, Loss: 0.0067910137586295605\n","Batch 2100/3470, Loss: 0.008278822526335716\n","Batch 2110/3470, Loss: 0.017113199457526207\n","Batch 2120/3470, Loss: 0.000959891767706722\n","Batch 2130/3470, Loss: 0.055978454649448395\n","Batch 2140/3470, Loss: 0.22816398739814758\n","Batch 2150/3470, Loss: 0.018717307597398758\n","Batch 2160/3470, Loss: 0.1243426501750946\n","Batch 2170/3470, Loss: 0.014928373508155346\n","Batch 2180/3470, Loss: 0.029709381982684135\n","Batch 2190/3470, Loss: 0.014545481652021408\n","Batch 2200/3470, Loss: 0.08779679983854294\n","Batch 2210/3470, Loss: 0.01650281995534897\n","Batch 2220/3470, Loss: 0.017720108851790428\n","Batch 2230/3470, Loss: 0.003220047801733017\n","Batch 2240/3470, Loss: 0.018724385648965836\n","Batch 2250/3470, Loss: 0.009840763173997402\n","Batch 2260/3470, Loss: 0.14268821477890015\n","Batch 2270/3470, Loss: 0.0248834528028965\n","Batch 2280/3470, Loss: 0.009167569689452648\n","Batch 2290/3470, Loss: 0.002896784571930766\n","Batch 2300/3470, Loss: 0.003502107225358486\n","Batch 2310/3470, Loss: 0.0007225071312859654\n","Batch 2320/3470, Loss: 0.29262974858283997\n","Batch 2330/3470, Loss: 0.05137379840016365\n","Batch 2340/3470, Loss: 0.014238426461815834\n","Batch 2350/3470, Loss: 0.025126947090029716\n","Batch 2360/3470, Loss: 0.002269695745781064\n","Batch 2370/3470, Loss: 0.422799676656723\n","Batch 2380/3470, Loss: 0.005618985276669264\n","Batch 2390/3470, Loss: 0.05720888823270798\n","Batch 2400/3470, Loss: 0.1499686986207962\n","Batch 2410/3470, Loss: 0.010317450389266014\n","Batch 2420/3470, Loss: 0.0018331699538975954\n","Batch 2430/3470, Loss: 0.006464795675128698\n","Batch 2440/3470, Loss: 0.20532242953777313\n","Batch 2450/3470, Loss: 0.01283485908061266\n","Batch 2460/3470, Loss: 0.0128164142370224\n","Batch 2470/3470, Loss: 0.24863818287849426\n","Batch 2480/3470, Loss: 0.008376608602702618\n","Batch 2490/3470, Loss: 0.49212968349456787\n","Batch 2500/3470, Loss: 0.17593134939670563\n","Batch 2510/3470, Loss: 0.09620853513479233\n","Batch 2520/3470, Loss: 0.04887691140174866\n","Batch 2530/3470, Loss: 0.0055273063480854034\n","Batch 2540/3470, Loss: 0.004171042237430811\n","Batch 2550/3470, Loss: 0.07592768222093582\n","Batch 2560/3470, Loss: 0.013458069413900375\n","Batch 2570/3470, Loss: 0.011066733859479427\n","Batch 2580/3470, Loss: 0.04948360100388527\n","Batch 2590/3470, Loss: 0.006766985636204481\n","Batch 2600/3470, Loss: 0.031123735010623932\n","Batch 2610/3470, Loss: 0.006631221156567335\n","Batch 2620/3470, Loss: 0.19261738657951355\n","Batch 2630/3470, Loss: 0.060833945870399475\n","Batch 2640/3470, Loss: 0.16662898659706116\n","Batch 2650/3470, Loss: 0.0015679618809372187\n","Batch 2660/3470, Loss: 0.07063064724206924\n","Batch 2670/3470, Loss: 0.01951649598777294\n","Batch 2680/3470, Loss: 0.08056078851222992\n","Batch 2690/3470, Loss: 0.03775437921285629\n","Batch 2700/3470, Loss: 0.08248787373304367\n","Batch 2710/3470, Loss: 0.020714767277240753\n","Batch 2720/3470, Loss: 0.06259088218212128\n","Batch 2730/3470, Loss: 0.05198323354125023\n","Batch 2740/3470, Loss: 0.12311504781246185\n","Batch 2750/3470, Loss: 0.00171513797249645\n","Batch 2760/3470, Loss: 0.1499919891357422\n","Batch 2770/3470, Loss: 0.05189051851630211\n","Batch 2780/3470, Loss: 0.12423643469810486\n","Batch 2790/3470, Loss: 0.03832031786441803\n","Batch 2800/3470, Loss: 0.004146468825638294\n","Batch 2810/3470, Loss: 0.0006197396432980895\n","Batch 2820/3470, Loss: 0.0019409649539738894\n","Batch 2830/3470, Loss: 0.0034320109989494085\n","Batch 2840/3470, Loss: 0.02484888769686222\n","Batch 2850/3470, Loss: 0.0013512014411389828\n","Batch 2860/3470, Loss: 0.000994406407698989\n","Batch 2870/3470, Loss: 0.010270449332892895\n","Batch 2880/3470, Loss: 0.011849377304315567\n","Batch 2890/3470, Loss: 0.0005247507942840457\n","Batch 2900/3470, Loss: 0.01775551401078701\n","Batch 2910/3470, Loss: 0.005398585926741362\n","Batch 2920/3470, Loss: 0.07700532674789429\n","Batch 2930/3470, Loss: 0.0006607045652344823\n","Batch 2940/3470, Loss: 0.07840169966220856\n","Batch 2950/3470, Loss: 0.005849829409271479\n","Batch 2960/3470, Loss: 0.007631409913301468\n","Batch 2970/3470, Loss: 0.3944056034088135\n","Batch 2980/3470, Loss: 0.008029201999306679\n","Batch 2990/3470, Loss: 0.5065388083457947\n","Batch 3000/3470, Loss: 0.0005929681356064975\n","Batch 3010/3470, Loss: 0.0696895569562912\n","Batch 3020/3470, Loss: 0.04384029284119606\n","Batch 3030/3470, Loss: 0.008056413382291794\n","Batch 3040/3470, Loss: 0.0042641740292310715\n","Batch 3050/3470, Loss: 0.004277389030903578\n","Batch 3060/3470, Loss: 0.025596965104341507\n","Batch 3070/3470, Loss: 0.007247375790029764\n","Batch 3080/3470, Loss: 0.14948512613773346\n","Batch 3090/3470, Loss: 0.1700150966644287\n","Batch 3100/3470, Loss: 0.11865944415330887\n","Batch 3110/3470, Loss: 0.08426519483327866\n","Batch 3120/3470, Loss: 0.046425960958004\n","Batch 3130/3470, Loss: 0.025379281491041183\n","Batch 3140/3470, Loss: 0.0012266372796148062\n","Batch 3150/3470, Loss: 0.024752015247941017\n","Batch 3160/3470, Loss: 0.027349695563316345\n","Batch 3170/3470, Loss: 0.0028597956988960505\n","Batch 3180/3470, Loss: 0.003515998600050807\n","Batch 3190/3470, Loss: 0.0030503585003316402\n","Batch 3200/3470, Loss: 0.018350442871451378\n","Batch 3210/3470, Loss: 0.048819150775671005\n","Batch 3220/3470, Loss: 0.350994348526001\n","Batch 3230/3470, Loss: 0.07587777823209763\n","Batch 3240/3470, Loss: 0.009966621175408363\n","Batch 3250/3470, Loss: 0.00741315633058548\n","Batch 3260/3470, Loss: 0.2755224108695984\n","Batch 3270/3470, Loss: 0.034945689141750336\n","Batch 3280/3470, Loss: 0.06496205925941467\n","Batch 3290/3470, Loss: 0.08016832917928696\n","Batch 3300/3470, Loss: 0.006153934169560671\n","Batch 3310/3470, Loss: 0.009763116016983986\n","Batch 3320/3470, Loss: 0.12991660833358765\n","Batch 3330/3470, Loss: 0.13708806037902832\n","Batch 3340/3470, Loss: 0.0005499197286553681\n","Batch 3350/3470, Loss: 0.0392482690513134\n","Batch 3360/3470, Loss: 0.024500586092472076\n","Batch 3370/3470, Loss: 0.025615738704800606\n","Batch 3380/3470, Loss: 0.034337908029556274\n","Batch 3390/3470, Loss: 0.002303454792127013\n","Batch 3400/3470, Loss: 0.008040270768105984\n","Batch 3410/3470, Loss: 0.010181024670600891\n","Batch 3420/3470, Loss: 0.028129735961556435\n","Batch 3430/3470, Loss: 0.0010415716096758842\n","Batch 3440/3470, Loss: 0.00021426794410217553\n","Batch 3450/3470, Loss: 0.011234583333134651\n","Batch 3460/3470, Loss: 0.0025408370420336723\n","Training epoch completed in: 9m 35s\n","Train loss 0.07364912183205238 accuracy 0.9731973989949025\n","Batch 0/868, Test Loss: 0.0004037803737446666\n","Batch 10/868, Test Loss: 0.0270642451941967\n","Batch 20/868, Test Loss: 0.0004997348878532648\n","Batch 30/868, Test Loss: 0.13103681802749634\n","Batch 40/868, Test Loss: 0.00022859001182951033\n","Batch 50/868, Test Loss: 0.02242407016456127\n","Batch 60/868, Test Loss: 0.06336776167154312\n","Batch 70/868, Test Loss: 0.012972904369235039\n","Batch 80/868, Test Loss: 0.0018575664144009352\n","Batch 90/868, Test Loss: 0.0007662528078071773\n","Batch 100/868, Test Loss: 0.023747336119413376\n","Batch 110/868, Test Loss: 0.11575024574995041\n","Batch 120/868, Test Loss: 0.1220841258764267\n","Batch 130/868, Test Loss: 0.0002215640852227807\n","Batch 140/868, Test Loss: 0.050683751702308655\n","Batch 150/868, Test Loss: 0.10146445780992508\n","Batch 160/868, Test Loss: 0.03665472939610481\n","Batch 170/868, Test Loss: 0.1566981077194214\n","Batch 180/868, Test Loss: 0.07956700026988983\n","Batch 190/868, Test Loss: 0.001157409162260592\n","Batch 200/868, Test Loss: 0.031746506690979004\n","Batch 210/868, Test Loss: 0.0006835362873971462\n","Batch 220/868, Test Loss: 0.0015436668181791902\n","Batch 230/868, Test Loss: 0.03773122280836105\n","Batch 240/868, Test Loss: 0.0020464425906538963\n","Batch 250/868, Test Loss: 0.005903967656195164\n","Batch 260/868, Test Loss: 0.11717905104160309\n","Batch 270/868, Test Loss: 0.1305960863828659\n","Batch 280/868, Test Loss: 0.025773806497454643\n","Batch 290/868, Test Loss: 0.007260962855070829\n","Batch 300/868, Test Loss: 0.07372080534696579\n","Batch 310/868, Test Loss: 0.009993537329137325\n","Batch 320/868, Test Loss: 0.021966872736811638\n","Batch 330/868, Test Loss: 0.16445475816726685\n","Batch 340/868, Test Loss: 0.007244762498885393\n","Batch 350/868, Test Loss: 0.0051593780517578125\n","Batch 360/868, Test Loss: 0.10928847640752792\n","Batch 370/868, Test Loss: 0.010459410026669502\n","Batch 380/868, Test Loss: 0.0005947094759903848\n","Batch 390/868, Test Loss: 0.00034816411789506674\n","Batch 400/868, Test Loss: 0.0004545914416667074\n","Batch 410/868, Test Loss: 0.017581991851329803\n","Batch 420/868, Test Loss: 0.07441402971744537\n","Batch 430/868, Test Loss: 0.004982651676982641\n","Batch 440/868, Test Loss: 0.04598080739378929\n","Batch 450/868, Test Loss: 0.014478711411356926\n","Batch 460/868, Test Loss: 0.009929333813488483\n","Batch 470/868, Test Loss: 0.0054877824150025845\n","Batch 480/868, Test Loss: 0.00362774939276278\n","Batch 490/868, Test Loss: 0.03672979027032852\n","Batch 500/868, Test Loss: 0.0002783512172754854\n","Batch 510/868, Test Loss: 0.023676440119743347\n","Batch 520/868, Test Loss: 0.00047725645708851516\n","Batch 530/868, Test Loss: 0.0019961653742939234\n","Batch 540/868, Test Loss: 0.005578427575528622\n","Batch 550/868, Test Loss: 0.01583837904036045\n","Batch 560/868, Test Loss: 0.048292651772499084\n","Batch 570/868, Test Loss: 0.009273560717701912\n","Batch 580/868, Test Loss: 0.0009078753064386547\n","Batch 590/868, Test Loss: 0.06661001592874527\n","Batch 600/868, Test Loss: 0.028657181188464165\n","Batch 610/868, Test Loss: 0.003957229200750589\n","Batch 620/868, Test Loss: 0.09412728995084763\n","Batch 630/868, Test Loss: 0.11906421929597855\n","Batch 640/868, Test Loss: 0.15546630322933197\n","Batch 650/868, Test Loss: 0.001158112077973783\n","Batch 660/868, Test Loss: 0.1621847003698349\n","Batch 670/868, Test Loss: 0.0008230669191107154\n","Batch 680/868, Test Loss: 0.037896838039159775\n","Batch 690/868, Test Loss: 0.0824207291007042\n","Batch 700/868, Test Loss: 0.17722617089748383\n","Batch 710/868, Test Loss: 0.0004276419058442116\n","Batch 720/868, Test Loss: 0.06646853685379028\n","Batch 730/868, Test Loss: 0.02862699329853058\n","Batch 740/868, Test Loss: 0.006700026337057352\n","Batch 750/868, Test Loss: 0.43144872784614563\n","Batch 760/868, Test Loss: 0.0015377068193629384\n","Batch 770/868, Test Loss: 0.004219995811581612\n","Batch 780/868, Test Loss: 0.0671110600233078\n","Batch 790/868, Test Loss: 0.0002961377031169832\n","Batch 800/868, Test Loss: 0.0006298814550973475\n","Batch 810/868, Test Loss: 0.0003375333617441356\n","Batch 820/868, Test Loss: 0.0027512090746313334\n","Batch 830/868, Test Loss: 0.025412453338503838\n","Batch 840/868, Test Loss: 0.5557277202606201\n","Batch 850/868, Test Loss: 0.6793484091758728\n","Batch 860/868, Test Loss: 0.0009290141169913113\n","Test evaluation completed in: 0m 46s\n","Test loss 0.052296452367409915 accuracy 0.9817723342939482\n","Test Precision: 0.9817761054423507\n","Test Recall: 0.9817723342939482\n","Test F1 Score: 0.9817742099963767\n","Starting epoch 2/10\n","Batch 0/3470, Loss: 0.0011221879394724965\n","Batch 10/3470, Loss: 0.10527579486370087\n","Batch 20/3470, Loss: 0.0005668821977451444\n","Batch 30/3470, Loss: 0.058475494384765625\n","Batch 40/3470, Loss: 0.03869111090898514\n","Batch 50/3470, Loss: 0.0005146684125065804\n","Batch 60/3470, Loss: 0.001883793156594038\n","Batch 70/3470, Loss: 0.03409741446375847\n","Batch 80/3470, Loss: 0.004636663477867842\n","Batch 90/3470, Loss: 0.023801390081644058\n","Batch 100/3470, Loss: 0.06106738746166229\n","Batch 110/3470, Loss: 0.004095358774065971\n","Batch 120/3470, Loss: 0.02179800532758236\n","Batch 130/3470, Loss: 0.032371073961257935\n","Batch 140/3470, Loss: 0.2175130397081375\n","Batch 150/3470, Loss: 0.002576843136921525\n","Batch 160/3470, Loss: 0.018087461590766907\n","Batch 170/3470, Loss: 0.001919470727443695\n","Batch 180/3470, Loss: 0.1945638209581375\n","Batch 190/3470, Loss: 0.06554350256919861\n","Batch 200/3470, Loss: 0.0196346715092659\n","Batch 210/3470, Loss: 0.006092619150876999\n","Batch 220/3470, Loss: 0.026486001908779144\n","Batch 230/3470, Loss: 0.000666813226416707\n","Batch 240/3470, Loss: 0.0004988859291188419\n","Batch 250/3470, Loss: 0.10552409291267395\n","Batch 260/3470, Loss: 0.03602612018585205\n","Batch 270/3470, Loss: 0.0006749153835698962\n","Batch 280/3470, Loss: 0.000629184884019196\n","Batch 290/3470, Loss: 0.029038434848189354\n","Batch 300/3470, Loss: 0.0014272057451307774\n","Batch 310/3470, Loss: 0.09384547919034958\n","Batch 320/3470, Loss: 0.0004203723801765591\n","Batch 330/3470, Loss: 0.0010238521499559283\n","Batch 340/3470, Loss: 0.0002366983680985868\n","Batch 350/3470, Loss: 0.01338114868849516\n","Batch 360/3470, Loss: 0.0030191403347998857\n","Batch 370/3470, Loss: 0.04901023954153061\n","Batch 380/3470, Loss: 0.0006159872282296419\n","Batch 390/3470, Loss: 0.18318761885166168\n","Batch 400/3470, Loss: 0.1728585660457611\n","Batch 410/3470, Loss: 0.0317690446972847\n","Batch 420/3470, Loss: 0.04917789250612259\n","Batch 430/3470, Loss: 0.0006004683673381805\n","Batch 440/3470, Loss: 0.07462222874164581\n","Batch 450/3470, Loss: 0.0014759107725694776\n","Batch 460/3470, Loss: 0.02850027196109295\n","Batch 470/3470, Loss: 0.0022394650150090456\n","Batch 480/3470, Loss: 0.01919824630022049\n","Batch 490/3470, Loss: 0.13277676701545715\n","Batch 500/3470, Loss: 0.05875355005264282\n","Batch 510/3470, Loss: 0.003324852790683508\n","Batch 520/3470, Loss: 0.004677129909396172\n","Batch 530/3470, Loss: 0.520594596862793\n","Batch 540/3470, Loss: 0.07281514257192612\n","Batch 550/3470, Loss: 0.001396043924614787\n","Batch 560/3470, Loss: 0.024115394800901413\n","Batch 570/3470, Loss: 0.0006980462349019945\n","Batch 580/3470, Loss: 0.003444909118115902\n","Batch 590/3470, Loss: 0.0016526503022760153\n","Batch 600/3470, Loss: 0.0015979059971868992\n","Batch 610/3470, Loss: 0.006671618204563856\n","Batch 620/3470, Loss: 0.004930164664983749\n","Batch 630/3470, Loss: 0.012728292495012283\n","Batch 640/3470, Loss: 0.0020607919432222843\n","Batch 650/3470, Loss: 0.004713892005383968\n","Batch 660/3470, Loss: 0.08236224949359894\n","Batch 670/3470, Loss: 0.00042802770622074604\n","Batch 680/3470, Loss: 0.030452702194452286\n","Batch 690/3470, Loss: 0.000343969848472625\n","Batch 700/3470, Loss: 0.00024209618277382106\n","Batch 710/3470, Loss: 0.0010949971619993448\n","Batch 720/3470, Loss: 0.001996671548113227\n","Batch 730/3470, Loss: 0.000764338590670377\n","Batch 740/3470, Loss: 0.08087257295846939\n","Batch 750/3470, Loss: 0.00041710841469466686\n","Batch 760/3470, Loss: 0.0012074606493115425\n","Batch 770/3470, Loss: 0.025798901915550232\n","Batch 780/3470, Loss: 0.0015017855912446976\n","Batch 790/3470, Loss: 0.048949480056762695\n","Batch 800/3470, Loss: 0.0005588174099102616\n","Batch 810/3470, Loss: 0.0004069716378580779\n","Batch 820/3470, Loss: 0.007897187024354935\n","Batch 830/3470, Loss: 0.00023706343199592084\n","Batch 840/3470, Loss: 0.00029293340048752725\n","Batch 850/3470, Loss: 0.00025346409529447556\n","Batch 860/3470, Loss: 0.013440858572721481\n","Batch 870/3470, Loss: 0.031675197184085846\n","Batch 880/3470, Loss: 0.00027152663096785545\n","Batch 890/3470, Loss: 0.00301999575458467\n","Batch 900/3470, Loss: 0.04523565620183945\n","Batch 910/3470, Loss: 0.43423888087272644\n","Batch 920/3470, Loss: 0.001909241545945406\n","Batch 930/3470, Loss: 0.0026487649884074926\n","Batch 940/3470, Loss: 0.00113653892185539\n","Batch 950/3470, Loss: 0.03843839094042778\n","Batch 960/3470, Loss: 0.021964581683278084\n","Batch 970/3470, Loss: 0.0024110078811645508\n","Batch 980/3470, Loss: 0.005152941215783358\n","Batch 990/3470, Loss: 0.006464547477662563\n","Batch 1000/3470, Loss: 0.05781799927353859\n","Batch 1010/3470, Loss: 0.00972753670066595\n","Batch 1020/3470, Loss: 0.2636078894138336\n","Batch 1030/3470, Loss: 0.0022883075289428234\n","Batch 1040/3470, Loss: 0.010834180749952793\n","Batch 1050/3470, Loss: 0.0011368980631232262\n","Batch 1060/3470, Loss: 0.004876027349382639\n","Batch 1070/3470, Loss: 0.3979805111885071\n","Batch 1080/3470, Loss: 0.004202554002404213\n","Batch 1090/3470, Loss: 0.005079162772744894\n","Batch 1100/3470, Loss: 0.02102629654109478\n","Batch 1110/3470, Loss: 0.0827217549085617\n","Batch 1120/3470, Loss: 0.0017036942299455404\n","Batch 1130/3470, Loss: 0.15029534697532654\n","Batch 1140/3470, Loss: 0.010454727336764336\n","Batch 1150/3470, Loss: 0.0032614595256745815\n","Batch 1160/3470, Loss: 0.0033508616033941507\n","Batch 1170/3470, Loss: 0.009133183397352695\n","Batch 1180/3470, Loss: 0.0013092446606606245\n","Batch 1190/3470, Loss: 0.030730407685041428\n","Batch 1200/3470, Loss: 0.0009675278561189771\n","Batch 1210/3470, Loss: 0.0005679900059476495\n","Batch 1220/3470, Loss: 0.003480724524706602\n","Batch 1230/3470, Loss: 0.0025006961077451706\n","Batch 1240/3470, Loss: 0.017968397587537766\n","Batch 1250/3470, Loss: 0.005945926997810602\n","Batch 1260/3470, Loss: 0.048682354390621185\n","Batch 1270/3470, Loss: 0.0004326827183831483\n","Batch 1280/3470, Loss: 0.007506947033107281\n","Batch 1290/3470, Loss: 0.001993311569094658\n","Batch 1300/3470, Loss: 0.009030858054757118\n","Batch 1310/3470, Loss: 0.0012997656594961882\n","Batch 1320/3470, Loss: 0.1716323047876358\n","Batch 1330/3470, Loss: 0.004317592829465866\n","Batch 1340/3470, Loss: 0.07649846374988556\n","Batch 1350/3470, Loss: 0.021795867010951042\n","Batch 1360/3470, Loss: 0.08281999826431274\n","Batch 1370/3470, Loss: 0.0009411757928319275\n","Batch 1380/3470, Loss: 0.0027252850122749805\n","Batch 1390/3470, Loss: 0.0001845967781264335\n","Batch 1400/3470, Loss: 0.0002595644036773592\n","Batch 1410/3470, Loss: 0.00019788813369814306\n","Batch 1420/3470, Loss: 0.0005981698632240295\n","Batch 1430/3470, Loss: 0.0009885895997285843\n","Batch 1440/3470, Loss: 0.045036572962999344\n","Batch 1450/3470, Loss: 0.00047214311780408025\n","Batch 1460/3470, Loss: 0.002379323123022914\n","Batch 1470/3470, Loss: 0.0003960923640988767\n","Batch 1480/3470, Loss: 0.0037250863388180733\n","Batch 1490/3470, Loss: 0.027645189315080643\n","Batch 1500/3470, Loss: 0.14182907342910767\n","Batch 1510/3470, Loss: 0.0007017880561761558\n","Batch 1520/3470, Loss: 0.005310306791216135\n","Batch 1530/3470, Loss: 0.09149664640426636\n","Batch 1540/3470, Loss: 0.02823037840425968\n","Batch 1550/3470, Loss: 0.30209511518478394\n","Batch 1560/3470, Loss: 0.003769031260162592\n","Batch 1570/3470, Loss: 0.0009799608960747719\n","Batch 1580/3470, Loss: 0.0020793189760297537\n","Batch 1590/3470, Loss: 0.0003973717975895852\n","Batch 1600/3470, Loss: 0.0003471375966910273\n","Batch 1610/3470, Loss: 0.0004712040536105633\n","Batch 1620/3470, Loss: 0.0006405531312339008\n","Batch 1630/3470, Loss: 0.00044762788456864655\n","Batch 1640/3470, Loss: 0.00297782220877707\n","Batch 1650/3470, Loss: 0.0005246704677119851\n","Batch 1660/3470, Loss: 0.003967850934714079\n","Batch 1670/3470, Loss: 0.0009553839918226004\n","Batch 1680/3470, Loss: 0.0007692232611589134\n","Batch 1690/3470, Loss: 0.04114709421992302\n","Batch 1700/3470, Loss: 0.0002930646878667176\n","Batch 1710/3470, Loss: 0.00021125294733792543\n","Batch 1720/3470, Loss: 0.0004942999803461134\n","Batch 1730/3470, Loss: 0.0071924361400306225\n","Batch 1740/3470, Loss: 0.026622602716088295\n","Batch 1750/3470, Loss: 0.0007238571997731924\n","Batch 1760/3470, Loss: 0.00024147226940840483\n","Batch 1770/3470, Loss: 0.00019907011301256716\n","Batch 1780/3470, Loss: 0.02095181681215763\n","Batch 1790/3470, Loss: 0.00020790826238226146\n","Batch 1800/3470, Loss: 0.12779834866523743\n","Batch 1810/3470, Loss: 0.03225947171449661\n","Batch 1820/3470, Loss: 0.003952423110604286\n","Batch 1830/3470, Loss: 0.0026328247040510178\n","Batch 1840/3470, Loss: 0.003880716161802411\n","Batch 1850/3470, Loss: 0.1399783492088318\n","Batch 1860/3470, Loss: 0.006263237912207842\n","Batch 1870/3470, Loss: 0.007942469790577888\n","Batch 1880/3470, Loss: 0.011856595985591412\n","Batch 1890/3470, Loss: 0.09519921988248825\n","Batch 1900/3470, Loss: 0.001314860419370234\n","Batch 1910/3470, Loss: 0.020738493651151657\n","Batch 1920/3470, Loss: 0.015111842192709446\n","Batch 1930/3470, Loss: 0.0007404561620205641\n","Batch 1940/3470, Loss: 0.026669753715395927\n","Batch 1950/3470, Loss: 0.011620496399700642\n","Batch 1960/3470, Loss: 0.012692587450146675\n","Batch 1970/3470, Loss: 0.03638352081179619\n","Batch 1980/3470, Loss: 0.0017017085338011384\n","Batch 1990/3470, Loss: 0.003094561630859971\n","Batch 2000/3470, Loss: 0.0004899168852716684\n","Batch 2010/3470, Loss: 0.022663239389657974\n","Batch 2020/3470, Loss: 0.032845981419086456\n","Batch 2030/3470, Loss: 0.113144151866436\n","Batch 2040/3470, Loss: 0.0007596575887873769\n","Batch 2050/3470, Loss: 0.07174372673034668\n","Batch 2060/3470, Loss: 0.018991373479366302\n","Batch 2070/3470, Loss: 0.0010026419768109918\n","Batch 2080/3470, Loss: 0.0009435909450985491\n","Batch 2090/3470, Loss: 0.002955216681584716\n","Batch 2100/3470, Loss: 0.0002455678186379373\n","Batch 2110/3470, Loss: 0.008382213301956654\n","Batch 2120/3470, Loss: 0.00015482486924156547\n","Batch 2130/3470, Loss: 0.021845560520887375\n","Batch 2140/3470, Loss: 0.052369169890880585\n","Batch 2150/3470, Loss: 0.014913580380380154\n","Batch 2160/3470, Loss: 0.02681657113134861\n","Batch 2170/3470, Loss: 0.03299305960536003\n","Batch 2180/3470, Loss: 0.05263352021574974\n","Batch 2190/3470, Loss: 0.008700359612703323\n","Batch 2200/3470, Loss: 0.15475699305534363\n","Batch 2210/3470, Loss: 0.014773879200220108\n","Batch 2220/3470, Loss: 0.0054483553394675255\n","Batch 2230/3470, Loss: 0.0006181411445140839\n","Batch 2240/3470, Loss: 0.07075119763612747\n","Batch 2250/3470, Loss: 0.014340044930577278\n","Batch 2260/3470, Loss: 0.10303237289190292\n","Batch 2270/3470, Loss: 0.002510275226086378\n","Batch 2280/3470, Loss: 0.000571069773286581\n","Batch 2290/3470, Loss: 0.0008494511712342501\n","Batch 2300/3470, Loss: 0.0005467302980832756\n","Batch 2310/3470, Loss: 0.03208950534462929\n","Batch 2320/3470, Loss: 0.2935325801372528\n","Batch 2330/3470, Loss: 0.0016630839090794325\n","Batch 2340/3470, Loss: 0.0027709442656487226\n","Batch 2350/3470, Loss: 0.023617032915353775\n","Batch 2360/3470, Loss: 0.0001685488095972687\n","Batch 2370/3470, Loss: 0.18761225044727325\n","Batch 2380/3470, Loss: 0.008228191174566746\n","Batch 2390/3470, Loss: 0.004239638801664114\n","Batch 2400/3470, Loss: 0.08804640173912048\n","Batch 2410/3470, Loss: 0.005825432017445564\n","Batch 2420/3470, Loss: 0.0016725310124456882\n","Batch 2430/3470, Loss: 0.002188766375184059\n","Batch 2440/3470, Loss: 0.06133020296692848\n","Batch 2450/3470, Loss: 0.0013694307999685407\n","Batch 2460/3470, Loss: 0.0037863431498408318\n","Batch 2470/3470, Loss: 0.03764323890209198\n","Batch 2480/3470, Loss: 0.014460552483797073\n","Batch 2490/3470, Loss: 0.12154822051525116\n","Batch 2500/3470, Loss: 0.23848265409469604\n","Batch 2510/3470, Loss: 0.10935661941766739\n","Batch 2520/3470, Loss: 0.040064919739961624\n","Batch 2530/3470, Loss: 0.005135789979249239\n","Batch 2540/3470, Loss: 0.021402467042207718\n","Batch 2550/3470, Loss: 0.007037581875920296\n","Batch 2560/3470, Loss: 0.0027322182431817055\n","Batch 2570/3470, Loss: 0.0012885452015325427\n","Batch 2580/3470, Loss: 0.0030219978652894497\n","Batch 2590/3470, Loss: 0.002063102787360549\n","Batch 2600/3470, Loss: 0.005016976967453957\n","Batch 2610/3470, Loss: 0.0005445159040391445\n","Batch 2620/3470, Loss: 0.027845855802297592\n","Batch 2630/3470, Loss: 0.0015072694513946772\n","Batch 2640/3470, Loss: 0.032466016709804535\n","Batch 2650/3470, Loss: 0.000953226292040199\n","Batch 2660/3470, Loss: 0.002816976048052311\n","Batch 2670/3470, Loss: 0.0004872321733273566\n","Batch 2680/3470, Loss: 0.001099009532481432\n","Batch 2690/3470, Loss: 0.0013548590941354632\n","Batch 2700/3470, Loss: 0.005709037650376558\n","Batch 2710/3470, Loss: 0.021418452262878418\n","Batch 2720/3470, Loss: 0.0030453456565737724\n","Batch 2730/3470, Loss: 0.10160528868436813\n","Batch 2740/3470, Loss: 0.007575891446322203\n","Batch 2750/3470, Loss: 0.0004628711612895131\n","Batch 2760/3470, Loss: 0.15807265043258667\n","Batch 2770/3470, Loss: 0.005866214632987976\n","Batch 2780/3470, Loss: 0.032560884952545166\n","Batch 2790/3470, Loss: 0.038156744092702866\n","Batch 2800/3470, Loss: 0.00032739335438236594\n","Batch 2810/3470, Loss: 0.0006818784167990088\n","Batch 2820/3470, Loss: 0.0008530813502147794\n","Batch 2830/3470, Loss: 0.00012143527419539168\n","Batch 2840/3470, Loss: 0.002356740878894925\n","Batch 2850/3470, Loss: 0.0005463244742713869\n","Batch 2860/3470, Loss: 0.004668754991143942\n","Batch 2870/3470, Loss: 0.005037133116275072\n","Batch 2880/3470, Loss: 0.0019797179847955704\n","Batch 2890/3470, Loss: 0.0005510982591658831\n","Batch 2900/3470, Loss: 0.45188942551612854\n","Batch 2910/3470, Loss: 0.0023118178360164165\n","Batch 2920/3470, Loss: 0.013881589286029339\n","Batch 2930/3470, Loss: 0.0004874634905718267\n","Batch 2940/3470, Loss: 0.006983148865401745\n","Batch 2950/3470, Loss: 0.0013377602444961667\n","Batch 2960/3470, Loss: 0.00196456559933722\n","Batch 2970/3470, Loss: 0.13171862065792084\n","Batch 2980/3470, Loss: 0.009191076271235943\n","Batch 2990/3470, Loss: 0.2597929537296295\n","Batch 3000/3470, Loss: 0.00023131936904974282\n","Batch 3010/3470, Loss: 0.0346858948469162\n","Batch 3020/3470, Loss: 0.14592783153057098\n","Batch 3030/3470, Loss: 0.00393753033131361\n","Batch 3040/3470, Loss: 0.003920138347893953\n","Batch 3050/3470, Loss: 0.004624371416866779\n","Batch 3060/3470, Loss: 0.008648970164358616\n","Batch 3070/3470, Loss: 0.0026102380361407995\n","Batch 3080/3470, Loss: 0.0006753514753654599\n","Batch 3090/3470, Loss: 0.03463098034262657\n","Batch 3100/3470, Loss: 0.0033638710156083107\n","Batch 3110/3470, Loss: 0.006126719061285257\n","Batch 3120/3470, Loss: 0.002234171610325575\n","Batch 3130/3470, Loss: 0.018053611740469933\n","Batch 3140/3470, Loss: 0.03883086144924164\n","Batch 3150/3470, Loss: 0.08669787645339966\n","Batch 3160/3470, Loss: 0.0012302931863814592\n","Batch 3170/3470, Loss: 0.0007315184338949621\n","Batch 3180/3470, Loss: 0.00036140994052402675\n","Batch 3190/3470, Loss: 0.0018325545825064182\n","Batch 3200/3470, Loss: 0.015497395768761635\n","Batch 3210/3470, Loss: 0.020590582862496376\n","Batch 3220/3470, Loss: 0.3662712574005127\n","Batch 3230/3470, Loss: 0.11580514907836914\n","Batch 3240/3470, Loss: 0.007169927470386028\n","Batch 3250/3470, Loss: 0.012312483042478561\n","Batch 3260/3470, Loss: 0.06321174651384354\n","Batch 3270/3470, Loss: 0.01282285712659359\n","Batch 3280/3470, Loss: 0.03580132871866226\n","Batch 3290/3470, Loss: 0.03189508244395256\n","Batch 3300/3470, Loss: 0.00619154330343008\n","Batch 3310/3470, Loss: 0.017716240137815475\n","Batch 3320/3470, Loss: 0.003216981654986739\n","Batch 3330/3470, Loss: 0.08073313534259796\n","Batch 3340/3470, Loss: 9.418572153663263e-05\n","Batch 3350/3470, Loss: 0.00045260199112817645\n","Batch 3360/3470, Loss: 0.01502018328756094\n","Batch 3370/3470, Loss: 0.0014606771292164922\n","Batch 3380/3470, Loss: 0.012665996327996254\n","Batch 3390/3470, Loss: 0.0003890327934641391\n","Batch 3400/3470, Loss: 0.003948054276406765\n","Batch 3410/3470, Loss: 0.000330206414218992\n","Batch 3420/3470, Loss: 0.0007602067780680954\n","Batch 3430/3470, Loss: 0.00011164492025272921\n","Batch 3440/3470, Loss: 8.180390432244167e-05\n","Batch 3450/3470, Loss: 0.00036099989665672183\n","Batch 3460/3470, Loss: 0.0005653271800838411\n","Training epoch completed in: 9m 37s\n","Train loss 0.029667223855637193 accuracy 0.9896067871102545\n","Batch 0/868, Test Loss: 0.0002573364181444049\n","Batch 10/868, Test Loss: 0.00045445800060406327\n","Batch 20/868, Test Loss: 0.200737863779068\n","Batch 30/868, Test Loss: 0.3528943359851837\n","Batch 40/868, Test Loss: 5.9662463172571734e-05\n","Batch 50/868, Test Loss: 8.273265848401934e-05\n","Batch 60/868, Test Loss: 0.24178563058376312\n","Batch 70/868, Test Loss: 7.030762935755774e-05\n","Batch 80/868, Test Loss: 0.00032572855707257986\n","Batch 90/868, Test Loss: 0.15459954738616943\n","Batch 100/868, Test Loss: 0.0009685527184046805\n","Batch 110/868, Test Loss: 0.059781573712825775\n","Batch 120/868, Test Loss: 0.10806963592767715\n","Batch 130/868, Test Loss: 0.00015111117681954056\n","Batch 140/868, Test Loss: 0.194120392203331\n","Batch 150/868, Test Loss: 0.386452317237854\n","Batch 160/868, Test Loss: 0.0013387801591306925\n","Batch 170/868, Test Loss: 0.4906342029571533\n","Batch 180/868, Test Loss: 0.028794271871447563\n","Batch 190/868, Test Loss: 0.0003926015633624047\n","Batch 200/868, Test Loss: 0.05198245495557785\n","Batch 210/868, Test Loss: 0.004531570244580507\n","Batch 220/868, Test Loss: 0.00025647319853305817\n","Batch 230/868, Test Loss: 0.1881689578294754\n","Batch 240/868, Test Loss: 0.00036098898272030056\n","Batch 250/868, Test Loss: 0.2437877655029297\n","Batch 260/868, Test Loss: 0.16085481643676758\n","Batch 270/868, Test Loss: 0.039437539875507355\n","Batch 280/868, Test Loss: 0.002709513995796442\n","Batch 290/868, Test Loss: 0.22869299352169037\n","Batch 300/868, Test Loss: 0.15049493312835693\n","Batch 310/868, Test Loss: 0.4362284541130066\n","Batch 320/868, Test Loss: 0.0007027299143373966\n","Batch 330/868, Test Loss: 0.4075016677379608\n","Batch 340/868, Test Loss: 0.000457199988886714\n","Batch 350/868, Test Loss: 0.00051503925351426\n","Batch 360/868, Test Loss: 0.05823183059692383\n","Batch 370/868, Test Loss: 0.005855284631252289\n","Batch 380/868, Test Loss: 0.02108963578939438\n","Batch 390/868, Test Loss: 0.0002473609638400376\n","Batch 400/868, Test Loss: 0.00023466315178666264\n","Batch 410/868, Test Loss: 0.0037535028532147408\n","Batch 420/868, Test Loss: 0.06585555523633957\n","Batch 430/868, Test Loss: 0.0008344033267349005\n","Batch 440/868, Test Loss: 0.05729342997074127\n","Batch 450/868, Test Loss: 0.0007949549471959472\n","Batch 460/868, Test Loss: 0.004306609742343426\n","Batch 470/868, Test Loss: 0.00024229841073974967\n","Batch 480/868, Test Loss: 0.05037814751267433\n","Batch 490/868, Test Loss: 0.08559160679578781\n","Batch 500/868, Test Loss: 0.0011324374936521053\n","Batch 510/868, Test Loss: 0.00036283605732023716\n","Batch 520/868, Test Loss: 0.0002353768068132922\n","Batch 530/868, Test Loss: 0.00021327204012777656\n","Batch 540/868, Test Loss: 0.0002990375505760312\n","Batch 550/868, Test Loss: 0.2007109820842743\n","Batch 560/868, Test Loss: 0.3820885121822357\n","Batch 570/868, Test Loss: 0.001278578070923686\n","Batch 580/868, Test Loss: 0.0001655693049542606\n","Batch 590/868, Test Loss: 0.08029593527317047\n","Batch 600/868, Test Loss: 0.0004112953902222216\n","Batch 610/868, Test Loss: 0.0015197570901364088\n","Batch 620/868, Test Loss: 0.1377716064453125\n","Batch 630/868, Test Loss: 0.5315438508987427\n","Batch 640/868, Test Loss: 0.39549899101257324\n","Batch 650/868, Test Loss: 0.0002438810479361564\n","Batch 660/868, Test Loss: 0.31781554222106934\n","Batch 670/868, Test Loss: 0.00027815060457214713\n","Batch 680/868, Test Loss: 0.028717556968331337\n","Batch 690/868, Test Loss: 0.04426164925098419\n","Batch 700/868, Test Loss: 0.21715587377548218\n","Batch 710/868, Test Loss: 0.0002658168086782098\n","Batch 720/868, Test Loss: 0.2635498046875\n","Batch 730/868, Test Loss: 0.005563936196267605\n","Batch 740/868, Test Loss: 0.000948358210735023\n","Batch 750/868, Test Loss: 0.7243471145629883\n","Batch 760/868, Test Loss: 0.0002475908841006458\n","Batch 770/868, Test Loss: 0.000358268734999001\n","Batch 780/868, Test Loss: 0.1676943451166153\n","Batch 790/868, Test Loss: 0.00024005741579458117\n","Batch 800/868, Test Loss: 0.000408252322813496\n","Batch 810/868, Test Loss: 0.00016311056970153004\n","Batch 820/868, Test Loss: 0.0006152120186015964\n","Batch 830/868, Test Loss: 0.3009583353996277\n","Batch 840/868, Test Loss: 0.6090917587280273\n","Batch 850/868, Test Loss: 0.713164210319519\n","Batch 860/868, Test Loss: 0.0036286793183535337\n","Test evaluation completed in: 0m 46s\n","Test loss 0.0746335278728836 accuracy 0.9806916426512968\n","Test Precision: 0.9812080730454694\n","Test Recall: 0.9806916426512968\n","Test F1 Score: 0.9808694346896119\n","Starting epoch 3/10\n","Batch 0/3470, Loss: 0.00047777945292182267\n","Batch 10/3470, Loss: 0.31548869609832764\n","Batch 20/3470, Loss: 0.00034501845948398113\n","Batch 30/3470, Loss: 0.046534568071365356\n","Batch 40/3470, Loss: 0.0013985591940581799\n","Batch 50/3470, Loss: 0.0005777746555395424\n","Batch 60/3470, Loss: 0.0011503244750201702\n","Batch 70/3470, Loss: 0.014940276741981506\n","Batch 80/3470, Loss: 0.0018340673996135592\n","Batch 90/3470, Loss: 0.0008322836947627366\n","Batch 100/3470, Loss: 0.007799506187438965\n","Batch 110/3470, Loss: 0.0003582671342883259\n","Batch 120/3470, Loss: 0.001009120955131948\n","Batch 130/3470, Loss: 0.008942515589296818\n","Batch 140/3470, Loss: 0.008698773570358753\n","Batch 150/3470, Loss: 0.00032729393569752574\n","Batch 160/3470, Loss: 0.00033069209894165397\n","Batch 170/3470, Loss: 0.0003459461440797895\n","Batch 180/3470, Loss: 0.27655479311943054\n","Batch 190/3470, Loss: 0.003408509772270918\n","Batch 200/3470, Loss: 0.2273513525724411\n","Batch 210/3470, Loss: 0.0018987099174410105\n","Batch 220/3470, Loss: 0.0009787214221432805\n","Batch 230/3470, Loss: 0.00039119829307310283\n","Batch 240/3470, Loss: 0.0003980014007538557\n","Batch 250/3470, Loss: 0.015605010092258453\n","Batch 260/3470, Loss: 0.030209550634026527\n","Batch 270/3470, Loss: 0.0008621320012025535\n","Batch 280/3470, Loss: 0.005231825169175863\n","Batch 290/3470, Loss: 0.0067577785812318325\n","Batch 300/3470, Loss: 0.0006697282660752535\n","Batch 310/3470, Loss: 0.006211433093994856\n","Batch 320/3470, Loss: 0.0008343832450918853\n","Batch 330/3470, Loss: 0.0006097603472881019\n","Batch 340/3470, Loss: 0.00012493820395320654\n","Batch 350/3470, Loss: 0.0005334396846592426\n","Batch 360/3470, Loss: 0.000430479267379269\n","Batch 370/3470, Loss: 0.007061019539833069\n","Batch 380/3470, Loss: 0.00036014968645758927\n","Batch 390/3470, Loss: 0.14863905310630798\n","Batch 400/3470, Loss: 0.0014079222455620766\n","Batch 410/3470, Loss: 0.0029168114997446537\n","Batch 420/3470, Loss: 0.0006425388855859637\n","Batch 430/3470, Loss: 0.00029188834014348686\n","Batch 440/3470, Loss: 0.00025711453054100275\n","Batch 450/3470, Loss: 0.02490207739174366\n","Batch 460/3470, Loss: 0.0010261748684570193\n","Batch 470/3470, Loss: 0.029147861525416374\n","Batch 480/3470, Loss: 0.0013154979096725583\n","Batch 490/3470, Loss: 0.0008046065340749919\n","Batch 500/3470, Loss: 0.0017348750261589885\n","Batch 510/3470, Loss: 0.0011585024185478687\n","Batch 520/3470, Loss: 0.011238534934818745\n","Batch 530/3470, Loss: 0.3557751476764679\n","Batch 540/3470, Loss: 0.027312003076076508\n","Batch 550/3470, Loss: 0.002569166710600257\n","Batch 560/3470, Loss: 0.007982475683093071\n","Batch 570/3470, Loss: 0.005821867845952511\n","Batch 580/3470, Loss: 0.0036943869199603796\n","Batch 590/3470, Loss: 0.0034674033522605896\n","Batch 600/3470, Loss: 0.0006082160398364067\n","Batch 610/3470, Loss: 0.0007858540047891438\n","Batch 620/3470, Loss: 0.00062888755928725\n","Batch 630/3470, Loss: 0.002474264707416296\n","Batch 640/3470, Loss: 0.00041741153108887374\n","Batch 650/3470, Loss: 0.022985467687249184\n","Batch 660/3470, Loss: 0.021089985966682434\n","Batch 670/3470, Loss: 0.0012562363408505917\n","Batch 680/3470, Loss: 0.0009165416704490781\n","Batch 690/3470, Loss: 0.0003675910993479192\n","Batch 700/3470, Loss: 0.00022889520914759487\n","Batch 710/3470, Loss: 0.0005006288993172348\n","Batch 720/3470, Loss: 0.0002098726254189387\n","Batch 730/3470, Loss: 0.00031695037614554167\n","Batch 740/3470, Loss: 0.001950196921825409\n","Batch 750/3470, Loss: 0.0004238606197759509\n","Batch 760/3470, Loss: 0.0013214264763519168\n","Batch 770/3470, Loss: 0.001676764921285212\n","Batch 780/3470, Loss: 0.0005552918300963938\n","Batch 790/3470, Loss: 0.001191797317005694\n","Batch 800/3470, Loss: 0.0011059331009164453\n","Batch 810/3470, Loss: 0.008749560452997684\n","Batch 820/3470, Loss: 0.010766919702291489\n","Batch 830/3470, Loss: 0.00025779736461117864\n","Batch 840/3470, Loss: 0.0005414168699644506\n","Batch 850/3470, Loss: 0.0006927904905751348\n","Batch 860/3470, Loss: 0.0025465642102062702\n","Batch 870/3470, Loss: 0.007926524616777897\n","Batch 880/3470, Loss: 0.00013845908688381314\n","Batch 890/3470, Loss: 0.002273675287142396\n","Batch 900/3470, Loss: 0.042066801339387894\n","Batch 910/3470, Loss: 0.29052209854125977\n","Batch 920/3470, Loss: 0.0009760163957253098\n","Batch 930/3470, Loss: 0.006558350753039122\n","Batch 940/3470, Loss: 0.0005934445653110743\n","Batch 950/3470, Loss: 0.006497070658951998\n","Batch 960/3470, Loss: 0.000958430056925863\n","Batch 970/3470, Loss: 0.006422431208193302\n","Batch 980/3470, Loss: 0.001017534057609737\n","Batch 990/3470, Loss: 0.0003215182514395565\n","Batch 1000/3470, Loss: 0.009628389030694962\n","Batch 1010/3470, Loss: 0.00185321643948555\n","Batch 1020/3470, Loss: 0.03139908239245415\n","Batch 1030/3470, Loss: 0.0005803930689580739\n","Batch 1040/3470, Loss: 0.002854847814887762\n","Batch 1050/3470, Loss: 0.00014475306670647115\n","Batch 1060/3470, Loss: 0.0007411196711473167\n","Batch 1070/3470, Loss: 0.28912612795829773\n","Batch 1080/3470, Loss: 0.0007798417937010527\n","Batch 1090/3470, Loss: 0.001183610293082893\n","Batch 1100/3470, Loss: 0.0007695064996369183\n","Batch 1110/3470, Loss: 0.004825257696211338\n","Batch 1120/3470, Loss: 0.0018531501991674304\n","Batch 1130/3470, Loss: 0.012698259204626083\n","Batch 1140/3470, Loss: 0.002339444123208523\n","Batch 1150/3470, Loss: 0.00035690958611667156\n","Batch 1160/3470, Loss: 0.00023913872428238392\n","Batch 1170/3470, Loss: 0.000704423408024013\n","Batch 1180/3470, Loss: 0.00022128221462480724\n","Batch 1190/3470, Loss: 0.03252275288105011\n","Batch 1200/3470, Loss: 0.0014038420049473643\n","Batch 1210/3470, Loss: 0.006760898977518082\n","Batch 1220/3470, Loss: 0.009609132073819637\n","Batch 1230/3470, Loss: 0.0015738607617095113\n","Batch 1240/3470, Loss: 0.00937445554882288\n","Batch 1250/3470, Loss: 0.00034964492078870535\n","Batch 1260/3470, Loss: 0.018296774476766586\n","Batch 1270/3470, Loss: 0.00027487287297844887\n","Batch 1280/3470, Loss: 0.0011395663022994995\n","Batch 1290/3470, Loss: 0.001103460555896163\n","Batch 1300/3470, Loss: 0.04061998799443245\n","Batch 1310/3470, Loss: 0.013426349498331547\n","Batch 1320/3470, Loss: 0.040692951530218124\n","Batch 1330/3470, Loss: 0.00042899680556729436\n","Batch 1340/3470, Loss: 0.0001732132222969085\n","Batch 1350/3470, Loss: 0.015693197026848793\n","Batch 1360/3470, Loss: 0.023441964760422707\n","Batch 1370/3470, Loss: 0.00015321795945055783\n","Batch 1380/3470, Loss: 0.0003111951518803835\n","Batch 1390/3470, Loss: 0.003407412674278021\n","Batch 1400/3470, Loss: 0.0036345478147268295\n","Batch 1410/3470, Loss: 0.0030612642876803875\n","Batch 1420/3470, Loss: 0.003071731422096491\n","Batch 1430/3470, Loss: 0.0024023030418902636\n","Batch 1440/3470, Loss: 0.0037696612998843193\n","Batch 1450/3470, Loss: 0.0017718342132866383\n","Batch 1460/3470, Loss: 0.0017865768168121576\n","Batch 1470/3470, Loss: 0.0006872998783364892\n","Batch 1480/3470, Loss: 0.0006461577722802758\n","Batch 1490/3470, Loss: 0.002826409414410591\n","Batch 1500/3470, Loss: 0.003694833954796195\n","Batch 1510/3470, Loss: 0.0003438781714066863\n","Batch 1520/3470, Loss: 0.0032747953664511442\n","Batch 1530/3470, Loss: 0.026791661977767944\n","Batch 1540/3470, Loss: 0.013943701982498169\n","Batch 1550/3470, Loss: 0.021859493106603622\n","Batch 1560/3470, Loss: 0.004676303826272488\n","Batch 1570/3470, Loss: 0.00038962342659942806\n","Batch 1580/3470, Loss: 0.0041141388937830925\n","Batch 1590/3470, Loss: 0.09704834967851639\n","Batch 1600/3470, Loss: 0.00018794498464558274\n","Batch 1610/3470, Loss: 0.0001825446670409292\n","Batch 1620/3470, Loss: 0.00021622225176542997\n","Batch 1630/3470, Loss: 0.0006685417029075325\n","Batch 1640/3470, Loss: 0.00031542632495984435\n","Batch 1650/3470, Loss: 0.00037956179585307837\n","Batch 1660/3470, Loss: 0.00032778980676084757\n","Batch 1670/3470, Loss: 0.0001453333825338632\n","Batch 1680/3470, Loss: 0.00021332236065063626\n","Batch 1690/3470, Loss: 0.011238502338528633\n","Batch 1700/3470, Loss: 0.0001394519640598446\n","Batch 1710/3470, Loss: 9.622680227039382e-05\n","Batch 1720/3470, Loss: 0.00010518029012018815\n","Batch 1730/3470, Loss: 0.00016826484352350235\n","Batch 1740/3470, Loss: 0.011490656062960625\n","Batch 1750/3470, Loss: 0.0001442892535123974\n","Batch 1760/3470, Loss: 0.0001610888575669378\n","Batch 1770/3470, Loss: 0.0006201382493600249\n","Batch 1780/3470, Loss: 0.00034894890268333256\n","Batch 1790/3470, Loss: 0.00046562255010940135\n","Batch 1800/3470, Loss: 0.16816158592700958\n","Batch 1810/3470, Loss: 0.0023646289482712746\n","Batch 1820/3470, Loss: 0.0009446132462471724\n","Batch 1830/3470, Loss: 0.0007555073243565857\n","Batch 1840/3470, Loss: 0.023971233516931534\n","Batch 1850/3470, Loss: 0.0371997207403183\n","Batch 1860/3470, Loss: 0.0011062647681683302\n","Batch 1870/3470, Loss: 0.0010927198454737663\n","Batch 1880/3470, Loss: 0.0009086622158065438\n","Batch 1890/3470, Loss: 0.05666577070951462\n","Batch 1900/3470, Loss: 0.000469665857963264\n","Batch 1910/3470, Loss: 0.012400705367326736\n","Batch 1920/3470, Loss: 0.16100874543190002\n","Batch 1930/3470, Loss: 0.001256115734577179\n","Batch 1940/3470, Loss: 0.0028436915017664433\n","Batch 1950/3470, Loss: 0.00460031209513545\n","Batch 1960/3470, Loss: 0.05478769913315773\n","Batch 1970/3470, Loss: 0.003072531893849373\n","Batch 1980/3470, Loss: 0.00027593367849476635\n","Batch 1990/3470, Loss: 0.0033739586360752583\n","Batch 2000/3470, Loss: 0.00013370526721701026\n","Batch 2010/3470, Loss: 0.0005046697915531695\n","Batch 2020/3470, Loss: 0.0012143563944846392\n","Batch 2030/3470, Loss: 0.0020577518735080957\n","Batch 2040/3470, Loss: 0.00018820498371496797\n","Batch 2050/3470, Loss: 0.08603349328041077\n","Batch 2060/3470, Loss: 0.005588964559137821\n","Batch 2070/3470, Loss: 0.0003741905966307968\n","Batch 2080/3470, Loss: 0.00027793951448984444\n","Batch 2090/3470, Loss: 0.0002111614594468847\n","Batch 2100/3470, Loss: 0.00012241231161169708\n","Batch 2110/3470, Loss: 0.000648011511657387\n","Batch 2120/3470, Loss: 0.00011504979192977771\n","Batch 2130/3470, Loss: 0.0016471251146867871\n","Batch 2140/3470, Loss: 0.0008568071643821895\n","Batch 2150/3470, Loss: 0.01111872959882021\n","Batch 2160/3470, Loss: 0.10477123409509659\n","Batch 2170/3470, Loss: 0.0002604478213470429\n","Batch 2180/3470, Loss: 0.05731651186943054\n","Batch 2190/3470, Loss: 0.004143738187849522\n","Batch 2200/3470, Loss: 0.021130001172423363\n","Batch 2210/3470, Loss: 0.027650492265820503\n","Batch 2220/3470, Loss: 0.0010616336949169636\n","Batch 2230/3470, Loss: 0.00015350869216490537\n","Batch 2240/3470, Loss: 0.00023489855811931193\n","Batch 2250/3470, Loss: 0.020602542906999588\n","Batch 2260/3470, Loss: 0.0026111488696187735\n","Batch 2270/3470, Loss: 9.548728849040344e-05\n","Batch 2280/3470, Loss: 0.00012298341607674956\n","Batch 2290/3470, Loss: 0.0001152498516603373\n","Batch 2300/3470, Loss: 0.00010889052646234632\n","Batch 2310/3470, Loss: 6.11369323451072e-05\n","Batch 2320/3470, Loss: 0.0029648460913449526\n","Batch 2330/3470, Loss: 5.822435559821315e-05\n","Batch 2340/3470, Loss: 0.00024694122839719057\n","Batch 2350/3470, Loss: 9.914684778777882e-05\n","Batch 2360/3470, Loss: 5.760616113548167e-05\n","Batch 2370/3470, Loss: 0.12488775700330734\n","Batch 2380/3470, Loss: 8.420798985753208e-05\n","Batch 2390/3470, Loss: 0.0006166021921671927\n","Batch 2400/3470, Loss: 0.009164794348180294\n","Batch 2410/3470, Loss: 0.19045424461364746\n","Batch 2420/3470, Loss: 0.00022533706214744598\n","Batch 2430/3470, Loss: 0.0005005008424632251\n","Batch 2440/3470, Loss: 0.019545627757906914\n","Batch 2450/3470, Loss: 0.0005090925842523575\n","Batch 2460/3470, Loss: 0.002878952771425247\n","Batch 2470/3470, Loss: 0.0034662007819861174\n","Batch 2480/3470, Loss: 0.006410888861864805\n","Batch 2490/3470, Loss: 0.12005577236413956\n","Batch 2500/3470, Loss: 0.008148573338985443\n","Batch 2510/3470, Loss: 0.0033180899918079376\n","Batch 2520/3470, Loss: 0.0006013266392983496\n","Batch 2530/3470, Loss: 0.0008959924452938139\n","Batch 2540/3470, Loss: 0.004958436358720064\n","Batch 2550/3470, Loss: 0.0012011833023279905\n","Batch 2560/3470, Loss: 0.0004104888648726046\n","Batch 2570/3470, Loss: 0.00041818851605057716\n","Batch 2580/3470, Loss: 0.0022403476759791374\n","Batch 2590/3470, Loss: 0.0013837831793352962\n","Batch 2600/3470, Loss: 0.0006867088377475739\n","Batch 2610/3470, Loss: 0.00022189828450791538\n","Batch 2620/3470, Loss: 0.04537493735551834\n","Batch 2630/3470, Loss: 0.006189447361975908\n","Batch 2640/3470, Loss: 0.0017427039565518498\n","Batch 2650/3470, Loss: 0.00037358730332925916\n","Batch 2660/3470, Loss: 0.003530222922563553\n","Batch 2670/3470, Loss: 0.00025310361525043845\n","Batch 2680/3470, Loss: 0.0019252848578616977\n","Batch 2690/3470, Loss: 0.011512387543916702\n","Batch 2700/3470, Loss: 0.007404245436191559\n","Batch 2710/3470, Loss: 0.14597196877002716\n","Batch 2720/3470, Loss: 0.004634201526641846\n","Batch 2730/3470, Loss: 0.0021129671949893236\n","Batch 2740/3470, Loss: 0.00059859937755391\n","Batch 2750/3470, Loss: 0.00027313787722960114\n","Batch 2760/3470, Loss: 0.0015330015448853374\n","Batch 2770/3470, Loss: 0.23353801667690277\n","Batch 2780/3470, Loss: 0.0073003764264285564\n","Batch 2790/3470, Loss: 0.07597998529672623\n","Batch 2800/3470, Loss: 0.00020384267554618418\n","Batch 2810/3470, Loss: 0.0004477747716009617\n","Batch 2820/3470, Loss: 0.0008291969425044954\n","Batch 2830/3470, Loss: 0.00011834596807602793\n","Batch 2840/3470, Loss: 0.00027296357438899577\n","Batch 2850/3470, Loss: 0.0001865139784058556\n","Batch 2860/3470, Loss: 0.00022241796250455081\n","Batch 2870/3470, Loss: 0.010147771798074245\n","Batch 2880/3470, Loss: 0.00031134180608205497\n","Batch 2890/3470, Loss: 0.0002998954150825739\n","Batch 2900/3470, Loss: 0.00013784243492409587\n","Batch 2910/3470, Loss: 0.0002453936613164842\n","Batch 2920/3470, Loss: 0.0053112865425646305\n","Batch 2930/3470, Loss: 0.00014075971557758749\n","Batch 2940/3470, Loss: 0.021078281104564667\n","Batch 2950/3470, Loss: 0.00015838783292565495\n","Batch 2960/3470, Loss: 0.0009382077259942889\n","Batch 2970/3470, Loss: 0.06672113388776779\n","Batch 2980/3470, Loss: 0.0011473546037450433\n","Batch 2990/3470, Loss: 0.2433469146490097\n","Batch 3000/3470, Loss: 6.80139783071354e-05\n","Batch 3010/3470, Loss: 0.03510141000151634\n","Batch 3020/3470, Loss: 0.006051964592188597\n","Batch 3030/3470, Loss: 0.00021694293536711484\n","Batch 3040/3470, Loss: 0.0003279365773778409\n","Batch 3050/3470, Loss: 0.00010385285713709891\n","Batch 3060/3470, Loss: 0.00054653495317325\n","Batch 3070/3470, Loss: 9.190629498334602e-05\n","Batch 3080/3470, Loss: 0.00022751106007490307\n","Batch 3090/3470, Loss: 0.016815848648548126\n","Batch 3100/3470, Loss: 0.0016602181131020188\n","Batch 3110/3470, Loss: 0.0002716974704526365\n","Batch 3120/3470, Loss: 0.003295459784567356\n","Batch 3130/3470, Loss: 0.09930393844842911\n","Batch 3140/3470, Loss: 0.00012927691568620503\n","Batch 3150/3470, Loss: 0.0018177927704527974\n","Batch 3160/3470, Loss: 0.006477512884885073\n","Batch 3170/3470, Loss: 0.00011252651893300936\n","Batch 3180/3470, Loss: 9.35118441702798e-05\n","Batch 3190/3470, Loss: 0.0002511241182219237\n","Batch 3200/3470, Loss: 0.0003723551635630429\n","Batch 3210/3470, Loss: 0.0033158129081130028\n","Batch 3220/3470, Loss: 0.14800015091896057\n","Batch 3230/3470, Loss: 0.004654085263609886\n","Batch 3240/3470, Loss: 0.0006479266448877752\n","Batch 3250/3470, Loss: 0.0021562997717410326\n","Batch 3260/3470, Loss: 0.37375855445861816\n","Batch 3270/3470, Loss: 0.015310777351260185\n","Batch 3280/3470, Loss: 0.0748957023024559\n","Batch 3290/3470, Loss: 0.009572900831699371\n","Batch 3300/3470, Loss: 0.0004884027293883264\n","Batch 3310/3470, Loss: 0.006779782008379698\n","Batch 3320/3470, Loss: 0.003615911118686199\n","Batch 3330/3470, Loss: 0.014969686977565289\n","Batch 3340/3470, Loss: 6.352862692438066e-05\n","Batch 3350/3470, Loss: 0.0001415308943251148\n","Batch 3360/3470, Loss: 0.008359691128134727\n","Batch 3370/3470, Loss: 0.00017534657672513276\n","Batch 3380/3470, Loss: 0.0013013968709856272\n","Batch 3390/3470, Loss: 0.000470413506263867\n","Batch 3400/3470, Loss: 0.0011079517425969243\n","Batch 3410/3470, Loss: 0.0001307194761466235\n","Batch 3420/3470, Loss: 0.021993884816765785\n","Batch 3430/3470, Loss: 0.0028975538443773985\n","Batch 3440/3470, Loss: 9.590639092493802e-05\n","Batch 3450/3470, Loss: 0.00029357607127167284\n","Batch 3460/3470, Loss: 0.0002782715018838644\n","Training epoch completed in: 9m 37s\n","Train loss 0.016221925949381585 accuracy 0.9948304123061404\n","Batch 0/868, Test Loss: 0.00011436744534876198\n","Batch 10/868, Test Loss: 0.09840153902769089\n","Batch 20/868, Test Loss: 0.0017152979271486402\n","Batch 30/868, Test Loss: 0.09863992780447006\n","Batch 40/868, Test Loss: 7.81386042945087e-05\n","Batch 50/868, Test Loss: 0.00011045972496503964\n","Batch 60/868, Test Loss: 0.11600619554519653\n","Batch 70/868, Test Loss: 0.003969866316765547\n","Batch 80/868, Test Loss: 0.05706198513507843\n","Batch 90/868, Test Loss: 0.035169024020433426\n","Batch 100/868, Test Loss: 0.00048048453754745424\n","Batch 110/868, Test Loss: 0.15871834754943848\n","Batch 120/868, Test Loss: 0.1367732286453247\n","Batch 130/868, Test Loss: 8.58190469443798e-05\n","Batch 140/868, Test Loss: 0.0119967982172966\n","Batch 150/868, Test Loss: 0.13723689317703247\n","Batch 160/868, Test Loss: 0.00043378351256251335\n","Batch 170/868, Test Loss: 0.5108774304389954\n","Batch 180/868, Test Loss: 0.003495505778118968\n","Batch 190/868, Test Loss: 7.406342774629593e-05\n","Batch 200/868, Test Loss: 0.0014949984615668654\n","Batch 210/868, Test Loss: 0.007555120158940554\n","Batch 220/868, Test Loss: 0.00010077690239995718\n","Batch 230/868, Test Loss: 0.17447695136070251\n","Batch 240/868, Test Loss: 0.00011364206147845834\n","Batch 250/868, Test Loss: 0.11262442916631699\n","Batch 260/868, Test Loss: 0.15963201224803925\n","Batch 270/868, Test Loss: 0.1272907555103302\n","Batch 280/868, Test Loss: 0.16061930358409882\n","Batch 290/868, Test Loss: 0.01181111205369234\n","Batch 300/868, Test Loss: 0.09631083160638809\n","Batch 310/868, Test Loss: 0.00476855831220746\n","Batch 320/868, Test Loss: 0.03785144165158272\n","Batch 330/868, Test Loss: 0.13302625715732574\n","Batch 340/868, Test Loss: 0.0007671322673559189\n","Batch 350/868, Test Loss: 0.0015880729770287871\n","Batch 360/868, Test Loss: 0.0934954285621643\n","Batch 370/868, Test Loss: 0.058289553970098495\n","Batch 380/868, Test Loss: 0.0023509778548032045\n","Batch 390/868, Test Loss: 0.00011776814790209755\n","Batch 400/868, Test Loss: 0.00013163083349354565\n","Batch 410/868, Test Loss: 0.02266993559896946\n","Batch 420/868, Test Loss: 0.1810770034790039\n","Batch 430/868, Test Loss: 0.0006798086687922478\n","Batch 440/868, Test Loss: 0.015684081241488457\n","Batch 450/868, Test Loss: 0.024217797443270683\n","Batch 460/868, Test Loss: 0.0011566547909751534\n","Batch 470/868, Test Loss: 9.358862007502466e-05\n","Batch 480/868, Test Loss: 0.0003176157479174435\n","Batch 490/868, Test Loss: 0.2238367795944214\n","Batch 500/868, Test Loss: 0.08229345828294754\n","Batch 510/868, Test Loss: 0.00011246516078244895\n","Batch 520/868, Test Loss: 9.198689804179594e-05\n","Batch 530/868, Test Loss: 0.0008507498423568904\n","Batch 540/868, Test Loss: 0.00020380577188916504\n","Batch 550/868, Test Loss: 0.06656600534915924\n","Batch 560/868, Test Loss: 0.22653576731681824\n","Batch 570/868, Test Loss: 0.0004283332091290504\n","Batch 580/868, Test Loss: 7.810133683960885e-05\n","Batch 590/868, Test Loss: 0.317577987909317\n","Batch 600/868, Test Loss: 0.0033531638327986\n","Batch 610/868, Test Loss: 0.00039836892392486334\n","Batch 620/868, Test Loss: 0.1653803437948227\n","Batch 630/868, Test Loss: 0.2660239040851593\n","Batch 640/868, Test Loss: 0.13078585267066956\n","Batch 650/868, Test Loss: 0.0004301690496504307\n","Batch 660/868, Test Loss: 0.04496617242693901\n","Batch 670/868, Test Loss: 0.0014551258645951748\n","Batch 680/868, Test Loss: 0.0024528871290385723\n","Batch 690/868, Test Loss: 0.014283575117588043\n","Batch 700/868, Test Loss: 0.12721683084964752\n","Batch 710/868, Test Loss: 0.00011725196236511692\n","Batch 720/868, Test Loss: 0.2247757613658905\n","Batch 730/868, Test Loss: 0.0023683924227952957\n","Batch 740/868, Test Loss: 0.0008235939894802868\n","Batch 750/868, Test Loss: 0.5535492300987244\n","Batch 760/868, Test Loss: 0.00010254250810248777\n","Batch 770/868, Test Loss: 0.0006260738591663539\n","Batch 780/868, Test Loss: 0.23370575904846191\n","Batch 790/868, Test Loss: 0.00032457400811836123\n","Batch 800/868, Test Loss: 0.00011488734162412584\n","Batch 810/868, Test Loss: 8.514054206898436e-05\n","Batch 820/868, Test Loss: 0.0009242374217137694\n","Batch 830/868, Test Loss: 0.11530083417892456\n","Batch 840/868, Test Loss: 0.5882912874221802\n","Batch 850/868, Test Loss: 0.8187954425811768\n","Batch 860/868, Test Loss: 0.00040797717520035803\n","Test evaluation completed in: 0m 46s\n","Test loss 0.0693149255918047 accuracy 0.9798270893371759\n","Test Precision: 0.9804188876186163\n","Test Recall: 0.9798270893371758\n","Test F1 Score: 0.9800285066560757\n","Starting epoch 4/10\n","Batch 0/3470, Loss: 0.00032678109710104764\n","Batch 10/3470, Loss: 0.006769211031496525\n","Batch 20/3470, Loss: 0.0004959216457791626\n","Batch 30/3470, Loss: 0.0015869993949308991\n","Batch 40/3470, Loss: 0.0002828044234775007\n","Batch 50/3470, Loss: 0.00020515764481388032\n","Batch 60/3470, Loss: 0.002291863551363349\n","Batch 70/3470, Loss: 0.0029781728517264128\n","Batch 80/3470, Loss: 0.0017506926087662578\n","Batch 90/3470, Loss: 0.0016211423790082335\n","Batch 100/3470, Loss: 0.025994878262281418\n","Batch 110/3470, Loss: 0.0005562326405197382\n","Batch 120/3470, Loss: 0.00042416961514391005\n","Batch 130/3470, Loss: 0.0011386319529265165\n","Batch 140/3470, Loss: 0.039376817643642426\n","Batch 150/3470, Loss: 0.00033729802817106247\n","Batch 160/3470, Loss: 0.000298419181490317\n","Batch 170/3470, Loss: 0.00029723651823587716\n","Batch 180/3470, Loss: 0.0015282438835129142\n","Batch 190/3470, Loss: 0.00020048006263095886\n","Batch 200/3470, Loss: 0.00021326843125279993\n","Batch 210/3470, Loss: 0.0005017915973439813\n","Batch 220/3470, Loss: 0.00017507612938061357\n","Batch 230/3470, Loss: 0.00015219577471725643\n","Batch 240/3470, Loss: 0.0001415648148395121\n","Batch 250/3470, Loss: 0.00015232736768666655\n","Batch 260/3470, Loss: 0.004309592768549919\n","Batch 270/3470, Loss: 0.00013583581312559545\n","Batch 280/3470, Loss: 0.00015680296928621829\n","Batch 290/3470, Loss: 0.0003652755985967815\n","Batch 300/3470, Loss: 0.00013520127686206251\n","Batch 310/3470, Loss: 0.00037852535024285316\n","Batch 320/3470, Loss: 0.00011691232066368684\n","Batch 330/3470, Loss: 9.133198909694329e-05\n","Batch 340/3470, Loss: 0.00015232541773002595\n","Batch 350/3470, Loss: 0.00010647634917404503\n","Batch 360/3470, Loss: 0.00010868832032429054\n","Batch 370/3470, Loss: 9.300069359596819e-05\n","Batch 380/3470, Loss: 8.779227209743112e-05\n","Batch 390/3470, Loss: 0.013560849241912365\n","Batch 400/3470, Loss: 0.0001502104423707351\n","Batch 410/3470, Loss: 0.0015836028615012765\n","Batch 420/3470, Loss: 0.013709324412047863\n","Batch 430/3470, Loss: 8.277873712358996e-05\n","Batch 440/3470, Loss: 7.723652379354462e-05\n","Batch 450/3470, Loss: 9.197733015753329e-05\n","Batch 460/3470, Loss: 8.61388398334384e-05\n","Batch 470/3470, Loss: 7.026274397503585e-05\n","Batch 480/3470, Loss: 6.907932402100414e-05\n","Batch 490/3470, Loss: 0.0005924473516643047\n","Batch 500/3470, Loss: 0.0004845437069889158\n","Batch 510/3470, Loss: 0.000816035782918334\n","Batch 520/3470, Loss: 0.0021264103706926107\n","Batch 530/3470, Loss: 0.32723918557167053\n","Batch 540/3470, Loss: 0.006130882538855076\n","Batch 550/3470, Loss: 0.0007735713734291494\n","Batch 560/3470, Loss: 0.00031864296761341393\n","Batch 570/3470, Loss: 0.0002479390532243997\n","Batch 580/3470, Loss: 0.007402569986879826\n","Batch 590/3470, Loss: 0.0007006462546996772\n","Batch 600/3470, Loss: 0.0001408350362908095\n","Batch 610/3470, Loss: 0.0005059262039139867\n","Batch 620/3470, Loss: 0.000819605658762157\n","Batch 630/3470, Loss: 0.0004335480334702879\n","Batch 640/3470, Loss: 0.004264999646693468\n","Batch 650/3470, Loss: 0.0028179059736430645\n","Batch 660/3470, Loss: 0.0005455085774883628\n","Batch 670/3470, Loss: 0.0005452476325444877\n","Batch 680/3470, Loss: 0.0002611050440464169\n","Batch 690/3470, Loss: 0.0003086160577367991\n","Batch 700/3470, Loss: 0.00017478171503171325\n","Batch 710/3470, Loss: 0.00039396382635459304\n","Batch 720/3470, Loss: 0.0009965378558263183\n","Batch 730/3470, Loss: 0.0027723577804863453\n","Batch 740/3470, Loss: 0.00036048676702193916\n","Batch 750/3470, Loss: 0.00025814748369157314\n","Batch 760/3470, Loss: 0.017958246171474457\n","Batch 770/3470, Loss: 0.0008263718336820602\n","Batch 780/3470, Loss: 0.00013525420217774808\n","Batch 790/3470, Loss: 9.68522290349938e-05\n","Batch 800/3470, Loss: 0.00012444161984603852\n","Batch 810/3470, Loss: 0.0001340635644737631\n","Batch 820/3470, Loss: 0.003941849805414677\n","Batch 830/3470, Loss: 0.00027524304459802806\n","Batch 840/3470, Loss: 0.000305949681205675\n","Batch 850/3470, Loss: 0.00020685225899796933\n","Batch 860/3470, Loss: 0.00811378937214613\n","Batch 870/3470, Loss: 0.00165607372764498\n","Batch 880/3470, Loss: 9.27550281630829e-05\n","Batch 890/3470, Loss: 0.0003350749029777944\n","Batch 900/3470, Loss: 0.0017668366199359298\n","Batch 910/3470, Loss: 0.035652585327625275\n","Batch 920/3470, Loss: 0.00018600604380480945\n","Batch 930/3470, Loss: 0.003738791448995471\n","Batch 940/3470, Loss: 0.0002462628181092441\n","Batch 950/3470, Loss: 0.00037691841134801507\n","Batch 960/3470, Loss: 0.00032105762511491776\n","Batch 970/3470, Loss: 0.0006286757998168468\n","Batch 980/3470, Loss: 0.000174826112925075\n","Batch 990/3470, Loss: 0.00015432752843480557\n","Batch 1000/3470, Loss: 0.019904304295778275\n","Batch 1010/3470, Loss: 0.00031595147447660565\n","Batch 1020/3470, Loss: 0.013248519040644169\n","Batch 1030/3470, Loss: 0.0006011415971443057\n","Batch 1040/3470, Loss: 0.0007182253175415099\n","Batch 1050/3470, Loss: 8.394756150664762e-05\n","Batch 1060/3470, Loss: 0.0005032026092521846\n","Batch 1070/3470, Loss: 0.18285192549228668\n","Batch 1080/3470, Loss: 0.0007291060173884034\n","Batch 1090/3470, Loss: 0.004094263073056936\n","Batch 1100/3470, Loss: 0.0006698633660562336\n","Batch 1110/3470, Loss: 0.0011268537491559982\n","Batch 1120/3470, Loss: 0.00040719209937378764\n","Batch 1130/3470, Loss: 0.005426556337624788\n","Batch 1140/3470, Loss: 0.004132477566599846\n","Batch 1150/3470, Loss: 0.00021031094365753233\n","Batch 1160/3470, Loss: 8.677958976477385e-05\n","Batch 1170/3470, Loss: 0.0007076837355270982\n","Batch 1180/3470, Loss: 0.00011514517245814204\n","Batch 1190/3470, Loss: 0.00013137068890500814\n","Batch 1200/3470, Loss: 7.434477447532117e-05\n","Batch 1210/3470, Loss: 7.303468009922653e-05\n","Batch 1220/3470, Loss: 0.0001438973704352975\n","Batch 1230/3470, Loss: 0.00011609122884692624\n","Batch 1240/3470, Loss: 0.007531727664172649\n","Batch 1250/3470, Loss: 0.0001035911773215048\n","Batch 1260/3470, Loss: 0.01946253515779972\n","Batch 1270/3470, Loss: 8.212298416765407e-05\n","Batch 1280/3470, Loss: 0.00010387115617049858\n","Batch 1290/3470, Loss: 0.001979012507945299\n","Batch 1300/3470, Loss: 0.00011071503104176372\n","Batch 1310/3470, Loss: 0.017564503476023674\n","Batch 1320/3470, Loss: 0.00013188662705942988\n","Batch 1330/3470, Loss: 0.0005872486508451402\n","Batch 1340/3470, Loss: 5.662987314281054e-05\n","Batch 1350/3470, Loss: 0.00023745295766275376\n","Batch 1360/3470, Loss: 0.0015399422263726592\n","Batch 1370/3470, Loss: 0.00012921294546686113\n","Batch 1380/3470, Loss: 6.028780626365915e-05\n","Batch 1390/3470, Loss: 9.728923032525927e-05\n","Batch 1400/3470, Loss: 8.182306919479743e-05\n","Batch 1410/3470, Loss: 5.696462176274508e-05\n","Batch 1420/3470, Loss: 0.0005083162104710937\n","Batch 1430/3470, Loss: 0.000708160107024014\n","Batch 1440/3470, Loss: 0.0021310679148882627\n","Batch 1450/3470, Loss: 0.0010246874298900366\n","Batch 1460/3470, Loss: 0.0006705753039568663\n","Batch 1470/3470, Loss: 0.0005647049401886761\n","Batch 1480/3470, Loss: 0.0004228665493428707\n","Batch 1490/3470, Loss: 0.0006692783790640533\n","Batch 1500/3470, Loss: 0.001321121002547443\n","Batch 1510/3470, Loss: 0.0005444612470455468\n","Batch 1520/3470, Loss: 0.00047918420750647783\n","Batch 1530/3470, Loss: 0.0005504040163941681\n","Batch 1540/3470, Loss: 0.22463206946849823\n","Batch 1550/3470, Loss: 0.006909681484103203\n","Batch 1560/3470, Loss: 0.005904225166887045\n","Batch 1570/3470, Loss: 0.0012767029693350196\n","Batch 1580/3470, Loss: 0.0004455663438420743\n","Batch 1590/3470, Loss: 0.000642807746771723\n","Batch 1600/3470, Loss: 0.00031385169131681323\n","Batch 1610/3470, Loss: 0.00020373679581098258\n","Batch 1620/3470, Loss: 0.003395112231373787\n","Batch 1630/3470, Loss: 0.00028472376288846135\n","Batch 1640/3470, Loss: 0.00022457452723756433\n","Batch 1650/3470, Loss: 0.00018439172708895057\n","Batch 1660/3470, Loss: 0.03162060305476189\n","Batch 1670/3470, Loss: 0.00018156084115616977\n","Batch 1680/3470, Loss: 0.00021659856429323554\n","Batch 1690/3470, Loss: 0.0008794862078502774\n","Batch 1700/3470, Loss: 0.00024345540441572666\n","Batch 1710/3470, Loss: 0.0002943435974884778\n","Batch 1720/3470, Loss: 0.0001717202103463933\n","Batch 1730/3470, Loss: 0.007080177776515484\n","Batch 1740/3470, Loss: 0.0025109185371547937\n","Batch 1750/3470, Loss: 0.000135017151478678\n","Batch 1760/3470, Loss: 0.00014401541557163\n","Batch 1770/3470, Loss: 0.00015644791710656136\n","Batch 1780/3470, Loss: 0.00011882202670676634\n","Batch 1790/3470, Loss: 0.0002127595362253487\n","Batch 1800/3470, Loss: 0.0038042422384023666\n","Batch 1810/3470, Loss: 0.00024097919231280684\n","Batch 1820/3470, Loss: 0.0007342383032664657\n","Batch 1830/3470, Loss: 0.00020897641661576927\n","Batch 1840/3470, Loss: 0.0021932930685579777\n","Batch 1850/3470, Loss: 0.0010733847739174962\n","Batch 1860/3470, Loss: 0.0002716037561185658\n","Batch 1870/3470, Loss: 0.0005170523072592914\n","Batch 1880/3470, Loss: 0.06998291611671448\n","Batch 1890/3470, Loss: 0.018633486703038216\n","Batch 1900/3470, Loss: 0.0006367550231516361\n","Batch 1910/3470, Loss: 0.5271115899085999\n","Batch 1920/3470, Loss: 0.002088233595713973\n","Batch 1930/3470, Loss: 0.0006979890749789774\n","Batch 1940/3470, Loss: 0.005271764472126961\n","Batch 1950/3470, Loss: 0.0009175191516987979\n","Batch 1960/3470, Loss: 0.006192970555275679\n","Batch 1970/3470, Loss: 0.003153640776872635\n","Batch 1980/3470, Loss: 0.0004502800584305078\n","Batch 1990/3470, Loss: 0.10574012994766235\n","Batch 2000/3470, Loss: 0.00028619167278520763\n","Batch 2010/3470, Loss: 0.0002863238623831421\n","Batch 2020/3470, Loss: 0.0075076064094901085\n","Batch 2030/3470, Loss: 0.0015371619956567883\n","Batch 2040/3470, Loss: 0.0005290079861879349\n","Batch 2050/3470, Loss: 0.00040878236177377403\n","Batch 2060/3470, Loss: 0.00046529978862963617\n","Batch 2070/3470, Loss: 0.00018767461006063968\n","Batch 2080/3470, Loss: 0.00022030170657671988\n","Batch 2090/3470, Loss: 0.0005076557863503695\n","Batch 2100/3470, Loss: 0.0010161210084334016\n","Batch 2110/3470, Loss: 0.0003114131395705044\n","Batch 2120/3470, Loss: 0.00015171855920925736\n","Batch 2130/3470, Loss: 0.0002846748393494636\n","Batch 2140/3470, Loss: 0.00026209073257632554\n","Batch 2150/3470, Loss: 0.0002873420889955014\n","Batch 2160/3470, Loss: 0.004175047390162945\n","Batch 2170/3470, Loss: 0.0042779650539159775\n","Batch 2180/3470, Loss: 0.00033881253330037\n","Batch 2190/3470, Loss: 0.0005983638111501932\n","Batch 2200/3470, Loss: 0.0015281620435416698\n","Batch 2210/3470, Loss: 0.0005030716420151293\n","Batch 2220/3470, Loss: 0.00019378832075744867\n","Batch 2230/3470, Loss: 0.00016513925220351666\n","Batch 2240/3470, Loss: 0.0013011565897613764\n","Batch 2250/3470, Loss: 0.00042888399912044406\n","Batch 2260/3470, Loss: 0.014778834767639637\n","Batch 2270/3470, Loss: 0.0001523593527963385\n","Batch 2280/3470, Loss: 0.0017149674240499735\n","Batch 2290/3470, Loss: 0.00019066003733314574\n","Batch 2300/3470, Loss: 0.0016502494690939784\n","Batch 2310/3470, Loss: 0.00027617145678959787\n","Batch 2320/3470, Loss: 0.05201185494661331\n","Batch 2330/3470, Loss: 0.00012249278370290995\n","Batch 2340/3470, Loss: 0.16723069548606873\n","Batch 2350/3470, Loss: 0.0001571564207552001\n","Batch 2360/3470, Loss: 0.00010290845239069313\n","Batch 2370/3470, Loss: 0.04380885511636734\n","Batch 2380/3470, Loss: 0.0001322573225479573\n","Batch 2390/3470, Loss: 0.004099182318896055\n","Batch 2400/3470, Loss: 0.0014008224243298173\n","Batch 2410/3470, Loss: 0.00013819315063301474\n","Batch 2420/3470, Loss: 8.231023093685508e-05\n","Batch 2430/3470, Loss: 7.202911365311593e-05\n","Batch 2440/3470, Loss: 0.004088100511580706\n","Batch 2450/3470, Loss: 6.493702676380053e-05\n","Batch 2460/3470, Loss: 0.014407739043235779\n","Batch 2470/3470, Loss: 7.425599324051291e-05\n","Batch 2480/3470, Loss: 0.00014075047511141747\n","Batch 2490/3470, Loss: 0.003040105104446411\n","Batch 2500/3470, Loss: 0.02430853061378002\n","Batch 2510/3470, Loss: 0.13402359187602997\n","Batch 2520/3470, Loss: 0.0006142780184745789\n","Batch 2530/3470, Loss: 0.0005985118332318962\n","Batch 2540/3470, Loss: 0.0008355285390280187\n","Batch 2550/3470, Loss: 0.0010955029865726829\n","Batch 2560/3470, Loss: 0.000774799264036119\n","Batch 2570/3470, Loss: 0.0003223625826649368\n","Batch 2580/3470, Loss: 0.0003813997027464211\n","Batch 2590/3470, Loss: 0.0004185295256320387\n","Batch 2600/3470, Loss: 0.0004388710658531636\n","Batch 2610/3470, Loss: 0.00011045343853766099\n","Batch 2620/3470, Loss: 0.00631997175514698\n","Batch 2630/3470, Loss: 0.00018827592430170625\n","Batch 2640/3470, Loss: 0.0004857380408793688\n","Batch 2650/3470, Loss: 0.0002012122276937589\n","Batch 2660/3470, Loss: 0.00018080884183291346\n","Batch 2670/3470, Loss: 0.0003110109828412533\n","Batch 2680/3470, Loss: 0.00013811441021971405\n","Batch 2690/3470, Loss: 0.00014466591528616846\n","Batch 2700/3470, Loss: 0.0006709762965328991\n","Batch 2710/3470, Loss: 0.0003175381862092763\n","Batch 2720/3470, Loss: 0.00027080520521849394\n","Batch 2730/3470, Loss: 0.00027565236086957157\n","Batch 2740/3470, Loss: 0.0003611459396779537\n","Batch 2750/3470, Loss: 9.18638615985401e-05\n","Batch 2760/3470, Loss: 0.00021509223734028637\n","Batch 2770/3470, Loss: 0.00015690566215198487\n","Batch 2780/3470, Loss: 0.0009795315563678741\n","Batch 2790/3470, Loss: 0.003919302951544523\n","Batch 2800/3470, Loss: 8.337135659530759e-05\n","Batch 2810/3470, Loss: 8.461715333396569e-05\n","Batch 2820/3470, Loss: 0.00019874529971275479\n","Batch 2830/3470, Loss: 0.00043188323616050184\n","Batch 2840/3470, Loss: 0.00010638214007485658\n","Batch 2850/3470, Loss: 9.100036550080404e-05\n","Batch 2860/3470, Loss: 8.191797678591684e-05\n","Batch 2870/3470, Loss: 8.899701060727239e-05\n","Batch 2880/3470, Loss: 0.00010604679846437648\n","Batch 2890/3470, Loss: 6.076268618926406e-05\n","Batch 2900/3470, Loss: 5.673305713571608e-05\n","Batch 2910/3470, Loss: 0.0001096378400688991\n","Batch 2920/3470, Loss: 0.0002180546143790707\n","Batch 2930/3470, Loss: 4.187090235063806e-05\n","Batch 2940/3470, Loss: 0.001041596056893468\n","Batch 2950/3470, Loss: 7.830692629795521e-05\n","Batch 2960/3470, Loss: 0.0018539215670898557\n","Batch 2970/3470, Loss: 0.5640864372253418\n","Batch 2980/3470, Loss: 0.015761716291308403\n","Batch 2990/3470, Loss: 0.005102152936160564\n","Batch 3000/3470, Loss: 4.984286715625785e-05\n","Batch 3010/3470, Loss: 0.018492132425308228\n","Batch 3020/3470, Loss: 0.00011861095845233649\n","Batch 3030/3470, Loss: 0.0001867171813501045\n","Batch 3040/3470, Loss: 0.00016085256356745958\n","Batch 3050/3470, Loss: 8.313069702126086e-05\n","Batch 3060/3470, Loss: 0.0006759325042366982\n","Batch 3070/3470, Loss: 0.006014502607285976\n","Batch 3080/3470, Loss: 0.00023850079742260277\n","Batch 3090/3470, Loss: 0.010311533696949482\n","Batch 3100/3470, Loss: 0.0019384759943932295\n","Batch 3110/3470, Loss: 9.920250158756971e-05\n","Batch 3120/3470, Loss: 0.0012795733055099845\n","Batch 3130/3470, Loss: 0.015594025142490864\n","Batch 3140/3470, Loss: 8.042771514737979e-05\n","Batch 3150/3470, Loss: 0.11153241246938705\n","Batch 3160/3470, Loss: 0.0029515482019633055\n","Batch 3170/3470, Loss: 0.00010018448665505275\n","Batch 3180/3470, Loss: 9.267514542443678e-05\n","Batch 3190/3470, Loss: 9.050166408997029e-05\n","Batch 3200/3470, Loss: 0.005734958685934544\n","Batch 3210/3470, Loss: 8.784666715655476e-05\n","Batch 3220/3470, Loss: 0.0006427859771065414\n","Batch 3230/3470, Loss: 0.00016203151608351618\n","Batch 3240/3470, Loss: 0.014274193905293941\n","Batch 3250/3470, Loss: 0.0014404854737222195\n","Batch 3260/3470, Loss: 0.004345091991126537\n","Batch 3270/3470, Loss: 0.007338627707213163\n","Batch 3280/3470, Loss: 0.0008212481625378132\n","Batch 3290/3470, Loss: 0.0001432050485163927\n","Batch 3300/3470, Loss: 8.761195931583643e-05\n","Batch 3310/3470, Loss: 0.0001851343986345455\n","Batch 3320/3470, Loss: 0.0007814950658939779\n","Batch 3330/3470, Loss: 0.001515463925898075\n","Batch 3340/3470, Loss: 3.6536817788146436e-05\n","Batch 3350/3470, Loss: 6.655207107542083e-05\n","Batch 3360/3470, Loss: 0.00032775933505035937\n","Batch 3370/3470, Loss: 0.07418373972177505\n","Batch 3380/3470, Loss: 0.0011649844236671925\n","Batch 3390/3470, Loss: 4.4552205508807674e-05\n","Batch 3400/3470, Loss: 0.0028689363971352577\n","Batch 3410/3470, Loss: 3.716225182870403e-05\n","Batch 3420/3470, Loss: 0.00031926631345413625\n","Batch 3430/3470, Loss: 3.257333082729019e-05\n","Batch 3440/3470, Loss: 3.951671169488691e-05\n","Batch 3450/3470, Loss: 8.768570114625618e-05\n","Batch 3460/3470, Loss: 7.468821422662586e-05\n","Training epoch completed in: 9m 37s\n","Train loss 0.008475980395376751 accuracy 0.9973701749013816\n","Batch 0/868, Test Loss: 6.121899059507996e-05\n","Batch 10/868, Test Loss: 0.0001659331755945459\n","Batch 20/868, Test Loss: 0.009968053549528122\n","Batch 30/868, Test Loss: 0.2389361560344696\n","Batch 40/868, Test Loss: 4.2646206566132605e-05\n","Batch 50/868, Test Loss: 0.00032863166416063905\n","Batch 60/868, Test Loss: 0.3478694260120392\n","Batch 70/868, Test Loss: 0.040598828345537186\n","Batch 80/868, Test Loss: 0.043720543384552\n","Batch 90/868, Test Loss: 0.05995205044746399\n","Batch 100/868, Test Loss: 0.0022086624521762133\n","Batch 110/868, Test Loss: 0.18467825651168823\n","Batch 120/868, Test Loss: 0.013078172691166401\n","Batch 130/868, Test Loss: 6.14938689977862e-05\n","Batch 140/868, Test Loss: 0.0025004625786095858\n","Batch 150/868, Test Loss: 0.5348819494247437\n","Batch 160/868, Test Loss: 7.503717642975971e-05\n","Batch 170/868, Test Loss: 0.05764557048678398\n","Batch 180/868, Test Loss: 0.0009393353830091655\n","Batch 190/868, Test Loss: 0.00020898153888992965\n","Batch 200/868, Test Loss: 0.145748108625412\n","Batch 210/868, Test Loss: 0.03344861790537834\n","Batch 220/868, Test Loss: 9.935896378010511e-05\n","Batch 230/868, Test Loss: 0.047106437385082245\n","Batch 240/868, Test Loss: 5.929693361395039e-05\n","Batch 250/868, Test Loss: 0.0022528376430273056\n","Batch 260/868, Test Loss: 0.4372861087322235\n","Batch 270/868, Test Loss: 0.16459979116916656\n","Batch 280/868, Test Loss: 0.1634472906589508\n","Batch 290/868, Test Loss: 0.5539217591285706\n","Batch 300/868, Test Loss: 0.38679176568984985\n","Batch 310/868, Test Loss: 5.2547336963471025e-05\n","Batch 320/868, Test Loss: 0.0010856192093342543\n","Batch 330/868, Test Loss: 0.5717366933822632\n","Batch 340/868, Test Loss: 0.0017100293189287186\n","Batch 350/868, Test Loss: 9.625692473491654e-05\n","Batch 360/868, Test Loss: 0.004277943167835474\n","Batch 370/868, Test Loss: 0.147505983710289\n","Batch 380/868, Test Loss: 0.024545617401599884\n","Batch 390/868, Test Loss: 0.0001213671566802077\n","Batch 400/868, Test Loss: 0.00017780587950255722\n","Batch 410/868, Test Loss: 0.03364011272788048\n","Batch 420/868, Test Loss: 0.5635607242584229\n","Batch 430/868, Test Loss: 0.0004896464524790645\n","Batch 440/868, Test Loss: 0.07173522561788559\n","Batch 450/868, Test Loss: 5.046136720920913e-05\n","Batch 460/868, Test Loss: 0.0006939410814084113\n","Batch 470/868, Test Loss: 0.0011706275399774313\n","Batch 480/868, Test Loss: 0.010049638338387012\n","Batch 490/868, Test Loss: 0.07350430637598038\n","Batch 500/868, Test Loss: 0.06576240807771683\n","Batch 510/868, Test Loss: 0.0007230594637803733\n","Batch 520/868, Test Loss: 6.677558849332854e-05\n","Batch 530/868, Test Loss: 0.0005437217187136412\n","Batch 540/868, Test Loss: 5.15042083861772e-05\n","Batch 550/868, Test Loss: 0.32218730449676514\n","Batch 560/868, Test Loss: 0.2503812313079834\n","Batch 570/868, Test Loss: 0.0033975234255194664\n","Batch 580/868, Test Loss: 0.0009886035695672035\n","Batch 590/868, Test Loss: 0.22522930800914764\n","Batch 600/868, Test Loss: 0.0015973530244082212\n","Batch 610/868, Test Loss: 0.0007012833375483751\n","Batch 620/868, Test Loss: 0.14185795187950134\n","Batch 630/868, Test Loss: 0.5611780285835266\n","Batch 640/868, Test Loss: 0.09060519933700562\n","Batch 650/868, Test Loss: 0.1214611753821373\n","Batch 660/868, Test Loss: 0.11993445456027985\n","Batch 670/868, Test Loss: 0.06266344338655472\n","Batch 680/868, Test Loss: 0.011799338273704052\n","Batch 690/868, Test Loss: 0.060240238904953\n","Batch 700/868, Test Loss: 0.20758551359176636\n","Batch 710/868, Test Loss: 6.159135227790102e-05\n","Batch 720/868, Test Loss: 0.012056434527039528\n","Batch 730/868, Test Loss: 0.04049816355109215\n","Batch 740/868, Test Loss: 5.967690231045708e-05\n","Batch 750/868, Test Loss: 0.26950201392173767\n","Batch 760/868, Test Loss: 0.00013283593580126762\n","Batch 770/868, Test Loss: 0.002139861462637782\n","Batch 780/868, Test Loss: 0.0999905988574028\n","Batch 790/868, Test Loss: 0.00011298768367851153\n","Batch 800/868, Test Loss: 0.0003799965779762715\n","Batch 810/868, Test Loss: 5.854440314578824e-05\n","Batch 820/868, Test Loss: 8.834930486045778e-05\n","Batch 830/868, Test Loss: 0.19045571982860565\n","Batch 840/868, Test Loss: 0.632577121257782\n","Batch 850/868, Test Loss: 1.1317178010940552\n","Batch 860/868, Test Loss: 0.00016772685921750963\n","Test evaluation completed in: 0m 46s\n","Test loss 0.0974428801157197 accuracy 0.9786743515850145\n","Test Precision: 0.9795202133284356\n","Test Recall: 0.9786743515850144\n","Test F1 Score: 0.9789446062168184\n","Starting epoch 5/10\n","Batch 0/3470, Loss: 0.00014721471234224737\n","Batch 10/3470, Loss: 0.12336848676204681\n","Batch 20/3470, Loss: 0.0007407045923173428\n","Batch 30/3470, Loss: 0.0076901717111468315\n","Batch 40/3470, Loss: 0.0003304134588688612\n","Batch 50/3470, Loss: 0.00022726684983354062\n","Batch 60/3470, Loss: 0.00041468560812063515\n","Batch 70/3470, Loss: 0.0034970734268426895\n","Batch 80/3470, Loss: 0.0017287515802308917\n","Batch 90/3470, Loss: 0.0018011934589594603\n","Batch 100/3470, Loss: 0.009121358394622803\n","Batch 110/3470, Loss: 0.0004194488865323365\n","Batch 120/3470, Loss: 0.00019241104018874466\n","Batch 130/3470, Loss: 0.00025315798120573163\n","Batch 140/3470, Loss: 0.0011124557349830866\n","Batch 150/3470, Loss: 0.00011476891086203977\n","Batch 160/3470, Loss: 0.00010951484000543132\n","Batch 170/3470, Loss: 0.00024095093249343336\n","Batch 180/3470, Loss: 0.11150038242340088\n","Batch 190/3470, Loss: 0.0006922071916051209\n","Batch 200/3470, Loss: 0.00023416061594616622\n","Batch 210/3470, Loss: 0.0015818967949599028\n","Batch 220/3470, Loss: 0.0008237715810537338\n","Batch 230/3470, Loss: 0.00037991709541529417\n","Batch 240/3470, Loss: 9.387674799654633e-05\n","Batch 250/3470, Loss: 0.0006565441144630313\n","Batch 260/3470, Loss: 0.00040763395372778177\n","Batch 270/3470, Loss: 0.0005325796082615852\n","Batch 280/3470, Loss: 0.00010462832869961858\n","Batch 290/3470, Loss: 0.034059323370456696\n","Batch 300/3470, Loss: 0.0001273898669751361\n","Batch 310/3470, Loss: 0.00045389545266516507\n","Batch 320/3470, Loss: 6.8728368205484e-05\n","Batch 330/3470, Loss: 5.8156612794846296e-05\n","Batch 340/3470, Loss: 6.225486868061125e-05\n","Batch 350/3470, Loss: 0.0001075764448614791\n","Batch 360/3470, Loss: 0.0001234259980265051\n","Batch 370/3470, Loss: 8.401411469094455e-05\n","Batch 380/3470, Loss: 7.29068779037334e-05\n","Batch 390/3470, Loss: 7.427517994074151e-05\n","Batch 400/3470, Loss: 0.0009042854653671384\n","Batch 410/3470, Loss: 0.0001363929477520287\n","Batch 420/3470, Loss: 0.0015397162642329931\n","Batch 430/3470, Loss: 6.723101978423074e-05\n","Batch 440/3470, Loss: 8.723146311240271e-05\n","Batch 450/3470, Loss: 7.682489376747981e-05\n","Batch 460/3470, Loss: 0.0005280721816234291\n","Batch 470/3470, Loss: 0.000688666885253042\n","Batch 480/3470, Loss: 0.000920635589864105\n","Batch 490/3470, Loss: 0.00036278946208767593\n","Batch 500/3470, Loss: 0.006045299582183361\n","Batch 510/3470, Loss: 0.00013594873598776758\n","Batch 520/3470, Loss: 0.0006338401581160724\n","Batch 530/3470, Loss: 0.1328866332769394\n","Batch 540/3470, Loss: 0.0017789526609703898\n","Batch 550/3470, Loss: 0.0001635981461731717\n","Batch 560/3470, Loss: 0.00026813894510269165\n","Batch 570/3470, Loss: 7.45474171708338e-05\n","Batch 580/3470, Loss: 0.00010609135642880574\n","Batch 590/3470, Loss: 0.000262536748778075\n","Batch 600/3470, Loss: 7.720652502030134e-05\n","Batch 610/3470, Loss: 8.340253407368436e-05\n","Batch 620/3470, Loss: 6.264197145355865e-05\n","Batch 630/3470, Loss: 5.5162556236609817e-05\n","Batch 640/3470, Loss: 5.806782064610161e-05\n","Batch 650/3470, Loss: 5.291252819006331e-05\n","Batch 660/3470, Loss: 0.00012202807556604967\n","Batch 670/3470, Loss: 6.693276372971013e-05\n","Batch 680/3470, Loss: 0.003722995752468705\n","Batch 690/3470, Loss: 0.00010248061153106391\n","Batch 700/3470, Loss: 6.117457814980298e-05\n","Batch 710/3470, Loss: 6.091335671953857e-05\n","Batch 720/3470, Loss: 7.089631981216371e-05\n","Batch 730/3470, Loss: 0.0012766432482749224\n","Batch 740/3470, Loss: 6.618811312364414e-05\n","Batch 750/3470, Loss: 8.889901801012456e-05\n","Batch 760/3470, Loss: 8.49669158924371e-05\n","Batch 770/3470, Loss: 0.0011623491300269961\n","Batch 780/3470, Loss: 0.0019281868590041995\n","Batch 790/3470, Loss: 5.366489494917914e-05\n","Batch 800/3470, Loss: 7.02183460816741e-05\n","Batch 810/3470, Loss: 9.60091856541112e-05\n","Batch 820/3470, Loss: 0.011039919219911098\n","Batch 830/3470, Loss: 6.631448195548728e-05\n","Batch 840/3470, Loss: 6.679863872705027e-05\n","Batch 850/3470, Loss: 6.390074850060046e-05\n","Batch 860/3470, Loss: 0.0008362993248738348\n","Batch 870/3470, Loss: 0.0005468521849252284\n","Batch 880/3470, Loss: 4.2705800296971574e-05\n","Batch 890/3470, Loss: 0.00044736420386470854\n","Batch 900/3470, Loss: 7.260895654326305e-05\n","Batch 910/3470, Loss: 0.002165287733078003\n","Batch 920/3470, Loss: 6.002651934977621e-05\n","Batch 930/3470, Loss: 0.0007137208012863994\n","Batch 940/3470, Loss: 5.9788188082166016e-05\n","Batch 950/3470, Loss: 7.922826625872403e-05\n","Batch 960/3470, Loss: 5.1816899940604344e-05\n","Batch 970/3470, Loss: 8.226468344219029e-05\n","Batch 980/3470, Loss: 6.345396104734391e-05\n","Batch 990/3470, Loss: 6.586077506653965e-05\n","Batch 1000/3470, Loss: 0.002819046378135681\n","Batch 1010/3470, Loss: 6.180779746500775e-05\n","Batch 1020/3470, Loss: 0.001400491688400507\n","Batch 1030/3470, Loss: 8.153451199177653e-05\n","Batch 1040/3470, Loss: 7.243736763484776e-05\n","Batch 1050/3470, Loss: 4.774952321895398e-05\n","Batch 1060/3470, Loss: 0.00015191517013590783\n","Batch 1070/3470, Loss: 0.0365760512650013\n","Batch 1080/3470, Loss: 4.6937158913351595e-05\n","Batch 1090/3470, Loss: 5.39327556907665e-05\n","Batch 1100/3470, Loss: 0.0011198846623301506\n","Batch 1110/3470, Loss: 0.0006210812134668231\n","Batch 1120/3470, Loss: 0.00026955120847560465\n","Batch 1130/3470, Loss: 0.0034709207247942686\n","Batch 1140/3470, Loss: 0.00012260337825864553\n","Batch 1150/3470, Loss: 9.293655602959916e-05\n","Batch 1160/3470, Loss: 3.756503065233119e-05\n","Batch 1170/3470, Loss: 5.765754758613184e-05\n","Batch 1180/3470, Loss: 4.126756175537594e-05\n","Batch 1190/3470, Loss: 0.00039191145333461463\n","Batch 1200/3470, Loss: 3.8012018194422126e-05\n","Batch 1210/3470, Loss: 4.4612774217966944e-05\n","Batch 1220/3470, Loss: 0.00011462174006737769\n","Batch 1230/3470, Loss: 0.00021014257799834013\n","Batch 1240/3470, Loss: 0.00034140824573114514\n","Batch 1250/3470, Loss: 0.00011778261978179216\n","Batch 1260/3470, Loss: 0.00030089973006397486\n","Batch 1270/3470, Loss: 5.391810191213153e-05\n","Batch 1280/3470, Loss: 8.57229097164236e-05\n","Batch 1290/3470, Loss: 6.478770956164226e-05\n","Batch 1300/3470, Loss: 0.00026314682327210903\n","Batch 1310/3470, Loss: 0.00012453588715288788\n","Batch 1320/3470, Loss: 0.0002335886674700305\n","Batch 1330/3470, Loss: 0.06760356575250626\n","Batch 1340/3470, Loss: 7.413760613417253e-05\n","Batch 1350/3470, Loss: 0.0006772749475203454\n","Batch 1360/3470, Loss: 0.00023580291599500924\n","Batch 1370/3470, Loss: 0.00013657272211275995\n","Batch 1380/3470, Loss: 8.176630944944918e-05\n","Batch 1390/3470, Loss: 0.00011436243948992342\n","Batch 1400/3470, Loss: 0.00010872057464439422\n","Batch 1410/3470, Loss: 7.35858193365857e-05\n","Batch 1420/3470, Loss: 5.909619358135387e-05\n","Batch 1430/3470, Loss: 8.702269406057894e-05\n","Batch 1440/3470, Loss: 0.004679362755268812\n","Batch 1450/3470, Loss: 6.864696479169652e-05\n","Batch 1460/3470, Loss: 0.002203817944973707\n","Batch 1470/3470, Loss: 6.1301194364205e-05\n","Batch 1480/3470, Loss: 0.0011361914221197367\n","Batch 1490/3470, Loss: 6.52420349069871e-05\n","Batch 1500/3470, Loss: 0.00924574863165617\n","Batch 1510/3470, Loss: 6.378962279995903e-05\n","Batch 1520/3470, Loss: 0.001144512789323926\n","Batch 1530/3470, Loss: 0.04848894849419594\n","Batch 1540/3470, Loss: 0.00012985403009224683\n","Batch 1550/3470, Loss: 0.17929290235042572\n","Batch 1560/3470, Loss: 7.436655869241804e-05\n","Batch 1570/3470, Loss: 0.00011943675053771585\n","Batch 1580/3470, Loss: 6.942179606994614e-05\n","Batch 1590/3470, Loss: 6.681383092654869e-05\n","Batch 1600/3470, Loss: 6.929883966222405e-05\n","Batch 1610/3470, Loss: 7.381648902082816e-05\n","Batch 1620/3470, Loss: 7.808522059349343e-05\n","Batch 1630/3470, Loss: 6.22022635070607e-05\n","Batch 1640/3470, Loss: 7.794807606842369e-05\n","Batch 1650/3470, Loss: 5.2018418500665575e-05\n","Batch 1660/3470, Loss: 6.499619485111907e-05\n","Batch 1670/3470, Loss: 7.15736168785952e-05\n","Batch 1680/3470, Loss: 7.719876157352701e-05\n","Batch 1690/3470, Loss: 0.015525760129094124\n","Batch 1700/3470, Loss: 5.1869450544472784e-05\n","Batch 1710/3470, Loss: 4.754099791171029e-05\n","Batch 1720/3470, Loss: 4.9999263865174726e-05\n","Batch 1730/3470, Loss: 0.0001725102192722261\n","Batch 1740/3470, Loss: 7.119200745364651e-05\n","Batch 1750/3470, Loss: 0.0007324599428102374\n","Batch 1760/3470, Loss: 5.508009417098947e-05\n","Batch 1770/3470, Loss: 0.00016400303866248578\n","Batch 1780/3470, Loss: 0.00015034343232400715\n","Batch 1790/3470, Loss: 5.910301115363836e-05\n","Batch 1800/3470, Loss: 0.003243362298235297\n","Batch 1810/3470, Loss: 0.001295012072660029\n","Batch 1820/3470, Loss: 8.515611989423633e-05\n","Batch 1830/3470, Loss: 6.217977352207527e-05\n","Batch 1840/3470, Loss: 7.334704423556104e-05\n","Batch 1850/3470, Loss: 0.03767130896449089\n","Batch 1860/3470, Loss: 0.0012373399222269654\n","Batch 1870/3470, Loss: 0.006805147510021925\n","Batch 1880/3470, Loss: 0.03132135048508644\n","Batch 1890/3470, Loss: 0.0008239429444074631\n","Batch 1900/3470, Loss: 0.00031849773949943483\n","Batch 1910/3470, Loss: 0.030164601281285286\n","Batch 1920/3470, Loss: 0.00016831542598083615\n","Batch 1930/3470, Loss: 0.00017362785001751035\n","Batch 1940/3470, Loss: 0.00041040763608179986\n","Batch 1950/3470, Loss: 0.0003191723662894219\n","Batch 1960/3470, Loss: 0.00018188677495345473\n","Batch 1970/3470, Loss: 0.00018850009655579925\n","Batch 1980/3470, Loss: 0.00016418784798588604\n","Batch 1990/3470, Loss: 0.00017662104801274836\n","Batch 2000/3470, Loss: 0.00012844068987760693\n","Batch 2010/3470, Loss: 0.0002749464474618435\n","Batch 2020/3470, Loss: 7.355666457442567e-05\n","Batch 2030/3470, Loss: 0.0006833482766523957\n","Batch 2040/3470, Loss: 0.0002775435277726501\n","Batch 2050/3470, Loss: 0.0011864331318065524\n","Batch 2060/3470, Loss: 0.00014043286500964314\n","Batch 2070/3470, Loss: 0.00033474466181360185\n","Batch 2080/3470, Loss: 0.0005292592104524374\n","Batch 2090/3470, Loss: 0.0013357772259041667\n","Batch 2100/3470, Loss: 0.0006743830745108426\n","Batch 2110/3470, Loss: 0.00636038463562727\n","Batch 2120/3470, Loss: 0.00035338904126547277\n","Batch 2130/3470, Loss: 0.0011279864702373743\n","Batch 2140/3470, Loss: 0.00029009877471253276\n","Batch 2150/3470, Loss: 0.0009945249184966087\n","Batch 2160/3470, Loss: 0.059492599219083786\n","Batch 2170/3470, Loss: 0.0029567964375019073\n","Batch 2180/3470, Loss: 0.0007171526667661965\n","Batch 2190/3470, Loss: 0.0015031372895464301\n","Batch 2200/3470, Loss: 0.012659011408686638\n","Batch 2210/3470, Loss: 0.000397925527067855\n","Batch 2220/3470, Loss: 0.00020006828708574176\n","Batch 2230/3470, Loss: 0.00036087652551941574\n","Batch 2240/3470, Loss: 0.002688043052330613\n","Batch 2250/3470, Loss: 0.000897651189006865\n","Batch 2260/3470, Loss: 0.002887407084926963\n","Batch 2270/3470, Loss: 0.00029679053113795817\n","Batch 2280/3470, Loss: 0.0003280996752437204\n","Batch 2290/3470, Loss: 8.728195098228753e-05\n","Batch 2300/3470, Loss: 0.0003050871309824288\n","Batch 2310/3470, Loss: 0.0001474837481509894\n","Batch 2320/3470, Loss: 0.19075298309326172\n","Batch 2330/3470, Loss: 0.00010741259029600769\n","Batch 2340/3470, Loss: 9.54504357650876e-05\n","Batch 2350/3470, Loss: 0.00021571750403381884\n","Batch 2360/3470, Loss: 7.804302003933117e-05\n","Batch 2370/3470, Loss: 0.0001726751506794244\n","Batch 2380/3470, Loss: 0.00016297907859552652\n","Batch 2390/3470, Loss: 0.001486868946813047\n","Batch 2400/3470, Loss: 0.0007150736055336893\n","Batch 2410/3470, Loss: 0.0003648451529443264\n","Batch 2420/3470, Loss: 3.611214197007939e-05\n","Batch 2430/3470, Loss: 4.0306345908902586e-05\n","Batch 2440/3470, Loss: 0.08364443480968475\n","Batch 2450/3470, Loss: 4.5767330448143184e-05\n","Batch 2460/3470, Loss: 0.00025974202435463667\n","Batch 2470/3470, Loss: 0.0031692255288362503\n","Batch 2480/3470, Loss: 0.000195561588043347\n","Batch 2490/3470, Loss: 0.00022784064640291035\n","Batch 2500/3470, Loss: 0.0002915433724410832\n","Batch 2510/3470, Loss: 0.00036357727367430925\n","Batch 2520/3470, Loss: 0.00014289861428551376\n","Batch 2530/3470, Loss: 0.0003730805474333465\n","Batch 2540/3470, Loss: 0.000335195247316733\n","Batch 2550/3470, Loss: 0.0006501592579297721\n","Batch 2560/3470, Loss: 0.00041291004163213074\n","Batch 2570/3470, Loss: 0.00015981026808731258\n","Batch 2580/3470, Loss: 0.00037499310565181077\n","Batch 2590/3470, Loss: 0.00037097017047926784\n","Batch 2600/3470, Loss: 0.0002856047358363867\n","Batch 2610/3470, Loss: 9.061080345418304e-05\n","Batch 2620/3470, Loss: 0.001664568088017404\n","Batch 2630/3470, Loss: 0.00019035850709769875\n","Batch 2640/3470, Loss: 0.00021459328127093613\n","Batch 2650/3470, Loss: 0.00020180316641926765\n","Batch 2660/3470, Loss: 0.22027115523815155\n","Batch 2670/3470, Loss: 0.0025636646896600723\n","Batch 2680/3470, Loss: 0.011012351140379906\n","Batch 2690/3470, Loss: 0.0004626990994438529\n","Batch 2700/3470, Loss: 0.0003506552893668413\n","Batch 2710/3470, Loss: 0.00024145869247149676\n","Batch 2720/3470, Loss: 0.04335225373506546\n","Batch 2730/3470, Loss: 0.0001006010061246343\n","Batch 2740/3470, Loss: 0.00020321230113040656\n","Batch 2750/3470, Loss: 0.000140362826641649\n","Batch 2760/3470, Loss: 0.0004047467082273215\n","Batch 2770/3470, Loss: 0.0002830976736731827\n","Batch 2780/3470, Loss: 0.00176617328543216\n","Batch 2790/3470, Loss: 0.0016358260763809085\n","Batch 2800/3470, Loss: 0.00018250818538945168\n","Batch 2810/3470, Loss: 6.637806654907763e-05\n","Batch 2820/3470, Loss: 0.00017379221390001476\n","Batch 2830/3470, Loss: 0.0002701464982237667\n","Batch 2840/3470, Loss: 0.00013404646597336978\n","Batch 2850/3470, Loss: 0.00011622824240475893\n","Batch 2860/3470, Loss: 8.783830708125606e-05\n","Batch 2870/3470, Loss: 0.00010111910523846745\n","Batch 2880/3470, Loss: 0.00011563155567273498\n","Batch 2890/3470, Loss: 6.310723256319761e-05\n","Batch 2900/3470, Loss: 0.0005000024102628231\n","Batch 2910/3470, Loss: 8.787200204096735e-05\n","Batch 2920/3470, Loss: 0.00013723669690079987\n","Batch 2930/3470, Loss: 4.4655389501713216e-05\n","Batch 2940/3470, Loss: 0.00029023707611486316\n","Batch 2950/3470, Loss: 7.711290527367964e-05\n","Batch 2960/3470, Loss: 8.74075703904964e-05\n","Batch 2970/3470, Loss: 0.0001341389724984765\n","Batch 2980/3470, Loss: 0.00011527177412062883\n","Batch 2990/3470, Loss: 0.0004504852113313973\n","Batch 3000/3470, Loss: 3.723571717273444e-05\n","Batch 3010/3470, Loss: 0.0003893956891261041\n","Batch 3020/3470, Loss: 4.421538324095309e-05\n","Batch 3030/3470, Loss: 5.910863183089532e-05\n","Batch 3040/3470, Loss: 0.0001242252765223384\n","Batch 3050/3470, Loss: 0.00017686575301922858\n","Batch 3060/3470, Loss: 7.077574991853908e-05\n","Batch 3070/3470, Loss: 5.18295491929166e-05\n","Batch 3080/3470, Loss: 7.288347114808857e-05\n","Batch 3090/3470, Loss: 0.07412165403366089\n","Batch 3100/3470, Loss: 6.097095683799125e-05\n","Batch 3110/3470, Loss: 0.0005986097385175526\n","Batch 3120/3470, Loss: 8.481187978759408e-05\n","Batch 3130/3470, Loss: 0.0024251453578472137\n","Batch 3140/3470, Loss: 4.5117962145013735e-05\n","Batch 3150/3470, Loss: 6.50983492960222e-05\n","Batch 3160/3470, Loss: 0.001715400954708457\n","Batch 3170/3470, Loss: 5.542764120036736e-05\n","Batch 3180/3470, Loss: 3.8063168176449835e-05\n","Batch 3190/3470, Loss: 4.3903637561015785e-05\n","Batch 3200/3470, Loss: 0.00014997283869888633\n","Batch 3210/3470, Loss: 3.804100924753584e-05\n","Batch 3220/3470, Loss: 0.2991620600223541\n","Batch 3230/3470, Loss: 0.0002264604700030759\n","Batch 3240/3470, Loss: 0.00012663191591855139\n","Batch 3250/3470, Loss: 0.00014412865857593715\n","Batch 3260/3470, Loss: 0.0026973471976816654\n","Batch 3270/3470, Loss: 0.001528411521576345\n","Batch 3280/3470, Loss: 0.0005307744722813368\n","Batch 3290/3470, Loss: 0.0004921437939628959\n","Batch 3300/3470, Loss: 7.85874726716429e-05\n","Batch 3310/3470, Loss: 0.0006814876105636358\n","Batch 3320/3470, Loss: 0.00045371809392236173\n","Batch 3330/3470, Loss: 0.0009212210425175726\n","Batch 3340/3470, Loss: 2.8729014957207255e-05\n","Batch 3350/3470, Loss: 4.2295305320294574e-05\n","Batch 3360/3470, Loss: 4.6451572416117415e-05\n","Batch 3370/3470, Loss: 2.931749986601062e-05\n","Batch 3380/3470, Loss: 6.209937419043854e-05\n","Batch 3390/3470, Loss: 5.953290019533597e-05\n","Batch 3400/3470, Loss: 0.002206426113843918\n","Batch 3410/3470, Loss: 3.9695354644209146e-05\n","Batch 3420/3470, Loss: 6.018999556545168e-05\n","Batch 3430/3470, Loss: 5.40269938937854e-05\n","Batch 3440/3470, Loss: 2.6434292522026226e-05\n","Batch 3450/3470, Loss: 7.643520802957937e-05\n","Batch 3460/3470, Loss: 7.157223444664851e-05\n","Training epoch completed in: 9m 37s\n","Train loss 0.005757632303543857 accuracy 0.9982888124358305\n","Batch 0/868, Test Loss: 3.900947922375053e-05\n","Batch 10/868, Test Loss: 4.9580026825424284e-05\n","Batch 20/868, Test Loss: 4.458958937902935e-05\n","Batch 30/868, Test Loss: 0.0045730010606348515\n","Batch 40/868, Test Loss: 2.224718627985567e-05\n","Batch 50/868, Test Loss: 5.146440526004881e-05\n","Batch 60/868, Test Loss: 0.010287287645041943\n","Batch 70/868, Test Loss: 4.601923137670383e-05\n","Batch 80/868, Test Loss: 6.53949537081644e-05\n","Batch 90/868, Test Loss: 0.00024443346774205565\n","Batch 100/868, Test Loss: 6.625804235227406e-05\n","Batch 110/868, Test Loss: 0.4626292586326599\n","Batch 120/868, Test Loss: 7.954140164656565e-05\n","Batch 130/868, Test Loss: 3.136582745355554e-05\n","Batch 140/868, Test Loss: 0.011870670132339\n","Batch 150/868, Test Loss: 0.43448972702026367\n","Batch 160/868, Test Loss: 5.787982081528753e-05\n","Batch 170/868, Test Loss: 0.5729268789291382\n","Batch 180/868, Test Loss: 8.701021579327062e-05\n","Batch 190/868, Test Loss: 2.5786022888496518e-05\n","Batch 200/868, Test Loss: 0.045703355222940445\n","Batch 210/868, Test Loss: 0.00011519988038344309\n","Batch 220/868, Test Loss: 4.089425419806503e-05\n","Batch 230/868, Test Loss: 0.17417985200881958\n","Batch 240/868, Test Loss: 4.938721758662723e-05\n","Batch 250/868, Test Loss: 0.48991113901138306\n","Batch 260/868, Test Loss: 0.4574557840824127\n","Batch 270/868, Test Loss: 0.6623045206069946\n","Batch 280/868, Test Loss: 0.012483632192015648\n","Batch 290/868, Test Loss: 0.5663986206054688\n","Batch 300/868, Test Loss: 0.06835321336984634\n","Batch 310/868, Test Loss: 0.046639468520879745\n","Batch 320/868, Test Loss: 0.00014600071881432086\n","Batch 330/868, Test Loss: 0.18805503845214844\n","Batch 340/868, Test Loss: 6.867529009468853e-05\n","Batch 350/868, Test Loss: 5.486272129928693e-05\n","Batch 360/868, Test Loss: 0.03210864216089249\n","Batch 370/868, Test Loss: 0.08671092242002487\n","Batch 380/868, Test Loss: 6.023756941431202e-05\n","Batch 390/868, Test Loss: 4.725668259197846e-05\n","Batch 400/868, Test Loss: 6.114095594966784e-05\n","Batch 410/868, Test Loss: 0.0002541885187383741\n","Batch 420/868, Test Loss: 0.10842246562242508\n","Batch 430/868, Test Loss: 0.0001248334883712232\n","Batch 440/868, Test Loss: 0.05134633183479309\n","Batch 450/868, Test Loss: 0.004315474070608616\n","Batch 460/868, Test Loss: 0.0001572246546857059\n","Batch 470/868, Test Loss: 4.774827903020196e-05\n","Batch 480/868, Test Loss: 6.977569864830002e-05\n","Batch 490/868, Test Loss: 0.09512023627758026\n","Batch 500/868, Test Loss: 0.0020994741935282946\n","Batch 510/868, Test Loss: 4.653417272493243e-05\n","Batch 520/868, Test Loss: 3.951603503082879e-05\n","Batch 530/868, Test Loss: 7.430179539369419e-05\n","Batch 540/868, Test Loss: 3.744523201021366e-05\n","Batch 550/868, Test Loss: 0.2885257601737976\n","Batch 560/868, Test Loss: 0.2081586867570877\n","Batch 570/868, Test Loss: 4.155755596002564e-05\n","Batch 580/868, Test Loss: 4.5012660848442465e-05\n","Batch 590/868, Test Loss: 0.06186886504292488\n","Batch 600/868, Test Loss: 5.374296597437933e-05\n","Batch 610/868, Test Loss: 7.13942717993632e-05\n","Batch 620/868, Test Loss: 0.13137193024158478\n","Batch 630/868, Test Loss: 0.46617498993873596\n","Batch 640/868, Test Loss: 0.443983793258667\n","Batch 650/868, Test Loss: 0.0004903162480331957\n","Batch 660/868, Test Loss: 0.2805335521697998\n","Batch 670/868, Test Loss: 0.01778443716466427\n","Batch 680/868, Test Loss: 3.8197453250177205e-05\n","Batch 690/868, Test Loss: 0.030615011230111122\n","Batch 700/868, Test Loss: 0.3911566734313965\n","Batch 710/868, Test Loss: 4.102114326087758e-05\n","Batch 720/868, Test Loss: 0.0065144822001457214\n","Batch 730/868, Test Loss: 0.0008337386534549296\n","Batch 740/868, Test Loss: 0.00021550021483562887\n","Batch 750/868, Test Loss: 0.4179690182209015\n","Batch 760/868, Test Loss: 3.936711073038168e-05\n","Batch 770/868, Test Loss: 5.2687064453493804e-05\n","Batch 780/868, Test Loss: 0.05136527866125107\n","Batch 790/868, Test Loss: 4.174353671260178e-05\n","Batch 800/868, Test Loss: 5.9645575674949214e-05\n","Batch 810/868, Test Loss: 3.094119165325537e-05\n","Batch 820/868, Test Loss: 0.0003571549605112523\n","Batch 830/868, Test Loss: 0.4767260253429413\n","Batch 840/868, Test Loss: 0.6699425578117371\n","Batch 850/868, Test Loss: 1.3402581214904785\n","Batch 860/868, Test Loss: 0.00010282500443281606\n","Test evaluation completed in: 0m 46s\n","Test loss 0.08959230090437734 accuracy 0.9812680115273775\n","Test Precision: 0.9813711382598058\n","Test Recall: 0.9812680115273775\n","Test F1 Score: 0.9813139486609886\n","Starting epoch 6/10\n","Batch 0/3470, Loss: 7.763345638522878e-05\n","Batch 10/3470, Loss: 0.00021462867152877152\n","Batch 20/3470, Loss: 0.00014460334205068648\n","Batch 30/3470, Loss: 0.0019115975592285395\n","Batch 40/3470, Loss: 0.00016301007417496294\n","Batch 50/3470, Loss: 0.0001360962342005223\n","Batch 60/3470, Loss: 0.00012172670540167019\n","Batch 70/3470, Loss: 0.0005403971299529076\n","Batch 80/3470, Loss: 0.000129442909383215\n","Batch 90/3470, Loss: 0.00011040836398024112\n","Batch 100/3470, Loss: 0.0001544628175906837\n","Batch 110/3470, Loss: 7.202184497145936e-05\n","Batch 120/3470, Loss: 8.052230259636417e-05\n","Batch 130/3470, Loss: 9.176316962111741e-05\n","Batch 140/3470, Loss: 0.00011651415115920827\n","Batch 150/3470, Loss: 5.7918856327887625e-05\n","Batch 160/3470, Loss: 6.154666334623471e-05\n","Batch 170/3470, Loss: 0.001956604653969407\n","Batch 180/3470, Loss: 7.120155351003632e-05\n","Batch 190/3470, Loss: 0.00011714877473423257\n","Batch 200/3470, Loss: 8.564365998608992e-05\n","Batch 210/3470, Loss: 0.0001042149742715992\n","Batch 220/3470, Loss: 5.61901179025881e-05\n","Batch 230/3470, Loss: 4.6192286390578374e-05\n","Batch 240/3470, Loss: 4.66912861156743e-05\n","Batch 250/3470, Loss: 5.176505510462448e-05\n","Batch 260/3470, Loss: 6.45715044811368e-05\n","Batch 270/3470, Loss: 6.802047573728487e-05\n","Batch 280/3470, Loss: 5.36942679900676e-05\n","Batch 290/3470, Loss: 6.089125236030668e-05\n","Batch 300/3470, Loss: 5.4536081734113395e-05\n","Batch 310/3470, Loss: 5.7724464568309486e-05\n","Batch 320/3470, Loss: 6.232843588804826e-05\n","Batch 330/3470, Loss: 4.2586401832522824e-05\n","Batch 340/3470, Loss: 3.636556721176021e-05\n","Batch 350/3470, Loss: 6.672359450021759e-05\n","Batch 360/3470, Loss: 7.990669837454334e-05\n","Batch 370/3470, Loss: 9.792557830223814e-05\n","Batch 380/3470, Loss: 4.7257439291570336e-05\n","Batch 390/3470, Loss: 4.1483755921944976e-05\n","Batch 400/3470, Loss: 5.1950719353044406e-05\n","Batch 410/3470, Loss: 4.984963743481785e-05\n","Batch 420/3470, Loss: 0.0001361078320769593\n","Batch 430/3470, Loss: 4.622949563781731e-05\n","Batch 440/3470, Loss: 4.184119825367816e-05\n","Batch 450/3470, Loss: 4.2556228436296806e-05\n","Batch 460/3470, Loss: 4.479898780118674e-05\n","Batch 470/3470, Loss: 3.443593959673308e-05\n","Batch 480/3470, Loss: 3.893515531672165e-05\n","Batch 490/3470, Loss: 4.20126598328352e-05\n","Batch 500/3470, Loss: 0.0001520247315056622\n","Batch 510/3470, Loss: 5.8648442063713446e-05\n","Batch 520/3470, Loss: 4.8740108468336985e-05\n","Batch 530/3470, Loss: 0.0008500192197971046\n","Batch 540/3470, Loss: 6.0145834140712395e-05\n","Batch 550/3470, Loss: 4.951481969328597e-05\n","Batch 560/3470, Loss: 0.000467370031401515\n","Batch 570/3470, Loss: 5.06620344822295e-05\n","Batch 580/3470, Loss: 3.874205503962003e-05\n","Batch 590/3470, Loss: 0.00020410183060448617\n","Batch 600/3470, Loss: 4.076095865457319e-05\n","Batch 610/3470, Loss: 4.418807293404825e-05\n","Batch 620/3470, Loss: 3.6745394027093425e-05\n","Batch 630/3470, Loss: 3.512883995426819e-05\n","Batch 640/3470, Loss: 3.53150608134456e-05\n","Batch 650/3470, Loss: 3.231262962799519e-05\n","Batch 660/3470, Loss: 3.259571167291142e-05\n","Batch 670/3470, Loss: 3.479351653368212e-05\n","Batch 680/3470, Loss: 4.387517037685029e-05\n","Batch 690/3470, Loss: 4.791311948793009e-05\n","Batch 700/3470, Loss: 3.576194649212994e-05\n","Batch 710/3470, Loss: 4.058211561641656e-05\n","Batch 720/3470, Loss: 3.33182979375124e-05\n","Batch 730/3470, Loss: 4.528982026386075e-05\n","Batch 740/3470, Loss: 7.813197589712217e-05\n","Batch 750/3470, Loss: 3.7542518839472905e-05\n","Batch 760/3470, Loss: 5.103412695461884e-05\n","Batch 770/3470, Loss: 0.0003399444103706628\n","Batch 780/3470, Loss: 0.0003054867556784302\n","Batch 790/3470, Loss: 3.231254959246144e-05\n","Batch 800/3470, Loss: 3.813115836237557e-05\n","Batch 810/3470, Loss: 4.660181366489269e-05\n","Batch 820/3470, Loss: 4.549105142359622e-05\n","Batch 830/3470, Loss: 3.0144499760353938e-05\n","Batch 840/3470, Loss: 3.078522058785893e-05\n","Batch 850/3470, Loss: 3.448045754339546e-05\n","Batch 860/3470, Loss: 6.60889272694476e-05\n","Batch 870/3470, Loss: 4.517874549492262e-05\n","Batch 880/3470, Loss: 2.5733963411767036e-05\n","Batch 890/3470, Loss: 3.418246342334896e-05\n","Batch 900/3470, Loss: 3.871961234835908e-05\n","Batch 910/3470, Loss: 0.00019049271941184998\n","Batch 920/3470, Loss: 3.162709981552325e-05\n","Batch 930/3470, Loss: 4.760506271850318e-05\n","Batch 940/3470, Loss: 2.6903589969151653e-05\n","Batch 950/3470, Loss: 0.00022803505999036133\n","Batch 960/3470, Loss: 3.2461510272696614e-05\n","Batch 970/3470, Loss: 3.902509342879057e-05\n","Batch 980/3470, Loss: 2.542101537983399e-05\n","Batch 990/3470, Loss: 2.6456602427060716e-05\n","Batch 1000/3470, Loss: 3.640238719526678e-05\n","Batch 1010/3470, Loss: 2.9473978429450653e-05\n","Batch 1020/3470, Loss: 0.15455961227416992\n","Batch 1030/3470, Loss: 4.865077426075004e-05\n","Batch 1040/3470, Loss: 3.982224006904289e-05\n","Batch 1050/3470, Loss: 4.4850494305137545e-05\n","Batch 1060/3470, Loss: 3.696127896546386e-05\n","Batch 1070/3470, Loss: 0.09416329115629196\n","Batch 1080/3470, Loss: 3.918150105164386e-05\n","Batch 1090/3470, Loss: 0.00012342623085714877\n","Batch 1100/3470, Loss: 7.671974162803963e-05\n","Batch 1110/3470, Loss: 5.577258707489818e-05\n","Batch 1120/3470, Loss: 5.4095478844828904e-05\n","Batch 1130/3470, Loss: 9.70763067016378e-05\n","Batch 1140/3470, Loss: 0.00011974976951023564\n","Batch 1150/3470, Loss: 3.7058307498227805e-05\n","Batch 1160/3470, Loss: 2.882585977204144e-05\n","Batch 1170/3470, Loss: 4.0910006646299735e-05\n","Batch 1180/3470, Loss: 3.5985409340355545e-05\n","Batch 1190/3470, Loss: 7.648624159628525e-05\n","Batch 1200/3470, Loss: 2.849058000720106e-05\n","Batch 1210/3470, Loss: 2.8453294362407178e-05\n","Batch 1220/3470, Loss: 4.0440630982629955e-05\n","Batch 1230/3470, Loss: 2.560731263656635e-05\n","Batch 1240/3470, Loss: 3.392922371858731e-05\n","Batch 1250/3470, Loss: 3.8890575524419546e-05\n","Batch 1260/3470, Loss: 0.005165646784007549\n","Batch 1270/3470, Loss: 2.641937680891715e-05\n","Batch 1280/3470, Loss: 9.798527753446251e-05\n","Batch 1290/3470, Loss: 3.808621477219276e-05\n","Batch 1300/3470, Loss: 0.003158550476655364\n","Batch 1310/3470, Loss: 2.51230248977663e-05\n","Batch 1320/3470, Loss: 0.0001490487775299698\n","Batch 1330/3470, Loss: 2.4944178221630864e-05\n","Batch 1340/3470, Loss: 2.8274511350900866e-05\n","Batch 1350/3470, Loss: 5.039047755417414e-05\n","Batch 1360/3470, Loss: 4.163262201473117e-05\n","Batch 1370/3470, Loss: 3.872668821713887e-05\n","Batch 1380/3470, Loss: 2.995078102685511e-05\n","Batch 1390/3470, Loss: 2.785725155263208e-05\n","Batch 1400/3470, Loss: 2.9824162993463688e-05\n","Batch 1410/3470, Loss: 2.6627987608662806e-05\n","Batch 1420/3470, Loss: 2.2850666937301867e-05\n","Batch 1430/3470, Loss: 2.982412297569681e-05\n","Batch 1440/3470, Loss: 4.845364674110897e-05\n","Batch 1450/3470, Loss: 3.6834695492871106e-05\n","Batch 1460/3470, Loss: 0.000137789043947123\n","Batch 1470/3470, Loss: 4.38293645856902e-05\n","Batch 1480/3470, Loss: 6.552251579705626e-05\n","Batch 1490/3470, Loss: 0.00016551735461689532\n","Batch 1500/3470, Loss: 0.007622991688549519\n","Batch 1510/3470, Loss: 3.711025419761427e-05\n","Batch 1520/3470, Loss: 3.142594505334273e-05\n","Batch 1530/3470, Loss: 4.745129626826383e-05\n","Batch 1540/3470, Loss: 4.5387521822704e-05\n","Batch 1550/3470, Loss: 0.00019939040066674352\n","Batch 1560/3470, Loss: 3.9189093513414264e-05\n","Batch 1570/3470, Loss: 0.0001483987580286339\n","Batch 1580/3470, Loss: 3.873456080327742e-05\n","Batch 1590/3470, Loss: 3.7281755794538185e-05\n","Batch 1600/3470, Loss: 0.0004928648704662919\n","Batch 1610/3470, Loss: 8.196727139875293e-05\n","Batch 1620/3470, Loss: 9.88491447060369e-05\n","Batch 1630/3470, Loss: 0.00012309102748986334\n","Batch 1640/3470, Loss: 6.360971019603312e-05\n","Batch 1650/3470, Loss: 7.357389404205605e-05\n","Batch 1660/3470, Loss: 0.00028186372946947813\n","Batch 1670/3470, Loss: 4.544725743471645e-05\n","Batch 1680/3470, Loss: 6.336398655548692e-05\n","Batch 1690/3470, Loss: 0.00048175835399888456\n","Batch 1700/3470, Loss: 3.6015408113598824e-05\n","Batch 1710/3470, Loss: 4.314459511078894e-05\n","Batch 1720/3470, Loss: 6.845770258223638e-05\n","Batch 1730/3470, Loss: 0.00014056281361263245\n","Batch 1740/3470, Loss: 0.0006164521328173578\n","Batch 1750/3470, Loss: 3.623140219133347e-05\n","Batch 1760/3470, Loss: 3.1269537430489436e-05\n","Batch 1770/3470, Loss: 0.00011291517876088619\n","Batch 1780/3470, Loss: 3.552357156877406e-05\n","Batch 1790/3470, Loss: 5.6672080972930416e-05\n","Batch 1800/3470, Loss: 0.002288846066221595\n","Batch 1810/3470, Loss: 0.00025185360573232174\n","Batch 1820/3470, Loss: 2.7916888939216733e-05\n","Batch 1830/3470, Loss: 3.26551562466193e-05\n","Batch 1840/3470, Loss: 0.12362895160913467\n","Batch 1850/3470, Loss: 6.556926382472739e-05\n","Batch 1860/3470, Loss: 2.5227334845112637e-05\n","Batch 1870/3470, Loss: 0.0017952955095097423\n","Batch 1880/3470, Loss: 2.731335007410962e-05\n","Batch 1890/3470, Loss: 5.948948455625214e-05\n","Batch 1900/3470, Loss: 4.4292381062405184e-05\n","Batch 1910/3470, Loss: 5.7214398111682385e-05\n","Batch 1920/3470, Loss: 4.009051917819306e-05\n","Batch 1930/3470, Loss: 3.278190706623718e-05\n","Batch 1940/3470, Loss: 0.0005767049733549356\n","Batch 1950/3470, Loss: 3.702838876051828e-05\n","Batch 1960/3470, Loss: 0.5280991792678833\n","Batch 1970/3470, Loss: 0.0003365101001691073\n","Batch 1980/3470, Loss: 3.2662792364135385e-05\n","Batch 1990/3470, Loss: 0.0010025681694969535\n","Batch 2000/3470, Loss: 0.002655342221260071\n","Batch 2010/3470, Loss: 0.0012121153995394707\n","Batch 2020/3470, Loss: 0.000595812511164695\n","Batch 2030/3470, Loss: 0.01823996566236019\n","Batch 2040/3470, Loss: 0.00132947345264256\n","Batch 2050/3470, Loss: 0.0007448571268469095\n","Batch 2060/3470, Loss: 0.06833575665950775\n","Batch 2070/3470, Loss: 0.0005728592514060438\n","Batch 2080/3470, Loss: 0.0002509380574338138\n","Batch 2090/3470, Loss: 0.00010271621431456879\n","Batch 2100/3470, Loss: 0.0001639385736780241\n","Batch 2110/3470, Loss: 0.001006942125968635\n","Batch 2120/3470, Loss: 0.0002479573304299265\n","Batch 2130/3470, Loss: 0.002697506221011281\n","Batch 2140/3470, Loss: 0.0002693014685064554\n","Batch 2150/3470, Loss: 0.0003609990526456386\n","Batch 2160/3470, Loss: 0.004415589384734631\n","Batch 2170/3470, Loss: 0.0008556113461963832\n","Batch 2180/3470, Loss: 0.00010129665315616876\n","Batch 2190/3470, Loss: 0.0004101443337276578\n","Batch 2200/3470, Loss: 0.04878489300608635\n","Batch 2210/3470, Loss: 0.0006852565566077828\n","Batch 2220/3470, Loss: 0.0001440755440853536\n","Batch 2230/3470, Loss: 6.09707749390509e-05\n","Batch 2240/3470, Loss: 0.0003093661507591605\n","Batch 2250/3470, Loss: 0.001487907487899065\n","Batch 2260/3470, Loss: 0.007755286525934935\n","Batch 2270/3470, Loss: 0.0002546788309700787\n","Batch 2280/3470, Loss: 0.00024675310123711824\n","Batch 2290/3470, Loss: 6.320011743810028e-05\n","Batch 2300/3470, Loss: 0.0009819153929129243\n","Batch 2310/3470, Loss: 3.441336957621388e-05\n","Batch 2320/3470, Loss: 0.04005115106701851\n","Batch 2330/3470, Loss: 0.0007270252681337297\n","Batch 2340/3470, Loss: 0.00017950154142454267\n","Batch 2350/3470, Loss: 0.00016713746299501508\n","Batch 2360/3470, Loss: 3.0874605727149174e-05\n","Batch 2370/3470, Loss: 5.4528427426703274e-05\n","Batch 2380/3470, Loss: 3.922605174011551e-05\n","Batch 2390/3470, Loss: 0.00048544525634497404\n","Batch 2400/3470, Loss: 0.0001930034050019458\n","Batch 2410/3470, Loss: 0.0003802420978900045\n","Batch 2420/3470, Loss: 4.1960400267271325e-05\n","Batch 2430/3470, Loss: 6.0612706874962896e-05\n","Batch 2440/3470, Loss: 0.0005719840410165489\n","Batch 2450/3470, Loss: 4.543161776382476e-05\n","Batch 2460/3470, Loss: 0.00016656660591252148\n","Batch 2470/3470, Loss: 0.00020796293392777443\n","Batch 2480/3470, Loss: 0.00014747852401342243\n","Batch 2490/3470, Loss: 0.00014014304906595498\n","Batch 2500/3470, Loss: 0.00010323458263883367\n","Batch 2510/3470, Loss: 0.0007035600719973445\n","Batch 2520/3470, Loss: 8.487046579830348e-05\n","Batch 2530/3470, Loss: 0.00018055926193483174\n","Batch 2540/3470, Loss: 0.0001291074586333707\n","Batch 2550/3470, Loss: 0.00018100417219102383\n","Batch 2560/3470, Loss: 0.0001333077234448865\n","Batch 2570/3470, Loss: 0.00010766430932562798\n","Batch 2580/3470, Loss: 0.00025980034843087196\n","Batch 2590/3470, Loss: 0.00019888125825673342\n","Batch 2600/3470, Loss: 0.0001897734182421118\n","Batch 2610/3470, Loss: 6.063895853003487e-05\n","Batch 2620/3470, Loss: 0.0007243719301186502\n","Batch 2630/3470, Loss: 0.000222367059905082\n","Batch 2640/3470, Loss: 0.00022648318554274738\n","Batch 2650/3470, Loss: 0.00015288600116036832\n","Batch 2660/3470, Loss: 0.00018747405556496233\n","Batch 2670/3470, Loss: 0.00022858880402054638\n","Batch 2680/3470, Loss: 0.00010014906001742929\n","Batch 2690/3470, Loss: 0.0001256304676644504\n","Batch 2700/3470, Loss: 0.00033250017440877855\n","Batch 2710/3470, Loss: 0.00010323000606149435\n","Batch 2720/3470, Loss: 6.635473255300894e-05\n","Batch 2730/3470, Loss: 6.901545566506684e-05\n","Batch 2740/3470, Loss: 0.00014112259668763727\n","Batch 2750/3470, Loss: 9.234342724084854e-05\n","Batch 2760/3470, Loss: 9.273365139961243e-05\n","Batch 2770/3470, Loss: 0.0002568708441685885\n","Batch 2780/3470, Loss: 0.0002598162100184709\n","Batch 2790/3470, Loss: 0.012979223392903805\n","Batch 2800/3470, Loss: 6.204127566888928e-05\n","Batch 2810/3470, Loss: 5.5047588830348104e-05\n","Batch 2820/3470, Loss: 8.60157233546488e-05\n","Batch 2830/3470, Loss: 8.693975541973487e-05\n","Batch 2840/3470, Loss: 9.626922837924212e-05\n","Batch 2850/3470, Loss: 0.00010398512677056715\n","Batch 2860/3470, Loss: 9.937012509908527e-05\n","Batch 2870/3470, Loss: 8.764923404669389e-05\n","Batch 2880/3470, Loss: 0.0006556790322065353\n","Batch 2890/3470, Loss: 7.557466597063467e-05\n","Batch 2900/3470, Loss: 0.0006626828690059483\n","Batch 2910/3470, Loss: 5.651259562000632e-05\n","Batch 2920/3470, Loss: 0.0007149559678509831\n","Batch 2930/3470, Loss: 5.109188350616023e-05\n","Batch 2940/3470, Loss: 0.00015039158461149782\n","Batch 2950/3470, Loss: 0.00012625035014934838\n","Batch 2960/3470, Loss: 0.00022504075604956597\n","Batch 2970/3470, Loss: 0.0003115813306067139\n","Batch 2980/3470, Loss: 0.0005102470749989152\n","Batch 2990/3470, Loss: 0.0008228727383539081\n","Batch 3000/3470, Loss: 7.627061131643131e-05\n","Batch 3010/3470, Loss: 0.0004895303281955421\n","Batch 3020/3470, Loss: 8.630876982351765e-05\n","Batch 3030/3470, Loss: 9.083243639906868e-05\n","Batch 3040/3470, Loss: 0.00011475316568976268\n","Batch 3050/3470, Loss: 0.00019344029715284705\n","Batch 3060/3470, Loss: 0.00011370173160685226\n","Batch 3070/3470, Loss: 5.983045411994681e-05\n","Batch 3080/3470, Loss: 0.00012728023284580559\n","Batch 3090/3470, Loss: 0.0005997042171657085\n","Batch 3100/3470, Loss: 0.0029313606210052967\n","Batch 3110/3470, Loss: 0.06884939968585968\n","Batch 3120/3470, Loss: 0.00014605905744247139\n","Batch 3130/3470, Loss: 0.012907578609883785\n","Batch 3140/3470, Loss: 5.106348544359207e-05\n","Batch 3150/3470, Loss: 8.877317304722965e-05\n","Batch 3160/3470, Loss: 6.0906280850758776e-05\n","Batch 3170/3470, Loss: 6.700445374008268e-05\n","Batch 3180/3470, Loss: 6.43010062049143e-05\n","Batch 3190/3470, Loss: 3.8026228139642626e-05\n","Batch 3200/3470, Loss: 6.804746226407588e-05\n","Batch 3210/3470, Loss: 0.00010373324766987935\n","Batch 3220/3470, Loss: 0.00042020203545689583\n","Batch 3230/3470, Loss: 0.002390485955402255\n","Batch 3240/3470, Loss: 0.001756800338625908\n","Batch 3250/3470, Loss: 7.745508628431708e-05\n","Batch 3260/3470, Loss: 0.0003875156689900905\n","Batch 3270/3470, Loss: 0.000567826209589839\n","Batch 3280/3470, Loss: 0.00012619164772331715\n","Batch 3290/3470, Loss: 0.0008470739121548831\n","Batch 3300/3470, Loss: 5.720882472814992e-05\n","Batch 3310/3470, Loss: 0.12834429740905762\n","Batch 3320/3470, Loss: 0.0008051631157286465\n","Batch 3330/3470, Loss: 0.0004408491950016469\n","Batch 3340/3470, Loss: 3.183581429766491e-05\n","Batch 3350/3470, Loss: 5.710541518055834e-05\n","Batch 3360/3470, Loss: 0.00016993877943605185\n","Batch 3370/3470, Loss: 5.251478432910517e-05\n","Batch 3380/3470, Loss: 0.0001669284829404205\n","Batch 3390/3470, Loss: 4.9484093324281275e-05\n","Batch 3400/3470, Loss: 0.00016896449960768223\n","Batch 3410/3470, Loss: 4.491780055104755e-05\n","Batch 3420/3470, Loss: 8.563257870264351e-05\n","Batch 3430/3470, Loss: 8.457751391688362e-05\n","Batch 3440/3470, Loss: 2.827453863574192e-05\n","Batch 3450/3470, Loss: 0.00010582361574051902\n","Batch 3460/3470, Loss: 7.84411677159369e-05\n","Training epoch completed in: 9m 37s\n","Train loss 0.0031167779954971893 accuracy 0.9990813624655511\n","Batch 0/868, Test Loss: 4.543899558484554e-05\n","Batch 10/868, Test Loss: 3.6938658013241366e-05\n","Batch 20/868, Test Loss: 4.555815758067183e-05\n","Batch 30/868, Test Loss: 0.02313462272286415\n","Batch 40/868, Test Loss: 2.8125545213697478e-05\n","Batch 50/868, Test Loss: 6.35040269116871e-05\n","Batch 60/868, Test Loss: 0.1270156353712082\n","Batch 70/868, Test Loss: 2.8207497962284833e-05\n","Batch 80/868, Test Loss: 5.53321951883845e-05\n","Batch 90/868, Test Loss: 0.06914624571800232\n","Batch 100/868, Test Loss: 0.0003559653996489942\n","Batch 110/868, Test Loss: 0.2518031597137451\n","Batch 120/868, Test Loss: 0.002317286329343915\n","Batch 130/868, Test Loss: 3.687909702421166e-05\n","Batch 140/868, Test Loss: 0.00032228624331764877\n","Batch 150/868, Test Loss: 0.5144174695014954\n","Batch 160/868, Test Loss: 5.4758831538492814e-05\n","Batch 170/868, Test Loss: 0.6633565425872803\n","Batch 180/868, Test Loss: 0.02355441078543663\n","Batch 190/868, Test Loss: 2.68291787506314e-05\n","Batch 200/868, Test Loss: 0.0017048154259100556\n","Batch 210/868, Test Loss: 0.0005641417810693383\n","Batch 220/868, Test Loss: 4.50813167844899e-05\n","Batch 230/868, Test Loss: 0.11634550243616104\n","Batch 240/868, Test Loss: 5.419260196504183e-05\n","Batch 250/868, Test Loss: 0.3609009385108948\n","Batch 260/868, Test Loss: 0.6041542291641235\n","Batch 270/868, Test Loss: 0.6646151542663574\n","Batch 280/868, Test Loss: 0.001482026418671012\n","Batch 290/868, Test Loss: 0.5144644975662231\n","Batch 300/868, Test Loss: 0.151032492518425\n","Batch 310/868, Test Loss: 0.031363703310489655\n","Batch 320/868, Test Loss: 0.0001445429224986583\n","Batch 330/868, Test Loss: 0.4562581181526184\n","Batch 340/868, Test Loss: 7.967822602950037e-05\n","Batch 350/868, Test Loss: 0.00012995144061278552\n","Batch 360/868, Test Loss: 0.005596673581749201\n","Batch 370/868, Test Loss: 0.05105653032660484\n","Batch 380/868, Test Loss: 6.598159961868078e-05\n","Batch 390/868, Test Loss: 4.6355315134860575e-05\n","Batch 400/868, Test Loss: 8.872504258761182e-05\n","Batch 410/868, Test Loss: 0.0006027471390552819\n","Batch 420/868, Test Loss: 0.19973361492156982\n","Batch 430/868, Test Loss: 0.0006111199618317187\n","Batch 440/868, Test Loss: 0.09141077101230621\n","Batch 450/868, Test Loss: 0.004778861068189144\n","Batch 460/868, Test Loss: 0.00033316059852950275\n","Batch 470/868, Test Loss: 4.733871901407838e-05\n","Batch 480/868, Test Loss: 0.0001721500011626631\n","Batch 490/868, Test Loss: 0.11161167174577713\n","Batch 500/868, Test Loss: 0.0013141005765646696\n","Batch 510/868, Test Loss: 6.649010174442083e-05\n","Batch 520/868, Test Loss: 4.505898687057197e-05\n","Batch 530/868, Test Loss: 5.007940490031615e-05\n","Batch 540/868, Test Loss: 4.7465327952522784e-05\n","Batch 550/868, Test Loss: 0.09108012914657593\n","Batch 560/868, Test Loss: 0.29379794001579285\n","Batch 570/868, Test Loss: 0.04733554273843765\n","Batch 580/868, Test Loss: 2.790203143376857e-05\n","Batch 590/868, Test Loss: 0.001824682462029159\n","Batch 600/868, Test Loss: 3.132911660941318e-05\n","Batch 610/868, Test Loss: 6.730452878400683e-05\n","Batch 620/868, Test Loss: 0.1292390525341034\n","Batch 630/868, Test Loss: 0.5922402739524841\n","Batch 640/868, Test Loss: 0.28793397545814514\n","Batch 650/868, Test Loss: 0.003542475402355194\n","Batch 660/868, Test Loss: 0.27275604009628296\n","Batch 670/868, Test Loss: 0.00018313150212634355\n","Batch 680/868, Test Loss: 0.12066487967967987\n","Batch 690/868, Test Loss: 0.0016209343448281288\n","Batch 700/868, Test Loss: 0.36070653796195984\n","Batch 710/868, Test Loss: 4.382988845463842e-05\n","Batch 720/868, Test Loss: 0.0025857172440737486\n","Batch 730/868, Test Loss: 0.003918682225048542\n","Batch 740/868, Test Loss: 9.45992796914652e-05\n","Batch 750/868, Test Loss: 0.5149358510971069\n","Batch 760/868, Test Loss: 4.38373172073625e-05\n","Batch 770/868, Test Loss: 0.00012342345144134015\n","Batch 780/868, Test Loss: 0.0009303870028816164\n","Batch 790/868, Test Loss: 4.5543216401711106e-05\n","Batch 800/868, Test Loss: 6.335581565508619e-05\n","Batch 810/868, Test Loss: 3.6163877666695043e-05\n","Batch 820/868, Test Loss: 0.0009656006586737931\n","Batch 830/868, Test Loss: 0.33669495582580566\n","Batch 840/868, Test Loss: 0.6573506593704224\n","Batch 850/868, Test Loss: 1.3150522708892822\n","Batch 860/868, Test Loss: 0.00014692911645397544\n","Test evaluation completed in: 0m 46s\n","Test loss 0.08431359382342853 accuracy 0.9828530259365995\n","Test Precision: 0.9828072977713924\n","Test Recall: 0.9828530259365994\n","Test F1 Score: 0.9828282087073938\n","Starting epoch 7/10\n","Batch 0/3470, Loss: 5.898281233385205e-05\n","Batch 10/3470, Loss: 0.0004266951000317931\n","Batch 20/3470, Loss: 0.0001918249181471765\n","Batch 30/3470, Loss: 0.0005223919870331883\n","Batch 40/3470, Loss: 0.00011077497765654698\n","Batch 50/3470, Loss: 0.00010449481487739831\n","Batch 60/3470, Loss: 0.00012379953113850206\n","Batch 70/3470, Loss: 0.00019314266683068126\n","Batch 80/3470, Loss: 9.726091957418248e-05\n","Batch 90/3470, Loss: 9.464248432777822e-05\n","Batch 100/3470, Loss: 0.00014652698882855475\n","Batch 110/3470, Loss: 6.92350949975662e-05\n","Batch 120/3470, Loss: 6.678408681182191e-05\n","Batch 130/3470, Loss: 0.0036296977195888758\n","Batch 140/3470, Loss: 0.0010225629666820168\n","Batch 150/3470, Loss: 6.204591772984713e-05\n","Batch 160/3470, Loss: 6.137556192697957e-05\n","Batch 170/3470, Loss: 0.026448732241988182\n","Batch 180/3470, Loss: 0.0001300871226703748\n","Batch 190/3470, Loss: 0.00010396276047686115\n","Batch 200/3470, Loss: 7.4695264629554e-05\n","Batch 210/3470, Loss: 8.968064503278583e-05\n","Batch 220/3470, Loss: 5.241290637059137e-05\n","Batch 230/3470, Loss: 5.554974632104859e-05\n","Batch 240/3470, Loss: 5.313572910381481e-05\n","Batch 250/3470, Loss: 5.602601959253661e-05\n","Batch 260/3470, Loss: 6.842213042546064e-05\n","Batch 270/3470, Loss: 0.0011462544789537787\n","Batch 280/3470, Loss: 5.7315020967507735e-05\n","Batch 290/3470, Loss: 5.713634891435504e-05\n","Batch 300/3470, Loss: 0.00022077091853134334\n","Batch 310/3470, Loss: 5.901325130253099e-05\n","Batch 320/3470, Loss: 4.6765999286435544e-05\n","Batch 330/3470, Loss: 4.5283293729880825e-05\n","Batch 340/3470, Loss: 3.8600694097112864e-05\n","Batch 350/3470, Loss: 6.11588402534835e-05\n","Batch 360/3470, Loss: 0.00023337487073149532\n","Batch 370/3470, Loss: 6.538940942846239e-05\n","Batch 380/3470, Loss: 5.606311606243253e-05\n","Batch 390/3470, Loss: 4.2482151911826804e-05\n","Batch 400/3470, Loss: 0.002056384924799204\n","Batch 410/3470, Loss: 0.0002833720063790679\n","Batch 420/3470, Loss: 6.022759771440178e-05\n","Batch 430/3470, Loss: 5.425253038993105e-05\n","Batch 440/3470, Loss: 4.648247340810485e-05\n","Batch 450/3470, Loss: 4.847918899031356e-05\n","Batch 460/3470, Loss: 5.478952152770944e-05\n","Batch 470/3470, Loss: 3.982974885730073e-05\n","Batch 480/3470, Loss: 3.8362057239282876e-05\n","Batch 490/3470, Loss: 4.847172385780141e-05\n","Batch 500/3470, Loss: 5.5772434279788285e-05\n","Batch 510/3470, Loss: 4.234787047607824e-05\n","Batch 520/3470, Loss: 3.96137656935025e-05\n","Batch 530/3470, Loss: 0.002542163711041212\n","Batch 540/3470, Loss: 8.385984983760864e-05\n","Batch 550/3470, Loss: 3.612699219956994e-05\n","Batch 560/3470, Loss: 0.0041779931634664536\n","Batch 570/3470, Loss: 3.555337025318295e-05\n","Batch 580/3470, Loss: 3.285638740635477e-05\n","Batch 590/3470, Loss: 3.79446501028724e-05\n","Batch 600/3470, Loss: 3.2938278309302405e-05\n","Batch 610/3470, Loss: 3.525536885717884e-05\n","Batch 620/3470, Loss: 3.139615728287026e-05\n","Batch 630/3470, Loss: 2.582336492196191e-05\n","Batch 640/3470, Loss: 3.224540705559775e-05\n","Batch 650/3470, Loss: 2.7604008209891617e-05\n","Batch 660/3470, Loss: 3.066603312618099e-05\n","Batch 670/3470, Loss: 3.826514512184076e-05\n","Batch 680/3470, Loss: 0.001283713849261403\n","Batch 690/3470, Loss: 3.5553272027755156e-05\n","Batch 700/3470, Loss: 2.7589085220824927e-05\n","Batch 710/3470, Loss: 3.321398253319785e-05\n","Batch 720/3470, Loss: 3.099381137872115e-05\n","Batch 730/3470, Loss: 3.328108141431585e-05\n","Batch 740/3470, Loss: 0.0003218082420062274\n","Batch 750/3470, Loss: 3.7899975723121315e-05\n","Batch 760/3470, Loss: 3.624621240305714e-05\n","Batch 770/3470, Loss: 0.00014375730825122446\n","Batch 780/3470, Loss: 5.812185554532334e-05\n","Batch 790/3470, Loss: 3.187292531947605e-05\n","Batch 800/3470, Loss: 3.723695408552885e-05\n","Batch 810/3470, Loss: 4.798708323505707e-05\n","Batch 820/3470, Loss: 4.564738264889456e-05\n","Batch 830/3470, Loss: 2.8237231163075194e-05\n","Batch 840/3470, Loss: 3.5933320759795606e-05\n","Batch 850/3470, Loss: 3.065115015488118e-05\n","Batch 860/3470, Loss: 4.403843195177615e-05\n","Batch 870/3470, Loss: 5.316412352840416e-05\n","Batch 880/3470, Loss: 2.633744225022383e-05\n","Batch 890/3470, Loss: 0.00011593994713621214\n","Batch 900/3470, Loss: 3.4383567253826186e-05\n","Batch 910/3470, Loss: 0.00011017868382623419\n","Batch 920/3470, Loss: 3.0099674404482357e-05\n","Batch 930/3470, Loss: 2.647893961693626e-05\n","Batch 940/3470, Loss: 3.2409298000857234e-05\n","Batch 950/3470, Loss: 0.07808174192905426\n","Batch 960/3470, Loss: 3.162704160786234e-05\n","Batch 970/3470, Loss: 4.505217293626629e-05\n","Batch 980/3470, Loss: 3.18952661473304e-05\n","Batch 990/3470, Loss: 2.9488886866602115e-05\n","Batch 1000/3470, Loss: 2.7186755687580444e-05\n","Batch 1010/3470, Loss: 2.9347249437705614e-05\n","Batch 1020/3470, Loss: 3.3683263609418646e-05\n","Batch 1030/3470, Loss: 3.656659828266129e-05\n","Batch 1040/3470, Loss: 3.921135430573486e-05\n","Batch 1050/3470, Loss: 2.6128831450478174e-05\n","Batch 1060/3470, Loss: 4.1982279071817175e-05\n","Batch 1070/3470, Loss: 0.00505101727321744\n","Batch 1080/3470, Loss: 3.2386942621087655e-05\n","Batch 1090/3470, Loss: 3.300536991446279e-05\n","Batch 1100/3470, Loss: 2.603942084533628e-05\n","Batch 1110/3470, Loss: 3.4867953218054026e-05\n","Batch 1120/3470, Loss: 0.00023637728008907288\n","Batch 1130/3470, Loss: 3.871909939334728e-05\n","Batch 1140/3470, Loss: 9.448220225749537e-05\n","Batch 1150/3470, Loss: 3.6655907024396583e-05\n","Batch 1160/3470, Loss: 2.3022019377094693e-05\n","Batch 1170/3470, Loss: 4.3673160689650103e-05\n","Batch 1180/3470, Loss: 2.5458204618189484e-05\n","Batch 1190/3470, Loss: 4.436512244865298e-05\n","Batch 1200/3470, Loss: 3.1798281270312145e-05\n","Batch 1210/3470, Loss: 2.5242145056836307e-05\n","Batch 1220/3470, Loss: 3.7914986023679376e-05\n","Batch 1230/3470, Loss: 2.3007116396911442e-05\n","Batch 1240/3470, Loss: 0.0002835092891473323\n","Batch 1250/3470, Loss: 9.246833360521123e-05\n","Batch 1260/3470, Loss: 0.00014482950791716576\n","Batch 1270/3470, Loss: 2.8959941118955612e-05\n","Batch 1280/3470, Loss: 0.0006752304616384208\n","Batch 1290/3470, Loss: 3.0330746085382998e-05\n","Batch 1300/3470, Loss: 2.8691591069218703e-05\n","Batch 1310/3470, Loss: 2.6382145733805373e-05\n","Batch 1320/3470, Loss: 2.9898628781666048e-05\n","Batch 1330/3470, Loss: 5.864947524969466e-05\n","Batch 1340/3470, Loss: 2.5085744709940627e-05\n","Batch 1350/3470, Loss: 4.836532025365159e-05\n","Batch 1360/3470, Loss: 8.72035525389947e-05\n","Batch 1370/3470, Loss: 3.2550786272622645e-05\n","Batch 1380/3470, Loss: 2.9637736588483676e-05\n","Batch 1390/3470, Loss: 2.8646983992075548e-05\n","Batch 1400/3470, Loss: 3.1515177397523075e-05\n","Batch 1410/3470, Loss: 2.445245991111733e-05\n","Batch 1420/3470, Loss: 2.1748004655819386e-05\n","Batch 1430/3470, Loss: 2.6501238608034328e-05\n","Batch 1440/3470, Loss: 2.876608050428331e-05\n","Batch 1450/3470, Loss: 2.838622640410904e-05\n","Batch 1460/3470, Loss: 3.1954794394550845e-05\n","Batch 1470/3470, Loss: 2.5838227884378284e-05\n","Batch 1480/3470, Loss: 3.126199226244353e-05\n","Batch 1490/3470, Loss: 3.034560359083116e-05\n","Batch 1500/3470, Loss: 3.927089710487053e-05\n","Batch 1510/3470, Loss: 2.415444396319799e-05\n","Batch 1520/3470, Loss: 2.6195808459306136e-05\n","Batch 1530/3470, Loss: 9.4128969067242e-05\n","Batch 1540/3470, Loss: 4.1565566789358854e-05\n","Batch 1550/3470, Loss: 3.7512672861339524e-05\n","Batch 1560/3470, Loss: 2.909404611273203e-05\n","Batch 1570/3470, Loss: 2.769338243524544e-05\n","Batch 1580/3470, Loss: 2.8840722734457813e-05\n","Batch 1590/3470, Loss: 3.068090882152319e-05\n","Batch 1600/3470, Loss: 2.9898652428528294e-05\n","Batch 1610/3470, Loss: 3.366098098922521e-05\n","Batch 1620/3470, Loss: 3.569490945665166e-05\n","Batch 1630/3470, Loss: 2.9943297704448923e-05\n","Batch 1640/3470, Loss: 3.4301585401408374e-05\n","Batch 1650/3470, Loss: 3.1902745831757784e-05\n","Batch 1660/3470, Loss: 3.928569640265778e-05\n","Batch 1670/3470, Loss: 4.070881550433114e-05\n","Batch 1680/3470, Loss: 3.7296616937965155e-05\n","Batch 1690/3470, Loss: 0.00013315959949977696\n","Batch 1700/3470, Loss: 2.9824119337718002e-05\n","Batch 1710/3470, Loss: 2.4221541025326587e-05\n","Batch 1720/3470, Loss: 2.887043774535414e-05\n","Batch 1730/3470, Loss: 4.9648911954136565e-05\n","Batch 1740/3470, Loss: 0.002682453254237771\n","Batch 1750/3470, Loss: 4.5259450416779146e-05\n","Batch 1760/3470, Loss: 3.398134867893532e-05\n","Batch 1770/3470, Loss: 9.251493611373007e-05\n","Batch 1780/3470, Loss: 3.424211899982765e-05\n","Batch 1790/3470, Loss: 3.7564797821687534e-05\n","Batch 1800/3470, Loss: 0.0015917024575173855\n","Batch 1810/3470, Loss: 0.0012876852415502071\n","Batch 1820/3470, Loss: 9.631556167732924e-05\n","Batch 1830/3470, Loss: 3.355665830895305e-05\n","Batch 1840/3470, Loss: 3.613448279793374e-05\n","Batch 1850/3470, Loss: 4.7764144255779684e-05\n","Batch 1860/3470, Loss: 2.540614514146e-05\n","Batch 1870/3470, Loss: 3.769159593502991e-05\n","Batch 1880/3470, Loss: 2.7834897991851903e-05\n","Batch 1890/3470, Loss: 0.000617052661255002\n","Batch 1900/3470, Loss: 3.7780839193146676e-05\n","Batch 1910/3470, Loss: 8.623045141575858e-05\n","Batch 1920/3470, Loss: 3.2625426683807746e-05\n","Batch 1930/3470, Loss: 3.287126310169697e-05\n","Batch 1940/3470, Loss: 3.254339026170783e-05\n","Batch 1950/3470, Loss: 0.00012202954530948773\n","Batch 1960/3470, Loss: 3.314669447718188e-05\n","Batch 1970/3470, Loss: 3.703593392856419e-05\n","Batch 1980/3470, Loss: 2.3737260562484153e-05\n","Batch 1990/3470, Loss: 2.968251283164136e-05\n","Batch 2000/3470, Loss: 2.3156129827839322e-05\n","Batch 2010/3470, Loss: 3.337786256452091e-05\n","Batch 2020/3470, Loss: 2.908649912569672e-05\n","Batch 2030/3470, Loss: 0.00013649824541062117\n","Batch 2040/3470, Loss: 2.8155242034699768e-05\n","Batch 2050/3470, Loss: 2.821480120474007e-05\n","Batch 2060/3470, Loss: 3.051697240152862e-05\n","Batch 2070/3470, Loss: 3.322142219985835e-05\n","Batch 2080/3470, Loss: 2.9123733838787302e-05\n","Batch 2090/3470, Loss: 2.178523936890997e-05\n","Batch 2100/3470, Loss: 2.375212534388993e-05\n","Batch 2110/3470, Loss: 2.5860548703349195e-05\n","Batch 2120/3470, Loss: 1.9244656868977472e-05\n","Batch 2130/3470, Loss: 2.9511154934880324e-05\n","Batch 2140/3470, Loss: 3.0069877539062873e-05\n","Batch 2150/3470, Loss: 2.7000422051060013e-05\n","Batch 2160/3470, Loss: 5.0199407269246876e-05\n","Batch 2170/3470, Loss: 3.3459764381404966e-05\n","Batch 2180/3470, Loss: 4.3986932723782957e-05\n","Batch 2190/3470, Loss: 0.0001975697960006073\n","Batch 2200/3470, Loss: 0.00011820416693808511\n","Batch 2210/3470, Loss: 0.0004005218215752393\n","Batch 2220/3470, Loss: 3.3728108974173665e-05\n","Batch 2230/3470, Loss: 2.5987257686210796e-05\n","Batch 2240/3470, Loss: 0.00018711788288783282\n","Batch 2250/3470, Loss: 5.572479130933061e-05\n","Batch 2260/3470, Loss: 2.785725155263208e-05\n","Batch 2270/3470, Loss: 2.5398683646926656e-05\n","Batch 2280/3470, Loss: 4.048534538014792e-05\n","Batch 2290/3470, Loss: 4.112612077733502e-05\n","Batch 2300/3470, Loss: 0.00013547568232752383\n","Batch 2310/3470, Loss: 2.4996339561766945e-05\n","Batch 2320/3470, Loss: 0.0003352297062519938\n","Batch 2330/3470, Loss: 2.618835605971981e-05\n","Batch 2340/3470, Loss: 2.7305892217555083e-05\n","Batch 2350/3470, Loss: 2.203111034759786e-05\n","Batch 2360/3470, Loss: 2.219502130174078e-05\n","Batch 2370/3470, Loss: 3.690162338898517e-05\n","Batch 2380/3470, Loss: 2.477278576407116e-05\n","Batch 2390/3470, Loss: 3.0725532269570976e-05\n","Batch 2400/3470, Loss: 2.8073269277228974e-05\n","Batch 2410/3470, Loss: 2.902681808336638e-05\n","Batch 2420/3470, Loss: 2.6709900339483283e-05\n","Batch 2430/3470, Loss: 2.9593134968308732e-05\n","Batch 2440/3470, Loss: 2.7611356927081943e-05\n","Batch 2450/3470, Loss: 2.930255504907109e-05\n","Batch 2460/3470, Loss: 3.392921280465089e-05\n","Batch 2470/3470, Loss: 0.0004115650081075728\n","Batch 2480/3470, Loss: 4.282207373762503e-05\n","Batch 2490/3470, Loss: 2.393838803982362e-05\n","Batch 2500/3470, Loss: 3.108317469013855e-05\n","Batch 2510/3470, Loss: 9.747318108566105e-05\n","Batch 2520/3470, Loss: 5.4992735385894775e-05\n","Batch 2530/3470, Loss: 0.00011116827226942405\n","Batch 2540/3470, Loss: 9.172008867608383e-05\n","Batch 2550/3470, Loss: 0.00015581047045998275\n","Batch 2560/3470, Loss: 7.686328171985224e-05\n","Batch 2570/3470, Loss: 8.648389484733343e-05\n","Batch 2580/3470, Loss: 0.00010407696390757337\n","Batch 2590/3470, Loss: 0.0001075923428288661\n","Batch 2600/3470, Loss: 0.0001398518797941506\n","Batch 2610/3470, Loss: 4.2457759263925254e-05\n","Batch 2620/3470, Loss: 0.0022297012619674206\n","Batch 2630/3470, Loss: 9.231584408553317e-05\n","Batch 2640/3470, Loss: 8.25438037281856e-05\n","Batch 2650/3470, Loss: 0.0001036547328112647\n","Batch 2660/3470, Loss: 7.92642094893381e-05\n","Batch 2670/3470, Loss: 4.6085944632068276e-05\n","Batch 2680/3470, Loss: 0.00012067644274793565\n","Batch 2690/3470, Loss: 5.694079300155863e-05\n","Batch 2700/3470, Loss: 0.00011959891708102077\n","Batch 2710/3470, Loss: 2.8669292078120634e-05\n","Batch 2720/3470, Loss: 5.272341149975546e-05\n","Batch 2730/3470, Loss: 4.5237258746055886e-05\n","Batch 2740/3470, Loss: 5.7097287935903296e-05\n","Batch 2750/3470, Loss: 5.749931369791739e-05\n","Batch 2760/3470, Loss: 7.08780498825945e-05\n","Batch 2770/3470, Loss: 0.0010548126883804798\n","Batch 2780/3470, Loss: 5.1718841859837994e-05\n","Batch 2790/3470, Loss: 7.79338224674575e-05\n","Batch 2800/3470, Loss: 3.491923416731879e-05\n","Batch 2810/3470, Loss: 4.497649206314236e-05\n","Batch 2820/3470, Loss: 5.221763058216311e-05\n","Batch 2830/3470, Loss: 2.5324121452285908e-05\n","Batch 2840/3470, Loss: 0.12698403000831604\n","Batch 2850/3470, Loss: 5.243357009021565e-05\n","Batch 2860/3470, Loss: 5.7490684412186965e-05\n","Batch 2870/3470, Loss: 0.00016769798821769655\n","Batch 2880/3470, Loss: 8.170289947884157e-05\n","Batch 2890/3470, Loss: 4.272694422979839e-05\n","Batch 2900/3470, Loss: 2.8609611035790294e-05\n","Batch 2910/3470, Loss: 3.533684866852127e-05\n","Batch 2920/3470, Loss: 0.018744323402643204\n","Batch 2930/3470, Loss: 7.187913433881477e-05\n","Batch 2940/3470, Loss: 0.0001335127599304542\n","Batch 2950/3470, Loss: 7.714579260209575e-05\n","Batch 2960/3470, Loss: 7.198358798632398e-05\n","Batch 2970/3470, Loss: 0.00031992801814340055\n","Batch 2980/3470, Loss: 8.134655945468694e-05\n","Batch 2990/3470, Loss: 0.006576757412403822\n","Batch 3000/3470, Loss: 4.662432547775097e-05\n","Batch 3010/3470, Loss: 0.004155867733061314\n","Batch 3020/3470, Loss: 8.110344788292423e-05\n","Batch 3030/3470, Loss: 6.035378100932576e-05\n","Batch 3040/3470, Loss: 0.0001201317209051922\n","Batch 3050/3470, Loss: 0.00021434706286527216\n","Batch 3060/3470, Loss: 6.0324349760776386e-05\n","Batch 3070/3470, Loss: 3.570214903447777e-05\n","Batch 3080/3470, Loss: 6.868273339932784e-05\n","Batch 3090/3470, Loss: 5.881853576283902e-05\n","Batch 3100/3470, Loss: 0.001474953256547451\n","Batch 3110/3470, Loss: 3.954604471800849e-05\n","Batch 3120/3470, Loss: 3.47335881087929e-05\n","Batch 3130/3470, Loss: 0.00022198379156179726\n","Batch 3140/3470, Loss: 4.1587205487303436e-05\n","Batch 3150/3470, Loss: 6.252889579627663e-05\n","Batch 3160/3470, Loss: 3.430167635087855e-05\n","Batch 3170/3470, Loss: 0.00018586471560411155\n","Batch 3180/3470, Loss: 4.01272154704202e-05\n","Batch 3190/3470, Loss: 3.593296787585132e-05\n","Batch 3200/3470, Loss: 0.0035143541172146797\n","Batch 3210/3470, Loss: 3.978445238317363e-05\n","Batch 3220/3470, Loss: 0.0016347606433555484\n","Batch 3230/3470, Loss: 4.1483068343950436e-05\n","Batch 3240/3470, Loss: 4.189299943391234e-05\n","Batch 3250/3470, Loss: 3.8928104913793504e-05\n","Batch 3260/3470, Loss: 3.851829387713224e-05\n","Batch 3270/3470, Loss: 0.00016179736121557653\n","Batch 3280/3470, Loss: 2.916837729571853e-05\n","Batch 3290/3470, Loss: 0.0023562258575111628\n","Batch 3300/3470, Loss: 0.00010629803000483662\n","Batch 3310/3470, Loss: 0.03698545694351196\n","Batch 3320/3470, Loss: 0.0001119957523769699\n","Batch 3330/3470, Loss: 0.00013915769523009658\n","Batch 3340/3470, Loss: 2.4780298190307803e-05\n","Batch 3350/3470, Loss: 4.4373675336828455e-05\n","Batch 3360/3470, Loss: 7.803486369084567e-05\n","Batch 3370/3470, Loss: 8.312563295476139e-05\n","Batch 3380/3470, Loss: 3.556792580639012e-05\n","Batch 3390/3470, Loss: 3.376496897544712e-05\n","Batch 3400/3470, Loss: 4.837444794247858e-05\n","Batch 3410/3470, Loss: 2.9280099624884315e-05\n","Batch 3420/3470, Loss: 3.781039413297549e-05\n","Batch 3430/3470, Loss: 2.521241367503535e-05\n","Batch 3440/3470, Loss: 2.3327480448642746e-05\n","Batch 3450/3470, Loss: 3.703578113345429e-05\n","Batch 3460/3470, Loss: 4.963331593899056e-05\n","Training epoch completed in: 9m 37s\n","Train loss 0.0015959231571934483 accuracy 0.9996037249851397\n","Batch 0/868, Test Loss: 2.8341302822809666e-05\n","Batch 10/868, Test Loss: 2.3766895537846722e-05\n","Batch 20/868, Test Loss: 2.86467547994107e-05\n","Batch 30/868, Test Loss: 0.0003450223011896014\n","Batch 40/868, Test Loss: 1.908820377138909e-05\n","Batch 50/868, Test Loss: 1.95650336536346e-05\n","Batch 60/868, Test Loss: 0.12789644300937653\n","Batch 70/868, Test Loss: 1.9930099369958043e-05\n","Batch 80/868, Test Loss: 2.820721419993788e-05\n","Batch 90/868, Test Loss: 6.14622695138678e-05\n","Batch 100/868, Test Loss: 2.8184860639157705e-05\n","Batch 110/868, Test Loss: 0.4252766966819763\n","Batch 120/868, Test Loss: 3.413758167880587e-05\n","Batch 130/868, Test Loss: 2.3841403162805364e-05\n","Batch 140/868, Test Loss: 8.862368122208863e-05\n","Batch 150/868, Test Loss: 0.5851839780807495\n","Batch 160/868, Test Loss: 3.3571323001524433e-05\n","Batch 170/868, Test Loss: 0.6772593259811401\n","Batch 180/868, Test Loss: 0.0231412872672081\n","Batch 190/868, Test Loss: 1.8998802261194214e-05\n","Batch 200/868, Test Loss: 2.1896961698075756e-05\n","Batch 210/868, Test Loss: 0.00010297339031239972\n","Batch 220/868, Test Loss: 2.854990634659771e-05\n","Batch 230/868, Test Loss: 0.0366777665913105\n","Batch 240/868, Test Loss: 3.3094504033215344e-05\n","Batch 250/868, Test Loss: 0.00024071747611742467\n","Batch 260/868, Test Loss: 0.7497190833091736\n","Batch 270/868, Test Loss: 0.6778014302253723\n","Batch 280/868, Test Loss: 0.007286036852747202\n","Batch 290/868, Test Loss: 0.00033438284299336374\n","Batch 300/868, Test Loss: 0.13912008702754974\n","Batch 310/868, Test Loss: 0.05583600699901581\n","Batch 320/868, Test Loss: 4.979782170266844e-05\n","Batch 330/868, Test Loss: 0.5534186363220215\n","Batch 340/868, Test Loss: 0.0001171594558400102\n","Batch 350/868, Test Loss: 7.903166260803118e-05\n","Batch 360/868, Test Loss: 0.005987075157463551\n","Batch 370/868, Test Loss: 0.18501128256320953\n","Batch 380/868, Test Loss: 2.425863203825429e-05\n","Batch 390/868, Test Loss: 2.910867624450475e-05\n","Batch 400/868, Test Loss: 2.849030715879053e-05\n","Batch 410/868, Test Loss: 5.555228199227713e-05\n","Batch 420/868, Test Loss: 0.44842392206192017\n","Batch 430/868, Test Loss: 4.90080528834369e-05\n","Batch 440/868, Test Loss: 0.0057254270650446415\n","Batch 450/868, Test Loss: 3.450127042015083e-05\n","Batch 460/868, Test Loss: 1.9579932995839044e-05\n","Batch 470/868, Test Loss: 2.906397457991261e-05\n","Batch 480/868, Test Loss: 2.3915916244732216e-05\n","Batch 490/868, Test Loss: 0.20909680426120758\n","Batch 500/868, Test Loss: 0.004574360325932503\n","Batch 510/868, Test Loss: 3.2438903872389346e-05\n","Batch 520/868, Test Loss: 2.8467949960031547e-05\n","Batch 530/868, Test Loss: 4.6724842832190916e-05\n","Batch 540/868, Test Loss: 2.7618661988526583e-05\n","Batch 550/868, Test Loss: 0.015489978715777397\n","Batch 560/868, Test Loss: 0.5028740167617798\n","Batch 570/868, Test Loss: 2.885537287511397e-05\n","Batch 580/868, Test Loss: 1.899135168059729e-05\n","Batch 590/868, Test Loss: 0.0030665327794849873\n","Batch 600/868, Test Loss: 1.9155260815750808e-05\n","Batch 610/868, Test Loss: 3.788496906054206e-05\n","Batch 620/868, Test Loss: 0.5850542783737183\n","Batch 630/868, Test Loss: 0.6169440746307373\n","Batch 640/868, Test Loss: 0.5598145723342896\n","Batch 650/868, Test Loss: 8.170150249497965e-05\n","Batch 660/868, Test Loss: 0.31260234117507935\n","Batch 670/868, Test Loss: 0.0001569548185216263\n","Batch 680/868, Test Loss: 8.601934678154066e-05\n","Batch 690/868, Test Loss: 0.0005374797037802637\n","Batch 700/868, Test Loss: 0.6559963822364807\n","Batch 710/868, Test Loss: 2.806566044455394e-05\n","Batch 720/868, Test Loss: 0.00021168947569094598\n","Batch 730/868, Test Loss: 0.0001183519052574411\n","Batch 740/868, Test Loss: 3.242401726311073e-05\n","Batch 750/868, Test Loss: 0.6740155816078186\n","Batch 760/868, Test Loss: 2.8318961994955316e-05\n","Batch 770/868, Test Loss: 3.5538025258574635e-05\n","Batch 780/868, Test Loss: 0.0004053486918564886\n","Batch 790/868, Test Loss: 2.8721258786390536e-05\n","Batch 800/868, Test Loss: 3.823511360678822e-05\n","Batch 810/868, Test Loss: 2.3722199330222793e-05\n","Batch 820/868, Test Loss: 0.0005484563880600035\n","Batch 830/868, Test Loss: 0.0031421545427292585\n","Batch 840/868, Test Loss: 0.6804655194282532\n","Batch 850/868, Test Loss: 1.3543490171432495\n","Batch 860/868, Test Loss: 0.00038452394073829055\n","Test evaluation completed in: 0m 46s\n","Test loss 0.10527754893759735 accuracy 0.9828530259365995\n","Test Precision: 0.9827690402348115\n","Test Recall: 0.9828530259365994\n","Test F1 Score: 0.9828031766189298\n","Starting epoch 8/10\n","Batch 0/3470, Loss: 4.0268820157507434e-05\n","Batch 10/3470, Loss: 0.0001078739805961959\n","Batch 20/3470, Loss: 9.755975770531222e-05\n","Batch 30/3470, Loss: 0.00014186857151798904\n","Batch 40/3470, Loss: 0.0001034976594382897\n","Batch 50/3470, Loss: 8.460460230708122e-05\n","Batch 60/3470, Loss: 9.84316211543046e-05\n","Batch 70/3470, Loss: 9.452050289837644e-05\n","Batch 80/3470, Loss: 9.635208698455244e-05\n","Batch 90/3470, Loss: 0.0002808828721754253\n","Batch 100/3470, Loss: 0.00011072813504142687\n","Batch 110/3470, Loss: 5.663019692292437e-05\n","Batch 120/3470, Loss: 6.017621126375161e-05\n","Batch 130/3470, Loss: 8.280156907858327e-05\n","Batch 140/3470, Loss: 6.451193621614948e-05\n","Batch 150/3470, Loss: 5.1787552365567535e-05\n","Batch 160/3470, Loss: 5.384354153648019e-05\n","Batch 170/3470, Loss: 8.846684795571491e-05\n","Batch 180/3470, Loss: 7.316863775486127e-05\n","Batch 190/3470, Loss: 7.972306048031896e-05\n","Batch 200/3470, Loss: 6.203853990882635e-05\n","Batch 210/3470, Loss: 6.750466127414256e-05\n","Batch 220/3470, Loss: 4.871800047112629e-05\n","Batch 230/3470, Loss: 4.844226350542158e-05\n","Batch 240/3470, Loss: 5.3277261031325907e-05\n","Batch 250/3470, Loss: 4.642334533855319e-05\n","Batch 260/3470, Loss: 9.117530862567946e-05\n","Batch 270/3470, Loss: 5.856672942172736e-05\n","Batch 280/3470, Loss: 5.4752399591961876e-05\n","Batch 290/3470, Loss: 5.740464257542044e-05\n","Batch 300/3470, Loss: 0.00010010720870923251\n","Batch 310/3470, Loss: 5.241310645942576e-05\n","Batch 320/3470, Loss: 5.2271414460847154e-05\n","Batch 330/3470, Loss: 0.0001409174728905782\n","Batch 340/3470, Loss: 4.061970685143024e-05\n","Batch 350/3470, Loss: 6.0235637647565454e-05\n","Batch 360/3470, Loss: 8.559241541661322e-05\n","Batch 370/3470, Loss: 0.0013673350913450122\n","Batch 380/3470, Loss: 4.688498665927909e-05\n","Batch 390/3470, Loss: 3.9651047700317577e-05\n","Batch 400/3470, Loss: 4.28323182859458e-05\n","Batch 410/3470, Loss: 4.5096920075593516e-05\n","Batch 420/3470, Loss: 4.593889752868563e-05\n","Batch 430/3470, Loss: 5.185411646380089e-05\n","Batch 440/3470, Loss: 4.502991941990331e-05\n","Batch 450/3470, Loss: 4.493310916586779e-05\n","Batch 460/3470, Loss: 8.977967081591487e-05\n","Batch 470/3470, Loss: 3.833237860817462e-05\n","Batch 480/3470, Loss: 0.00010273147199768573\n","Batch 490/3470, Loss: 4.9023263272829354e-05\n","Batch 500/3470, Loss: 0.00012872290972154588\n","Batch 510/3470, Loss: 4.4299820729065686e-05\n","Batch 520/3470, Loss: 4.2675615986809134e-05\n","Batch 530/3470, Loss: 0.07719606906175613\n","Batch 540/3470, Loss: 4.681801874539815e-05\n","Batch 550/3470, Loss: 0.0001919353671837598\n","Batch 560/3470, Loss: 0.0001850796543294564\n","Batch 570/3470, Loss: 3.52703282260336e-05\n","Batch 580/3470, Loss: 3.978506720159203e-05\n","Batch 590/3470, Loss: 0.0001937672059284523\n","Batch 600/3470, Loss: 4.456055830814876e-05\n","Batch 610/3470, Loss: 4.865034861722961e-05\n","Batch 620/3470, Loss: 3.520330210449174e-05\n","Batch 630/3470, Loss: 3.208166526746936e-05\n","Batch 640/3470, Loss: 3.5732191463466734e-05\n","Batch 650/3470, Loss: 3.413793456275016e-05\n","Batch 660/3470, Loss: 0.0268063023686409\n","Batch 670/3470, Loss: 4.2213621782138944e-05\n","Batch 680/3470, Loss: 3.8510854210471734e-05\n","Batch 690/3470, Loss: 4.567793075693771e-05\n","Batch 700/3470, Loss: 3.786285742535256e-05\n","Batch 710/3470, Loss: 4.14834103139583e-05\n","Batch 720/3470, Loss: 3.746807487914339e-05\n","Batch 730/3470, Loss: 3.4570028219604865e-05\n","Batch 740/3470, Loss: 3.912193642463535e-05\n","Batch 750/3470, Loss: 3.8697337004123256e-05\n","Batch 760/3470, Loss: 6.279415538301691e-05\n","Batch 770/3470, Loss: 3.7996946048224345e-05\n","Batch 780/3470, Loss: 3.9643364289077e-05\n","Batch 790/3470, Loss: 3.483814361970872e-05\n","Batch 800/3470, Loss: 4.135704512009397e-05\n","Batch 810/3470, Loss: 4.6914781705709174e-05\n","Batch 820/3470, Loss: 4.550670928438194e-05\n","Batch 830/3470, Loss: 3.584397927625105e-05\n","Batch 840/3470, Loss: 3.968077362515032e-05\n","Batch 850/3470, Loss: 3.7535075534833595e-05\n","Batch 860/3470, Loss: 0.09475710242986679\n","Batch 870/3470, Loss: 5.325465099303983e-05\n","Batch 880/3470, Loss: 3.3646199881331995e-05\n","Batch 890/3470, Loss: 4.226570308674127e-05\n","Batch 900/3470, Loss: 4.7011475544422865e-05\n","Batch 910/3470, Loss: 5.123573282617144e-05\n","Batch 920/3470, Loss: 4.386748332763091e-05\n","Batch 930/3470, Loss: 4.892572542303242e-05\n","Batch 940/3470, Loss: 4.085036562173627e-05\n","Batch 950/3470, Loss: 5.1635994168464094e-05\n","Batch 960/3470, Loss: 3.895054032909684e-05\n","Batch 970/3470, Loss: 5.3135459893383086e-05\n","Batch 980/3470, Loss: 3.560541881597601e-05\n","Batch 990/3470, Loss: 3.487548747216351e-05\n","Batch 1000/3470, Loss: 3.282667603343725e-05\n","Batch 1010/3470, Loss: 3.602269134717062e-05\n","Batch 1020/3470, Loss: 0.002494130050763488\n","Batch 1030/3470, Loss: 4.7696994442958385e-05\n","Batch 1040/3470, Loss: 3.825027306447737e-05\n","Batch 1050/3470, Loss: 2.809573561535217e-05\n","Batch 1060/3470, Loss: 3.7624377000611275e-05\n","Batch 1070/3470, Loss: 0.0005245253560133278\n","Batch 1080/3470, Loss: 3.278926305938512e-05\n","Batch 1090/3470, Loss: 4.11333967349492e-05\n","Batch 1100/3470, Loss: 2.8065922379028052e-05\n","Batch 1110/3470, Loss: 3.9330654544755816e-05\n","Batch 1120/3470, Loss: 3.591082713683136e-05\n","Batch 1130/3470, Loss: 0.0005433080368675292\n","Batch 1140/3470, Loss: 0.0005513548385351896\n","Batch 1150/3470, Loss: 4.528304634732194e-05\n","Batch 1160/3470, Loss: 2.9898714274168015e-05\n","Batch 1170/3470, Loss: 4.113353861612268e-05\n","Batch 1180/3470, Loss: 3.3243784855585545e-05\n","Batch 1190/3470, Loss: 0.0002541388967074454\n","Batch 1200/3470, Loss: 2.723893521761056e-05\n","Batch 1210/3470, Loss: 2.9421824365272187e-05\n","Batch 1220/3470, Loss: 4.198257374810055e-05\n","Batch 1230/3470, Loss: 2.5793568056542426e-05\n","Batch 1240/3470, Loss: 3.3281001378782094e-05\n","Batch 1250/3470, Loss: 3.454749094089493e-05\n","Batch 1260/3470, Loss: 3.130668483208865e-05\n","Batch 1270/3470, Loss: 2.749957457126584e-05\n","Batch 1280/3470, Loss: 0.00010889515397138894\n","Batch 1290/3470, Loss: 3.208893758710474e-05\n","Batch 1300/3470, Loss: 2.890031464630738e-05\n","Batch 1310/3470, Loss: 2.542849506426137e-05\n","Batch 1320/3470, Loss: 3.77362230210565e-05\n","Batch 1330/3470, Loss: 0.00011380738578736782\n","Batch 1340/3470, Loss: 2.7224003133596852e-05\n","Batch 1350/3470, Loss: 6.334267527563497e-05\n","Batch 1360/3470, Loss: 9.480168228037655e-05\n","Batch 1370/3470, Loss: 3.898725481121801e-05\n","Batch 1380/3470, Loss: 2.985384526255075e-05\n","Batch 1390/3470, Loss: 3.437608393142e-05\n","Batch 1400/3470, Loss: 3.160463529638946e-05\n","Batch 1410/3470, Loss: 2.9660250220331363e-05\n","Batch 1420/3470, Loss: 2.2992217054706998e-05\n","Batch 1430/3470, Loss: 3.161963832098991e-05\n","Batch 1440/3470, Loss: 0.000142726072226651\n","Batch 1450/3470, Loss: 2.6136229280382395e-05\n","Batch 1460/3470, Loss: 3.073301922995597e-05\n","Batch 1470/3470, Loss: 2.966767169709783e-05\n","Batch 1480/3470, Loss: 3.574707807274535e-05\n","Batch 1490/3470, Loss: 4.092429662705399e-05\n","Batch 1500/3470, Loss: 6.940479943295941e-05\n","Batch 1510/3470, Loss: 3.062127507291734e-05\n","Batch 1520/3470, Loss: 2.5800989533308893e-05\n","Batch 1530/3470, Loss: 7.822990301065147e-05\n","Batch 1540/3470, Loss: 4.397935845190659e-05\n","Batch 1550/3470, Loss: 3.876436312566511e-05\n","Batch 1560/3470, Loss: 2.9988015739945695e-05\n","Batch 1570/3470, Loss: 2.9377106329775415e-05\n","Batch 1580/3470, Loss: 3.0300925573101267e-05\n","Batch 1590/3470, Loss: 3.7743749999208376e-05\n","Batch 1600/3470, Loss: 2.7156957003171556e-05\n","Batch 1610/3470, Loss: 4.6326145820785314e-05\n","Batch 1620/3470, Loss: 3.9054782973835245e-05\n","Batch 1630/3470, Loss: 3.3102183806477115e-05\n","Batch 1640/3470, Loss: 3.531482070684433e-05\n","Batch 1650/3470, Loss: 3.222304439987056e-05\n","Batch 1660/3470, Loss: 4.2928822949761525e-05\n","Batch 1670/3470, Loss: 3.959118475904688e-05\n","Batch 1680/3470, Loss: 3.539686440490186e-05\n","Batch 1690/3470, Loss: 4.913426164421253e-05\n","Batch 1700/3470, Loss: 3.2014548196457326e-05\n","Batch 1710/3470, Loss: 2.5219884264515713e-05\n","Batch 1720/3470, Loss: 2.7820013201562688e-05\n","Batch 1730/3470, Loss: 4.588672527461313e-05\n","Batch 1740/3470, Loss: 3.615677997004241e-05\n","Batch 1750/3470, Loss: 3.7274236092343926e-05\n","Batch 1760/3470, Loss: 3.194733653799631e-05\n","Batch 1770/3470, Loss: 2.9913540856796317e-05\n","Batch 1780/3470, Loss: 3.351189661771059e-05\n","Batch 1790/3470, Loss: 3.779569669859484e-05\n","Batch 1800/3470, Loss: 0.00504159415140748\n","Batch 1810/3470, Loss: 4.061202344018966e-05\n","Batch 1820/3470, Loss: 2.8661870601354167e-05\n","Batch 1830/3470, Loss: 2.8967315301997587e-05\n","Batch 1840/3470, Loss: 3.4473057894501835e-05\n","Batch 1850/3470, Loss: 0.00011298889148747548\n","Batch 1860/3470, Loss: 2.5033619749592617e-05\n","Batch 1870/3470, Loss: 3.9025035221129656e-05\n","Batch 1880/3470, Loss: 2.9339837055886164e-05\n","Batch 1890/3470, Loss: 3.8950533053139225e-05\n","Batch 1900/3470, Loss: 3.7825473555130884e-05\n","Batch 1910/3470, Loss: 3.085211574216373e-05\n","Batch 1920/3470, Loss: 3.5486198612488806e-05\n","Batch 1930/3470, Loss: 2.959312951134052e-05\n","Batch 1940/3470, Loss: 3.061373718082905e-05\n","Batch 1950/3470, Loss: 2.8870455935248174e-05\n","Batch 1960/3470, Loss: 3.0844694265397266e-05\n","Batch 1970/3470, Loss: 0.00011950219777645543\n","Batch 1980/3470, Loss: 2.241108268208336e-05\n","Batch 1990/3470, Loss: 3.1276806112146005e-05\n","Batch 2000/3470, Loss: 2.2381283997674473e-05\n","Batch 2010/3470, Loss: 3.8265076000243425e-05\n","Batch 2020/3470, Loss: 7.048399129416794e-05\n","Batch 2030/3470, Loss: 5.056316877016798e-05\n","Batch 2040/3470, Loss: 4.432871355675161e-05\n","Batch 2050/3470, Loss: 0.044891003519296646\n","Batch 2060/3470, Loss: 4.0656559576746076e-05\n","Batch 2070/3470, Loss: 4.392715709400363e-05\n","Batch 2080/3470, Loss: 3.2118750823428854e-05\n","Batch 2090/3470, Loss: 2.764118653431069e-05\n","Batch 2100/3470, Loss: 2.8728800316457637e-05\n","Batch 2110/3470, Loss: 3.319895768072456e-05\n","Batch 2120/3470, Loss: 2.268675234518014e-05\n","Batch 2130/3470, Loss: 2.8862976250820793e-05\n","Batch 2140/3470, Loss: 3.131401172140613e-05\n","Batch 2150/3470, Loss: 2.916844459832646e-05\n","Batch 2160/3470, Loss: 4.6995926823001355e-05\n","Batch 2170/3470, Loss: 2.682159174582921e-05\n","Batch 2180/3470, Loss: 3.8607911847066134e-05\n","Batch 2190/3470, Loss: 0.00033650232944637537\n","Batch 2200/3470, Loss: 5.47911440662574e-05\n","Batch 2210/3470, Loss: 2.5897836167132482e-05\n","Batch 2220/3470, Loss: 2.8863027182524092e-05\n","Batch 2230/3470, Loss: 2.4415197913185693e-05\n","Batch 2240/3470, Loss: 3.0338063879753463e-05\n","Batch 2250/3470, Loss: 2.8959844712517224e-05\n","Batch 2260/3470, Loss: 2.4027789550018497e-05\n","Batch 2270/3470, Loss: 2.383407627348788e-05\n","Batch 2280/3470, Loss: 4.765956327901222e-05\n","Batch 2290/3470, Loss: 0.0001184265420306474\n","Batch 2300/3470, Loss: 7.042050128802657e-05\n","Batch 2310/3470, Loss: 4.364223059383221e-05\n","Batch 2320/3470, Loss: 0.00014637300046160817\n","Batch 2330/3470, Loss: 4.482629810809158e-05\n","Batch 2340/3470, Loss: 5.3615192882716656e-05\n","Batch 2350/3470, Loss: 2.453445085848216e-05\n","Batch 2360/3470, Loss: 2.3819213311071508e-05\n","Batch 2370/3470, Loss: 0.00012421552673913538\n","Batch 2380/3470, Loss: 3.9493523217970505e-05\n","Batch 2390/3470, Loss: 8.717862510820851e-05\n","Batch 2400/3470, Loss: 8.455781062366441e-05\n","Batch 2410/3470, Loss: 7.165349234128371e-05\n","Batch 2420/3470, Loss: 4.602003173204139e-05\n","Batch 2430/3470, Loss: 5.8572339185047895e-05\n","Batch 2440/3470, Loss: 5.779025377705693e-05\n","Batch 2450/3470, Loss: 5.953295112703927e-05\n","Batch 2460/3470, Loss: 0.00011558132973732427\n","Batch 2470/3470, Loss: 0.00012708488793578\n","Batch 2480/3470, Loss: 3.737023871508427e-05\n","Batch 2490/3470, Loss: 5.025067730457522e-05\n","Batch 2500/3470, Loss: 7.107968849595636e-05\n","Batch 2510/3470, Loss: 0.0003422728623263538\n","Batch 2520/3470, Loss: 0.00013735123502556235\n","Batch 2530/3470, Loss: 0.00021876662503927946\n","Batch 2540/3470, Loss: 0.00010403228952782229\n","Batch 2550/3470, Loss: 0.00022107850236352533\n","Batch 2560/3470, Loss: 0.00015385920414701104\n","Batch 2570/3470, Loss: 0.00012569435057230294\n","Batch 2580/3470, Loss: 0.00020686264906544238\n","Batch 2590/3470, Loss: 0.00022337061818689108\n","Batch 2600/3470, Loss: 0.00019380712183192372\n","Batch 2610/3470, Loss: 6.098687299527228e-05\n","Batch 2620/3470, Loss: 0.00022586097475141287\n","Batch 2630/3470, Loss: 0.00013392706750892103\n","Batch 2640/3470, Loss: 0.00016823499754536897\n","Batch 2650/3470, Loss: 0.00019230227917432785\n","Batch 2660/3470, Loss: 0.00010369342635385692\n","Batch 2670/3470, Loss: 0.0004405810032039881\n","Batch 2680/3470, Loss: 8.426657586824149e-05\n","Batch 2690/3470, Loss: 0.0001240157725987956\n","Batch 2700/3470, Loss: 0.0002880571992136538\n","Batch 2710/3470, Loss: 2.5733963411767036e-05\n","Batch 2720/3470, Loss: 7.135947817005217e-05\n","Batch 2730/3470, Loss: 7.999881927389652e-05\n","Batch 2740/3470, Loss: 8.815016190055758e-05\n","Batch 2750/3470, Loss: 9.63716083788313e-05\n","Batch 2760/3470, Loss: 0.00012646226969081908\n","Batch 2770/3470, Loss: 6.781479896744713e-05\n","Batch 2780/3470, Loss: 9.533694537822157e-05\n","Batch 2790/3470, Loss: 0.00015226028335746378\n","Batch 2800/3470, Loss: 4.826737495022826e-05\n","Batch 2810/3470, Loss: 6.047806527931243e-05\n","Batch 2820/3470, Loss: 0.00012568924285005778\n","Batch 2830/3470, Loss: 0.00147205067332834\n","Batch 2840/3470, Loss: 9.430423961021006e-05\n","Batch 2850/3470, Loss: 8.536640234524384e-05\n","Batch 2860/3470, Loss: 8.99315855349414e-05\n","Batch 2870/3470, Loss: 0.00024061476869974285\n","Batch 2880/3470, Loss: 0.0001224848529091105\n","Batch 2890/3470, Loss: 7.295397517737001e-05\n","Batch 2900/3470, Loss: 5.5286123824771494e-05\n","Batch 2910/3470, Loss: 4.8910016630543396e-05\n","Batch 2920/3470, Loss: 8.813713066047058e-05\n","Batch 2930/3470, Loss: 4.651104609365575e-05\n","Batch 2940/3470, Loss: 8.023944246815518e-05\n","Batch 2950/3470, Loss: 8.25286770123057e-05\n","Batch 2960/3470, Loss: 7.344091864069924e-05\n","Batch 2970/3470, Loss: 0.00026885548140853643\n","Batch 2980/3470, Loss: 0.00010024184302892536\n","Batch 2990/3470, Loss: 0.00024447898613289\n","Batch 3000/3470, Loss: 3.246163760195486e-05\n","Batch 3010/3470, Loss: 0.00013058236800134182\n","Batch 3020/3470, Loss: 0.0001142361288657412\n","Batch 3030/3470, Loss: 7.90940539445728e-05\n","Batch 3040/3470, Loss: 6.94755362928845e-05\n","Batch 3050/3470, Loss: 5.9413920098450035e-05\n","Batch 3060/3470, Loss: 0.00011335869203321636\n","Batch 3070/3470, Loss: 5.0309339712839574e-05\n","Batch 3080/3470, Loss: 0.00011856057972181588\n","Batch 3090/3470, Loss: 0.010910597629845142\n","Batch 3100/3470, Loss: 9.028305794345215e-05\n","Batch 3110/3470, Loss: 4.4722739403368905e-05\n","Batch 3120/3470, Loss: 4.967689892509952e-05\n","Batch 3130/3470, Loss: 9.448660421185195e-05\n","Batch 3140/3470, Loss: 6.231032602954656e-05\n","Batch 3150/3470, Loss: 8.346085814991966e-05\n","Batch 3160/3470, Loss: 4.38356728409417e-05\n","Batch 3170/3470, Loss: 6.842023867648095e-05\n","Batch 3180/3470, Loss: 5.927896563662216e-05\n","Batch 3190/3470, Loss: 3.991823177784681e-05\n","Batch 3200/3470, Loss: 7.0505979238078e-05\n","Batch 3210/3470, Loss: 5.0548886065371335e-05\n","Batch 3220/3470, Loss: 2.5979794372688048e-05\n","Batch 3230/3470, Loss: 5.0332968385191634e-05\n","Batch 3240/3470, Loss: 6.143290374893695e-05\n","Batch 3250/3470, Loss: 4.8180168960243464e-05\n","Batch 3260/3470, Loss: 0.00011015548079740256\n","Batch 3270/3470, Loss: 8.849702135194093e-05\n","Batch 3280/3470, Loss: 4.400100442580879e-05\n","Batch 3290/3470, Loss: 3.5150682379025966e-05\n","Batch 3300/3470, Loss: 4.846304364036769e-05\n","Batch 3310/3470, Loss: 6.059178849682212e-05\n","Batch 3320/3470, Loss: 0.00016152957687154412\n","Batch 3330/3470, Loss: 0.00022567516134586185\n","Batch 3340/3470, Loss: 2.326043431821745e-05\n","Batch 3350/3470, Loss: 4.359867671155371e-05\n","Batch 3360/3470, Loss: 0.0001276557013625279\n","Batch 3370/3470, Loss: 3.141841079923324e-05\n","Batch 3380/3470, Loss: 3.178334009135142e-05\n","Batch 3390/3470, Loss: 9.896393748931587e-05\n","Batch 3400/3470, Loss: 6.163456419017166e-05\n","Batch 3410/3470, Loss: 3.3802036341512576e-05\n","Batch 3420/3470, Loss: 4.2831426981138065e-05\n","Batch 3430/3470, Loss: 5.582688390859403e-05\n","Batch 3440/3470, Loss: 2.2768705093767494e-05\n","Batch 3450/3470, Loss: 3.9866266888566315e-05\n","Batch 3460/3470, Loss: 6.213362939888611e-05\n","Training epoch completed in: 9m 32s\n","Train loss 0.0013638505265812415 accuracy 0.9996037249851397\n","Batch 0/868, Test Loss: 3.1239193049259484e-05\n","Batch 10/868, Test Loss: 2.476508598192595e-05\n","Batch 20/868, Test Loss: 3.123173883068375e-05\n","Batch 30/868, Test Loss: 0.0014393201563507318\n","Batch 40/868, Test Loss: 1.8067494238493964e-05\n","Batch 50/868, Test Loss: 1.8507069398765452e-05\n","Batch 60/868, Test Loss: 0.5392497777938843\n","Batch 70/868, Test Loss: 1.8536871721153148e-05\n","Batch 80/868, Test Loss: 3.6685225495602936e-05\n","Batch 90/868, Test Loss: 3.306433427496813e-05\n","Batch 100/868, Test Loss: 3.1976698664948344e-05\n","Batch 110/868, Test Loss: 0.4269140660762787\n","Batch 120/868, Test Loss: 0.00010010984260588884\n","Batch 130/868, Test Loss: 2.46160889219027e-05\n","Batch 140/868, Test Loss: 0.00016689721087459475\n","Batch 150/868, Test Loss: 0.561896562576294\n","Batch 160/868, Test Loss: 3.782504427363165e-05\n","Batch 170/868, Test Loss: 0.681866466999054\n","Batch 180/868, Test Loss: 0.01197838969528675\n","Batch 190/868, Test Loss: 1.81419982254738e-05\n","Batch 200/868, Test Loss: 1.851452179835178e-05\n","Batch 210/868, Test Loss: 0.00011922066914848983\n","Batch 220/868, Test Loss: 3.119448956567794e-05\n","Batch 230/868, Test Loss: 0.07871872186660767\n","Batch 240/868, Test Loss: 3.765368819586001e-05\n","Batch 250/868, Test Loss: 0.020470524206757545\n","Batch 260/868, Test Loss: 0.5781817436218262\n","Batch 270/868, Test Loss: 0.6824455857276917\n","Batch 280/868, Test Loss: 0.03038753941655159\n","Batch 290/868, Test Loss: 0.5570456981658936\n","Batch 300/868, Test Loss: 0.5247988104820251\n","Batch 310/868, Test Loss: 3.111187106696889e-05\n","Batch 320/868, Test Loss: 0.00010748072963906452\n","Batch 330/868, Test Loss: 0.5585458278656006\n","Batch 340/868, Test Loss: 4.501438161241822e-05\n","Batch 350/868, Test Loss: 0.00032433131127618253\n","Batch 360/868, Test Loss: 0.020271867513656616\n","Batch 370/868, Test Loss: 0.34175214171409607\n","Batch 380/868, Test Loss: 2.4787455913610756e-05\n","Batch 390/868, Test Loss: 3.1365845643449575e-05\n","Batch 400/868, Test Loss: 3.110509715043008e-05\n","Batch 410/868, Test Loss: 6.627359834965318e-05\n","Batch 420/868, Test Loss: 0.4303346574306488\n","Batch 430/868, Test Loss: 8.938791143009439e-05\n","Batch 440/868, Test Loss: 0.06609155982732773\n","Batch 450/868, Test Loss: 2.5442999685765244e-05\n","Batch 460/868, Test Loss: 1.884978701127693e-05\n","Batch 470/868, Test Loss: 3.138820466119796e-05\n","Batch 480/868, Test Loss: 2.499605579941999e-05\n","Batch 490/868, Test Loss: 0.4274943768978119\n","Batch 500/868, Test Loss: 0.3855949640274048\n","Batch 510/868, Test Loss: 3.783248757827096e-05\n","Batch 520/868, Test Loss: 3.1127448892220855e-05\n","Batch 530/868, Test Loss: 2.6314781280234456e-05\n","Batch 540/868, Test Loss: 3.116469088126905e-05\n","Batch 550/868, Test Loss: 0.0947442576289177\n","Batch 560/868, Test Loss: 0.562852144241333\n","Batch 570/868, Test Loss: 0.04121455177664757\n","Batch 580/868, Test Loss: 1.8171800547861494e-05\n","Batch 590/868, Test Loss: 0.04895945265889168\n","Batch 600/868, Test Loss: 1.813454946386628e-05\n","Batch 610/868, Test Loss: 4.455243470147252e-05\n","Batch 620/868, Test Loss: 0.06411182880401611\n","Batch 630/868, Test Loss: 0.6300134062767029\n","Batch 640/868, Test Loss: 0.6139416098594666\n","Batch 650/868, Test Loss: 4.361322498880327e-05\n","Batch 660/868, Test Loss: 0.22956325113773346\n","Batch 670/868, Test Loss: 4.5476106606656685e-05\n","Batch 680/868, Test Loss: 3.872574961860664e-05\n","Batch 690/868, Test Loss: 0.015909001231193542\n","Batch 700/868, Test Loss: 0.6709486246109009\n","Batch 710/868, Test Loss: 3.136583836749196e-05\n","Batch 720/868, Test Loss: 0.0013131477171555161\n","Batch 730/868, Test Loss: 0.00023605911701451987\n","Batch 740/868, Test Loss: 3.796657620114274e-05\n","Batch 750/868, Test Loss: 0.7145152688026428\n","Batch 760/868, Test Loss: 3.101570109720342e-05\n","Batch 770/868, Test Loss: 4.310670556151308e-05\n","Batch 780/868, Test Loss: 0.009075414389371872\n","Batch 790/868, Test Loss: 3.1291343475459144e-05\n","Batch 800/868, Test Loss: 4.461205753614195e-05\n","Batch 810/868, Test Loss: 2.463099008309655e-05\n","Batch 820/868, Test Loss: 9.462371235713363e-05\n","Batch 830/868, Test Loss: 0.010086262598633766\n","Batch 840/868, Test Loss: 0.6830810308456421\n","Batch 850/868, Test Loss: 1.3633499145507812\n","Batch 860/868, Test Loss: 0.00047342778998427093\n","Test evaluation completed in: 0m 46s\n","Test loss 0.11537790792265776 accuracy 0.9820605187319885\n","Test Precision: 0.9822427578022014\n","Test Recall: 0.9820605187319885\n","Test F1 Score: 0.9821352849551807\n","Starting epoch 9/10\n","Batch 0/3470, Loss: 4.239920599502511e-05\n","Batch 10/3470, Loss: 7.26080106687732e-05\n","Batch 20/3470, Loss: 3.8123864214867353e-05\n","Batch 30/3470, Loss: 8.83477769093588e-05\n","Batch 40/3470, Loss: 6.921233580214903e-05\n","Batch 50/3470, Loss: 6.160527118481696e-05\n","Batch 60/3470, Loss: 5.9437876188894734e-05\n","Batch 70/3470, Loss: 8.837928908178583e-05\n","Batch 80/3470, Loss: 6.48092245683074e-05\n","Batch 90/3470, Loss: 3.90402419725433e-05\n","Batch 100/3470, Loss: 5.3031348215881735e-05\n","Batch 110/3470, Loss: 4.240028647473082e-05\n","Batch 120/3470, Loss: 5.376858825911768e-05\n","Batch 130/3470, Loss: 7.304832251975313e-05\n","Batch 140/3470, Loss: 0.00012776751827914268\n","Batch 150/3470, Loss: 4.996869756723754e-05\n","Batch 160/3470, Loss: 4.572280158754438e-05\n","Batch 170/3470, Loss: 7.346337952185422e-05\n","Batch 180/3470, Loss: 0.00036669333348982036\n","Batch 190/3470, Loss: 0.0003796487580984831\n","Batch 200/3470, Loss: 4.702638398157433e-05\n","Batch 210/3470, Loss: 4.546950003714301e-05\n","Batch 220/3470, Loss: 5.323885125108063e-05\n","Batch 230/3470, Loss: 4.672073555411771e-05\n","Batch 240/3470, Loss: 4.229547630529851e-05\n","Batch 250/3470, Loss: 4.336084020906128e-05\n","Batch 260/3470, Loss: 5.948969555902295e-05\n","Batch 270/3470, Loss: 6.160533666843548e-05\n","Batch 280/3470, Loss: 5.670352766173892e-05\n","Batch 290/3470, Loss: 5.1794169849017635e-05\n","Batch 300/3470, Loss: 6.0547434259206057e-05\n","Batch 310/3470, Loss: 7.109426223905757e-05\n","Batch 320/3470, Loss: 4.260107016307302e-05\n","Batch 330/3470, Loss: 3.924121483578347e-05\n","Batch 340/3470, Loss: 3.5061806556768715e-05\n","Batch 350/3470, Loss: 5.940032497164793e-05\n","Batch 360/3470, Loss: 9.445694013265893e-05\n","Batch 370/3470, Loss: 4.639293547370471e-05\n","Batch 380/3470, Loss: 5.3999294323148206e-05\n","Batch 390/3470, Loss: 4.1647374018793926e-05\n","Batch 400/3470, Loss: 4.939496284350753e-05\n","Batch 410/3470, Loss: 3.794462463702075e-05\n","Batch 420/3470, Loss: 5.3865311201661825e-05\n","Batch 430/3470, Loss: 5.6032611610135064e-05\n","Batch 440/3470, Loss: 4.485833051148802e-05\n","Batch 450/3470, Loss: 4.5692562707699835e-05\n","Batch 460/3470, Loss: 5.315754242474213e-05\n","Batch 470/3470, Loss: 3.941981412936002e-05\n","Batch 480/3470, Loss: 3.78629301849287e-05\n","Batch 490/3470, Loss: 5.766448157373816e-05\n","Batch 500/3470, Loss: 6.158248288556933e-05\n","Batch 510/3470, Loss: 7.251727220136672e-05\n","Batch 520/3470, Loss: 8.889799937605858e-05\n","Batch 530/3470, Loss: 5.999603672535159e-05\n","Batch 540/3470, Loss: 8.055469515966251e-05\n","Batch 550/3470, Loss: 8.252459520008415e-05\n","Batch 560/3470, Loss: 3.6842415283899754e-05\n","Batch 570/3470, Loss: 5.240438986220397e-05\n","Batch 580/3470, Loss: 5.044600402470678e-05\n","Batch 590/3470, Loss: 0.006044373847544193\n","Batch 600/3470, Loss: 4.6862503950251266e-05\n","Batch 610/3470, Loss: 5.393221363192424e-05\n","Batch 620/3470, Loss: 4.923847518512048e-05\n","Batch 630/3470, Loss: 3.7647056160494685e-05\n","Batch 640/3470, Loss: 4.3234329496044666e-05\n","Batch 650/3470, Loss: 3.670087244245224e-05\n","Batch 660/3470, Loss: 4.130473098484799e-05\n","Batch 670/3470, Loss: 5.165992843103595e-05\n","Batch 680/3470, Loss: 9.392119682161137e-05\n","Batch 690/3470, Loss: 7.032820576569065e-05\n","Batch 700/3470, Loss: 4.340575105743483e-05\n","Batch 710/3470, Loss: 5.9050245909020305e-05\n","Batch 720/3470, Loss: 4.3465170165291056e-05\n","Batch 730/3470, Loss: 4.273529339116067e-05\n","Batch 740/3470, Loss: 5.183866232982837e-05\n","Batch 750/3470, Loss: 5.417764623416588e-05\n","Batch 760/3470, Loss: 5.6479217164451256e-05\n","Batch 770/3470, Loss: 4.8165507905650884e-05\n","Batch 780/3470, Loss: 4.532039747573435e-05\n","Batch 790/3470, Loss: 4.2429539462318644e-05\n","Batch 800/3470, Loss: 5.036368747823872e-05\n","Batch 810/3470, Loss: 5.9549765865085647e-05\n","Batch 820/3470, Loss: 4.6042889152886346e-05\n","Batch 830/3470, Loss: 4.198252281639725e-05\n","Batch 840/3470, Loss: 4.5707685785600916e-05\n","Batch 850/3470, Loss: 4.6355642552953213e-05\n","Batch 860/3470, Loss: 0.00038943125400692225\n","Batch 870/3470, Loss: 7.687725883442909e-05\n","Batch 880/3470, Loss: 3.594093141146004e-05\n","Batch 890/3470, Loss: 0.0001815373107092455\n","Batch 900/3470, Loss: 5.528804103960283e-05\n","Batch 910/3470, Loss: 6.588187534362078e-05\n","Batch 920/3470, Loss: 4.727194755105302e-05\n","Batch 930/3470, Loss: 4.2690538975875825e-05\n","Batch 940/3470, Loss: 4.345778143033385e-05\n","Batch 950/3470, Loss: 4.082821396877989e-05\n","Batch 960/3470, Loss: 4.3792944779852405e-05\n","Batch 970/3470, Loss: 6.972576375119388e-05\n","Batch 980/3470, Loss: 3.930821912945248e-05\n","Batch 990/3470, Loss: 3.589605330489576e-05\n","Batch 1000/3470, Loss: 9.672440501162782e-05\n","Batch 1010/3470, Loss: 3.843661761493422e-05\n","Batch 1020/3470, Loss: 5.0340258894721046e-05\n","Batch 1030/3470, Loss: 5.7977649703389034e-05\n","Batch 1040/3470, Loss: 4.5245804358273745e-05\n","Batch 1050/3470, Loss: 3.029358595085796e-05\n","Batch 1060/3470, Loss: 4.582680048770271e-05\n","Batch 1070/3470, Loss: 0.02112266607582569\n","Batch 1080/3470, Loss: 4.132692629355006e-05\n","Batch 1090/3470, Loss: 4.324174369685352e-05\n","Batch 1100/3470, Loss: 2.9235638066893443e-05\n","Batch 1110/3470, Loss: 0.0001270073844352737\n","Batch 1120/3470, Loss: 0.0001032035579555668\n","Batch 1130/3470, Loss: 0.002453326480463147\n","Batch 1140/3470, Loss: 0.0016963337548077106\n","Batch 1150/3470, Loss: 6.209710409166291e-05\n","Batch 1160/3470, Loss: 3.0837465601507574e-05\n","Batch 1170/3470, Loss: 5.338109258445911e-05\n","Batch 1180/3470, Loss: 3.907718200935051e-05\n","Batch 1190/3470, Loss: 0.000953342707362026\n","Batch 1200/3470, Loss: 3.297559669590555e-05\n","Batch 1210/3470, Loss: 3.497960642562248e-05\n","Batch 1220/3470, Loss: 8.250795508502051e-05\n","Batch 1230/3470, Loss: 2.7492256776895374e-05\n","Batch 1240/3470, Loss: 4.018706749775447e-05\n","Batch 1250/3470, Loss: 3.641003422671929e-05\n","Batch 1260/3470, Loss: 4.853846621699631e-05\n","Batch 1270/3470, Loss: 3.695392661029473e-05\n","Batch 1280/3470, Loss: 4.312983219278976e-05\n","Batch 1290/3470, Loss: 3.811604983638972e-05\n","Batch 1300/3470, Loss: 3.7840356526430696e-05\n","Batch 1310/3470, Loss: 2.9138769605197012e-05\n","Batch 1320/3470, Loss: 8.504335710313171e-05\n","Batch 1330/3470, Loss: 6.286630377871916e-05\n","Batch 1340/3470, Loss: 3.284150443505496e-05\n","Batch 1350/3470, Loss: 0.0002838523651007563\n","Batch 1360/3470, Loss: 6.815233791712672e-05\n","Batch 1370/3470, Loss: 4.048509799758904e-05\n","Batch 1380/3470, Loss: 3.622357326094061e-05\n","Batch 1390/3470, Loss: 3.87418367608916e-05\n","Batch 1400/3470, Loss: 9.357307862956077e-05\n","Batch 1410/3470, Loss: 2.8557538826134987e-05\n","Batch 1420/3470, Loss: 2.6151181373279542e-05\n","Batch 1430/3470, Loss: 3.369073601788841e-05\n","Batch 1440/3470, Loss: 8.910292672226205e-05\n","Batch 1450/3470, Loss: 3.523282066453248e-05\n","Batch 1460/3470, Loss: 3.898773138644174e-05\n","Batch 1470/3470, Loss: 2.5815856133704074e-05\n","Batch 1480/3470, Loss: 7.033071597106755e-05\n","Batch 1490/3470, Loss: 3.565008955774829e-05\n","Batch 1500/3470, Loss: 6.732734618708491e-05\n","Batch 1510/3470, Loss: 3.226775152143091e-05\n","Batch 1520/3470, Loss: 3.004005156981293e-05\n","Batch 1530/3470, Loss: 4.555099076242186e-05\n","Batch 1540/3470, Loss: 4.4627482566284016e-05\n","Batch 1550/3470, Loss: 4.355431883595884e-05\n","Batch 1560/3470, Loss: 2.9034288672846742e-05\n","Batch 1570/3470, Loss: 2.5979705242207274e-05\n","Batch 1580/3470, Loss: 2.983147351187654e-05\n","Batch 1590/3470, Loss: 3.512114562909119e-05\n","Batch 1600/3470, Loss: 2.6702440663939342e-05\n","Batch 1610/3470, Loss: 4.5610915549332276e-05\n","Batch 1620/3470, Loss: 3.789254333241843e-05\n","Batch 1630/3470, Loss: 3.550091423676349e-05\n","Batch 1640/3470, Loss: 3.914390254067257e-05\n","Batch 1650/3470, Loss: 2.8639466108870693e-05\n","Batch 1660/3470, Loss: 3.968048622482456e-05\n","Batch 1670/3470, Loss: 3.791470953729004e-05\n","Batch 1680/3470, Loss: 3.5702163586393e-05\n","Batch 1690/3470, Loss: 5.863076148671098e-05\n","Batch 1700/3470, Loss: 2.9064152840874158e-05\n","Batch 1710/3470, Loss: 2.2850661480333656e-05\n","Batch 1720/3470, Loss: 2.6613053705659695e-05\n","Batch 1730/3470, Loss: 5.072917338111438e-05\n","Batch 1740/3470, Loss: 3.405562529223971e-05\n","Batch 1750/3470, Loss: 3.7624304241035134e-05\n","Batch 1760/3470, Loss: 3.166413080180064e-05\n","Batch 1770/3470, Loss: 3.196962279616855e-05\n","Batch 1780/3470, Loss: 3.8197758840397e-05\n","Batch 1790/3470, Loss: 3.596293390728533e-05\n","Batch 1800/3470, Loss: 3.228257628506981e-05\n","Batch 1810/3470, Loss: 6.258705980144441e-05\n","Batch 1820/3470, Loss: 3.31094597640913e-05\n","Batch 1830/3470, Loss: 3.072546678595245e-05\n","Batch 1840/3470, Loss: 3.623112934292294e-05\n","Batch 1850/3470, Loss: 4.897847611573525e-05\n","Batch 1860/3470, Loss: 2.2545196770806797e-05\n","Batch 1870/3470, Loss: 4.058961712871678e-05\n","Batch 1880/3470, Loss: 2.811796912283171e-05\n","Batch 1890/3470, Loss: 0.005600476171821356\n","Batch 1900/3470, Loss: 3.3832257031463087e-05\n","Batch 1910/3470, Loss: 3.156732418574393e-05\n","Batch 1920/3470, Loss: 3.5583081626100466e-05\n","Batch 1930/3470, Loss: 3.2744363124947995e-05\n","Batch 1940/3470, Loss: 3.601508433348499e-05\n","Batch 1950/3470, Loss: 2.9794178772135638e-05\n","Batch 1960/3470, Loss: 2.6426716431160457e-05\n","Batch 1970/3470, Loss: 3.4502765629440546e-05\n","Batch 1980/3470, Loss: 2.1956615455565043e-05\n","Batch 1990/3470, Loss: 3.0256165700848214e-05\n","Batch 2000/3470, Loss: 2.0876306734862737e-05\n","Batch 2010/3470, Loss: 3.910666418960318e-05\n","Batch 2020/3470, Loss: 3.7281479308148846e-05\n","Batch 2030/3470, Loss: 3.8168254832271487e-05\n","Batch 2040/3470, Loss: 2.9645203539985232e-05\n","Batch 2050/3470, Loss: 3.0382801924133673e-05\n","Batch 2060/3470, Loss: 3.444320464041084e-05\n","Batch 2070/3470, Loss: 4.181880649412051e-05\n","Batch 2080/3470, Loss: 3.1463016057386994e-05\n","Batch 2090/3470, Loss: 2.4795146600808948e-05\n","Batch 2100/3470, Loss: 2.405749910394661e-05\n","Batch 2110/3470, Loss: 2.8490423574112356e-05\n","Batch 2120/3470, Loss: 2.0995510567445308e-05\n","Batch 2130/3470, Loss: 3.7460147723322734e-05\n","Batch 2140/3470, Loss: 2.758154005277902e-05\n","Batch 2150/3470, Loss: 2.757403126452118e-05\n","Batch 2160/3470, Loss: 0.00020601054711733013\n","Batch 2170/3470, Loss: 2.4757779101491906e-05\n","Batch 2180/3470, Loss: 3.895056215696968e-05\n","Batch 2190/3470, Loss: 3.4130112908314914e-05\n","Batch 2200/3470, Loss: 5.41194349352736e-05\n","Batch 2210/3470, Loss: 2.7775171474786475e-05\n","Batch 2220/3470, Loss: 2.906415829784237e-05\n","Batch 2230/3470, Loss: 2.466838304826524e-05\n","Batch 2240/3470, Loss: 3.778026803047396e-05\n","Batch 2250/3470, Loss: 3.466646376182325e-05\n","Batch 2260/3470, Loss: 2.4094817490549758e-05\n","Batch 2270/3470, Loss: 2.3468957806471735e-05\n","Batch 2280/3470, Loss: 3.461460073594935e-05\n","Batch 2290/3470, Loss: 3.8704642065567896e-05\n","Batch 2300/3470, Loss: 7.723084854660556e-05\n","Batch 2310/3470, Loss: 2.5167617422994226e-05\n","Batch 2320/3470, Loss: 0.0005087416502647102\n","Batch 2330/3470, Loss: 2.2448319214163348e-05\n","Batch 2340/3470, Loss: 2.4005410523386672e-05\n","Batch 2350/3470, Loss: 2.0898645743727684e-05\n","Batch 2360/3470, Loss: 1.8864688172470778e-05\n","Batch 2370/3470, Loss: 3.2453979656565934e-05\n","Batch 2380/3470, Loss: 2.3960652470123023e-05\n","Batch 2390/3470, Loss: 4.202662603347562e-05\n","Batch 2400/3470, Loss: 2.8140271751908585e-05\n","Batch 2410/3470, Loss: 3.061379902646877e-05\n","Batch 2420/3470, Loss: 2.4422621208941564e-05\n","Batch 2430/3470, Loss: 2.5659352104412392e-05\n","Batch 2440/3470, Loss: 3.0457262255367823e-05\n","Batch 2450/3470, Loss: 2.8944792575202882e-05\n","Batch 2460/3470, Loss: 3.924092015950009e-05\n","Batch 2470/3470, Loss: 3.276651113992557e-05\n","Batch 2480/3470, Loss: 3.5320932511240244e-05\n","Batch 2490/3470, Loss: 2.1889487470616587e-05\n","Batch 2500/3470, Loss: 5.784817039966583e-05\n","Batch 2510/3470, Loss: 3.6156547139398754e-05\n","Batch 2520/3470, Loss: 2.8043186830473132e-05\n","Batch 2530/3470, Loss: 3.755696161533706e-05\n","Batch 2540/3470, Loss: 3.354125510668382e-05\n","Batch 2550/3470, Loss: 4.667528992285952e-05\n","Batch 2560/3470, Loss: 4.01864672312513e-05\n","Batch 2570/3470, Loss: 3.3012431231327355e-05\n","Batch 2580/3470, Loss: 5.475863872561604e-05\n","Batch 2590/3470, Loss: 5.539187623071484e-05\n","Batch 2600/3470, Loss: 5.6933997257146984e-05\n","Batch 2610/3470, Loss: 2.7022584617952816e-05\n","Batch 2620/3470, Loss: 0.008674205280840397\n","Batch 2630/3470, Loss: 5.303783473209478e-05\n","Batch 2640/3470, Loss: 5.235236676526256e-05\n","Batch 2650/3470, Loss: 6.170939013827592e-05\n","Batch 2660/3470, Loss: 4.1751198295969516e-05\n","Batch 2670/3470, Loss: 3.720668610185385e-05\n","Batch 2680/3470, Loss: 3.5247488995082676e-05\n","Batch 2690/3470, Loss: 4.46193189418409e-05\n","Batch 2700/3470, Loss: 6.525463686557487e-05\n","Batch 2710/3470, Loss: 2.3893711841083132e-05\n","Batch 2720/3470, Loss: 3.2699517760192975e-05\n","Batch 2730/3470, Loss: 3.813041257672012e-05\n","Batch 2740/3470, Loss: 4.263751543476246e-05\n","Batch 2750/3470, Loss: 4.0961531340144575e-05\n","Batch 2760/3470, Loss: 4.8277550376951694e-05\n","Batch 2770/3470, Loss: 3.541147816576995e-05\n","Batch 2780/3470, Loss: 3.953120540245436e-05\n","Batch 2790/3470, Loss: 6.131421105237678e-05\n","Batch 2800/3470, Loss: 2.676178701221943e-05\n","Batch 2810/3470, Loss: 3.423430462135002e-05\n","Batch 2820/3470, Loss: 4.067819827469066e-05\n","Batch 2830/3470, Loss: 2.093590774165932e-05\n","Batch 2840/3470, Loss: 3.959083915106021e-05\n","Batch 2850/3470, Loss: 3.941214527003467e-05\n","Batch 2860/3470, Loss: 3.3444393920945004e-05\n","Batch 2870/3470, Loss: 3.0025012165424414e-05\n","Batch 2880/3470, Loss: 4.533500032266602e-05\n","Batch 2890/3470, Loss: 3.3384949347237125e-05\n","Batch 2900/3470, Loss: 2.9048962460365146e-05\n","Batch 2910/3470, Loss: 2.5510185878374614e-05\n","Batch 2920/3470, Loss: 4.2168561776634306e-05\n","Batch 2930/3470, Loss: 2.6009347493527457e-05\n","Batch 2940/3470, Loss: 3.253570685046725e-05\n","Batch 2950/3470, Loss: 3.257284333813004e-05\n","Batch 2960/3470, Loss: 3.5403991205384955e-05\n","Batch 2970/3470, Loss: 2.8102836949983612e-05\n","Batch 2980/3470, Loss: 5.016200157115236e-05\n","Batch 2990/3470, Loss: 7.680745329707861e-05\n","Batch 3000/3470, Loss: 1.9706589228007942e-05\n","Batch 3010/3470, Loss: 3.5813824069919065e-05\n","Batch 3020/3470, Loss: 1.9639523088699207e-05\n","Batch 3030/3470, Loss: 3.9821698010200635e-05\n","Batch 3040/3470, Loss: 3.366796227055602e-05\n","Batch 3050/3470, Loss: 2.8639195079449564e-05\n","Batch 3060/3470, Loss: 4.1416307794861495e-05\n","Batch 3070/3470, Loss: 2.387869608355686e-05\n","Batch 3080/3470, Loss: 4.8270245315507054e-05\n","Batch 3090/3470, Loss: 3.760155595955439e-05\n","Batch 3100/3470, Loss: 4.702574005932547e-05\n","Batch 3110/3470, Loss: 2.2917507521924563e-05\n","Batch 3120/3470, Loss: 2.5346438633278012e-05\n","Batch 3130/3470, Loss: 4.6616252802778035e-05\n","Batch 3140/3470, Loss: 3.318382368888706e-05\n","Batch 3150/3470, Loss: 4.1066017729463056e-05\n","Batch 3160/3470, Loss: 2.3900982341729105e-05\n","Batch 3170/3470, Loss: 3.658857895061374e-05\n","Batch 3180/3470, Loss: 2.919060534622986e-05\n","Batch 3190/3470, Loss: 2.2612155589740723e-05\n","Batch 3200/3470, Loss: 3.590308551792987e-05\n","Batch 3210/3470, Loss: 3.421168003114872e-05\n","Batch 3220/3470, Loss: 0.0003500302555039525\n","Batch 3230/3470, Loss: 2.8475396902649663e-05\n","Batch 3240/3470, Loss: 3.827955515589565e-05\n","Batch 3250/3470, Loss: 2.8214726626174524e-05\n","Batch 3260/3470, Loss: 4.0886978240450844e-05\n","Batch 3270/3470, Loss: 4.2936102545354515e-05\n","Batch 3280/3470, Loss: 2.9294878913788125e-05\n","Batch 3290/3470, Loss: 2.5659210223238915e-05\n","Batch 3300/3470, Loss: 2.8609481887542643e-05\n","Batch 3310/3470, Loss: 4.3040141463279724e-05\n","Batch 3320/3470, Loss: 2.7566518838284537e-05\n","Batch 3330/3470, Loss: 4.5066673919791356e-05\n","Batch 3340/3470, Loss: 2.111471076204907e-05\n","Batch 3350/3470, Loss: 3.1887433578958735e-05\n","Batch 3360/3470, Loss: 2.1472256776178256e-05\n","Batch 3370/3470, Loss: 1.9736384274438024e-05\n","Batch 3380/3470, Loss: 2.145734106306918e-05\n","Batch 3390/3470, Loss: 6.048563591320999e-05\n","Batch 3400/3470, Loss: 3.8704318285454065e-05\n","Batch 3410/3470, Loss: 2.403513644821942e-05\n","Batch 3420/3470, Loss: 2.9048967917333357e-05\n","Batch 3430/3470, Loss: 3.992287383880466e-05\n","Batch 3440/3470, Loss: 1.8231394278700463e-05\n","Batch 3450/3470, Loss: 2.9846178222214803e-05\n","Batch 3460/3470, Loss: 4.0849918150343e-05\n","Training epoch completed in: 9m 37s\n","Train loss 0.0008666297275665191 accuracy 0.9998558999945962\n","Batch 0/868, Test Loss: 2.1032594304415397e-05\n","Batch 10/868, Test Loss: 1.7583119188202545e-05\n","Batch 20/868, Test Loss: 2.1084750187583268e-05\n","Batch 30/868, Test Loss: 0.002034354954957962\n","Batch 40/868, Test Loss: 1.3992090316605754e-05\n","Batch 50/868, Test Loss: 1.406659521308029e-05\n","Batch 60/868, Test Loss: 0.5298910737037659\n","Batch 70/868, Test Loss: 1.4148551599646453e-05\n","Batch 80/868, Test Loss: 2.101769860018976e-05\n","Batch 90/868, Test Loss: 2.2373596948455088e-05\n","Batch 100/868, Test Loss: 2.1300793378031813e-05\n","Batch 110/868, Test Loss: 0.4768531918525696\n","Batch 120/868, Test Loss: 0.0006099018501117826\n","Batch 130/868, Test Loss: 1.7530968762002885e-05\n","Batch 140/868, Test Loss: 8.984085434349254e-05\n","Batch 150/868, Test Loss: 0.5978215932846069\n","Batch 160/868, Test Loss: 2.4683226001798175e-05\n","Batch 170/868, Test Loss: 0.6979138255119324\n","Batch 180/868, Test Loss: 0.018048718571662903\n","Batch 190/868, Test Loss: 1.402189263899345e-05\n","Batch 200/868, Test Loss: 1.4088946954871062e-05\n","Batch 210/868, Test Loss: 5.4405150876846164e-05\n","Batch 220/868, Test Loss: 2.1032594304415397e-05\n","Batch 230/868, Test Loss: 0.027379706501960754\n","Batch 240/868, Test Loss: 2.4549126464989968e-05\n","Batch 250/868, Test Loss: 0.008683787658810616\n","Batch 260/868, Test Loss: 0.6049203276634216\n","Batch 270/868, Test Loss: 0.6986883878707886\n","Batch 280/868, Test Loss: 0.5665532350540161\n","Batch 290/868, Test Loss: 0.1540500670671463\n","Batch 300/868, Test Loss: 0.29870370030403137\n","Batch 310/868, Test Loss: 0.011723310686647892\n","Batch 320/868, Test Loss: 3.541160185704939e-05\n","Batch 330/868, Test Loss: 0.5972669720649719\n","Batch 340/868, Test Loss: 2.8073101930203848e-05\n","Batch 350/868, Test Loss: 7.768600335111842e-05\n","Batch 360/868, Test Loss: 0.006041506305336952\n","Batch 370/868, Test Loss: 0.1546296328306198\n","Batch 380/868, Test Loss: 1.7635273252381012e-05\n","Batch 390/868, Test Loss: 2.1181600459385663e-05\n","Batch 400/868, Test Loss: 2.1032597942394204e-05\n","Batch 410/868, Test Loss: 2.153178138541989e-05\n","Batch 420/868, Test Loss: 0.4234738349914551\n","Batch 430/868, Test Loss: 3.578410905902274e-05\n","Batch 440/868, Test Loss: 0.07330308854579926\n","Batch 450/868, Test Loss: 0.00024243041116278619\n","Batch 460/868, Test Loss: 1.4088946954871062e-05\n","Batch 470/868, Test Loss: 2.1107098291395232e-05\n","Batch 480/868, Test Loss: 1.7590571587788872e-05\n","Batch 490/868, Test Loss: 0.3468087315559387\n","Batch 500/868, Test Loss: 0.0021641524508595467\n","Batch 510/868, Test Loss: 2.4601271434221417e-05\n","Batch 520/868, Test Loss: 2.1002793801017106e-05\n","Batch 530/868, Test Loss: 1.7545875380164944e-05\n","Batch 540/868, Test Loss: 2.090594352921471e-05\n","Batch 550/868, Test Loss: 0.01469750888645649\n","Batch 560/868, Test Loss: 0.5993047952651978\n","Batch 570/868, Test Loss: 0.016083629801869392\n","Batch 580/868, Test Loss: 1.4036793800187297e-05\n","Batch 590/868, Test Loss: 0.001211813883855939\n","Batch 600/868, Test Loss: 1.4051695870875847e-05\n","Batch 610/868, Test Loss: 2.8184849725221284e-05\n","Batch 620/868, Test Loss: 0.13003769516944885\n","Batch 630/868, Test Loss: 0.6877601742744446\n","Batch 640/868, Test Loss: 0.6906830072402954\n","Batch 650/868, Test Loss: 2.1591384211205877e-05\n","Batch 660/868, Test Loss: 0.4871925413608551\n","Batch 670/868, Test Loss: 5.124445669935085e-05\n","Batch 680/868, Test Loss: 2.0600331481546164e-05\n","Batch 690/868, Test Loss: 0.005611210595816374\n","Batch 700/868, Test Loss: 0.6473038196563721\n","Batch 710/868, Test Loss: 2.106984356942121e-05\n","Batch 720/868, Test Loss: 2.2366140910889953e-05\n","Batch 730/868, Test Loss: 0.00010512334119994193\n","Batch 740/868, Test Loss: 2.4601275072200224e-05\n","Batch 750/868, Test Loss: 0.7822469472885132\n","Batch 760/868, Test Loss: 2.091339774779044e-05\n","Batch 770/868, Test Loss: 2.242581103928387e-05\n","Batch 780/868, Test Loss: 0.008377119898796082\n","Batch 790/868, Test Loss: 2.113689697580412e-05\n","Batch 800/868, Test Loss: 2.8132708393968642e-05\n","Batch 810/868, Test Loss: 1.7501168258604594e-05\n","Batch 820/868, Test Loss: 3.50385089404881e-05\n","Batch 830/868, Test Loss: 0.0019060943741351366\n","Batch 840/868, Test Loss: 0.6988765597343445\n","Batch 850/868, Test Loss: 1.396512508392334\n","Batch 860/868, Test Loss: 0.00011796034232247621\n","Test evaluation completed in: 0m 46s\n","Test loss 0.11900918919826739 accuracy 0.9828530259365995\n","Test Precision: 0.9829060983315661\n","Test Recall: 0.9828530259365994\n","Test F1 Score: 0.9828776304645104\n","Starting epoch 10/10\n","Batch 0/3470, Loss: 2.9697243007831275e-05\n","Batch 10/3470, Loss: 6.553503772011027e-05\n","Batch 20/3470, Loss: 2.6993073333869688e-05\n","Batch 30/3470, Loss: 5.6770415540086105e-05\n","Batch 40/3470, Loss: 4.0730839828029275e-05\n","Batch 50/3470, Loss: 3.784047657973133e-05\n","Batch 60/3470, Loss: 4.2258452594978735e-05\n","Batch 70/3470, Loss: 4.765963240060955e-05\n","Batch 80/3470, Loss: 4.1222730942536145e-05\n","Batch 90/3470, Loss: 2.7477344701765105e-05\n","Batch 100/3470, Loss: 3.0032713766559027e-05\n","Batch 110/3470, Loss: 2.706757550186012e-05\n","Batch 120/3470, Loss: 2.9034399631200358e-05\n","Batch 130/3470, Loss: 4.5260716433404014e-05\n","Batch 140/3470, Loss: 7.433255814248696e-05\n","Batch 150/3470, Loss: 2.9838936825399287e-05\n","Batch 160/3470, Loss: 2.9026876291027293e-05\n","Batch 170/3470, Loss: 3.21484767482616e-05\n","Batch 180/3470, Loss: 0.062484171241521835\n","Batch 190/3470, Loss: 4.08129817515146e-05\n","Batch 200/3470, Loss: 3.203676897101104e-05\n","Batch 210/3470, Loss: 2.6121289920411073e-05\n","Batch 220/3470, Loss: 3.4011023672064766e-05\n","Batch 230/3470, Loss: 3.018899224116467e-05\n","Batch 240/3470, Loss: 2.927272544184234e-05\n","Batch 250/3470, Loss: 2.997299816343002e-05\n","Batch 260/3470, Loss: 3.9382583054248244e-05\n","Batch 270/3470, Loss: 4.307042763684876e-05\n","Batch 280/3470, Loss: 3.471134914434515e-05\n","Batch 290/3470, Loss: 4.2101637518499047e-05\n","Batch 300/3470, Loss: 3.937498331652023e-05\n","Batch 310/3470, Loss: 3.959105379180983e-05\n","Batch 320/3470, Loss: 3.529242530930787e-05\n","Batch 330/3470, Loss: 2.872879667847883e-05\n","Batch 340/3470, Loss: 2.2038562747184187e-05\n","Batch 350/3470, Loss: 4.120021912967786e-05\n","Batch 360/3470, Loss: 3.852576992358081e-05\n","Batch 370/3470, Loss: 0.00013852521078661084\n","Batch 380/3470, Loss: 3.7042984331492335e-05\n","Batch 390/3470, Loss: 3.48224384651985e-05\n","Batch 400/3470, Loss: 3.616409230744466e-05\n","Batch 410/3470, Loss: 4.53019529231824e-05\n","Batch 420/3470, Loss: 3.7244211853249e-05\n","Batch 430/3470, Loss: 3.7460271414602175e-05\n","Batch 440/3470, Loss: 3.10458563035354e-05\n","Batch 450/3470, Loss: 2.9220571377663873e-05\n","Batch 460/3470, Loss: 3.729640957317315e-05\n","Batch 470/3470, Loss: 2.649371344887186e-05\n","Batch 480/3470, Loss: 2.26122465392109e-05\n","Batch 490/3470, Loss: 3.942717376048677e-05\n","Batch 500/3470, Loss: 4.031328353448771e-05\n","Batch 510/3470, Loss: 3.5292421671329066e-05\n","Batch 520/3470, Loss: 3.2543419365538284e-05\n","Batch 530/3470, Loss: 3.287086292402819e-05\n","Batch 540/3470, Loss: 3.393644874449819e-05\n","Batch 550/3470, Loss: 2.776775545498822e-05\n","Batch 560/3470, Loss: 2.2083273506723344e-05\n","Batch 570/3470, Loss: 2.5592309611965902e-05\n","Batch 580/3470, Loss: 2.5324048692709766e-05\n","Batch 590/3470, Loss: 0.0018251861911267042\n","Batch 600/3470, Loss: 2.7126981876790524e-05\n","Batch 610/3470, Loss: 3.1388535717269406e-05\n","Batch 620/3470, Loss: 2.5942503270925954e-05\n","Batch 630/3470, Loss: 2.1792700863443315e-05\n","Batch 640/3470, Loss: 2.6389529011794366e-05\n","Batch 650/3470, Loss: 2.142017001460772e-05\n","Batch 660/3470, Loss: 2.6374569642939605e-05\n","Batch 670/3470, Loss: 2.9846352845197544e-05\n","Batch 680/3470, Loss: 2.8594809919013642e-05\n","Batch 690/3470, Loss: 3.425684190005995e-05\n","Batch 700/3470, Loss: 2.3275279090739787e-05\n","Batch 710/3470, Loss: 3.129157266812399e-05\n","Batch 720/3470, Loss: 2.8184835173306055e-05\n","Batch 730/3470, Loss: 2.5674265998532064e-05\n","Batch 740/3470, Loss: 0.0016838879091665149\n","Batch 750/3470, Loss: 2.9019309295108542e-05\n","Batch 760/3470, Loss: 3.1589530408382416e-05\n","Batch 770/3470, Loss: 2.3789280021446757e-05\n","Batch 780/3470, Loss: 2.4996283173095435e-05\n","Batch 790/3470, Loss: 2.5361341613461263e-05\n","Batch 800/3470, Loss: 2.7223928555031307e-05\n","Batch 810/3470, Loss: 4.050002826261334e-05\n","Batch 820/3470, Loss: 3.079242378589697e-05\n","Batch 830/3470, Loss: 2.4884471713448875e-05\n","Batch 840/3470, Loss: 2.776026121864561e-05\n","Batch 850/3470, Loss: 2.776037399598863e-05\n","Batch 860/3470, Loss: 3.6804613046115264e-05\n","Batch 870/3470, Loss: 4.21912336605601e-05\n","Batch 880/3470, Loss: 2.0265366401872598e-05\n","Batch 890/3470, Loss: 5.423349284683354e-05\n","Batch 900/3470, Loss: 2.977935946546495e-05\n","Batch 910/3470, Loss: 3.680493318825029e-05\n","Batch 920/3470, Loss: 2.405755003564991e-05\n","Batch 930/3470, Loss: 2.5264413125114515e-05\n","Batch 940/3470, Loss: 2.62106659647543e-05\n","Batch 950/3470, Loss: 0.0002505616575945169\n","Batch 960/3470, Loss: 2.5607198040233925e-05\n","Batch 970/3470, Loss: 4.120025914744474e-05\n","Batch 980/3470, Loss: 2.2411011741496623e-05\n","Batch 990/3470, Loss: 2.475046130712144e-05\n","Batch 1000/3470, Loss: 2.5629542506067082e-05\n","Batch 1010/3470, Loss: 2.562946974649094e-05\n","Batch 1020/3470, Loss: 2.9704706321354024e-05\n","Batch 1030/3470, Loss: 3.9404858398484066e-05\n","Batch 1040/3470, Loss: 3.2401767384726554e-05\n","Batch 1050/3470, Loss: 2.0116358427912928e-05\n","Batch 1060/3470, Loss: 2.9056598577881232e-05\n","Batch 1070/3470, Loss: 0.00015151248953770846\n","Batch 1080/3470, Loss: 2.5815814296947792e-05\n","Batch 1090/3470, Loss: 3.260274388594553e-05\n","Batch 1100/3470, Loss: 1.9274462829343975e-05\n","Batch 1110/3470, Loss: 6.921483145561069e-05\n","Batch 1120/3470, Loss: 3.204409222234972e-05\n","Batch 1130/3470, Loss: 0.00039505885797552764\n","Batch 1140/3470, Loss: 6.198108894750476e-05\n","Batch 1150/3470, Loss: 3.778792597586289e-05\n","Batch 1160/3470, Loss: 2.0622986994567327e-05\n","Batch 1170/3470, Loss: 3.532211121637374e-05\n","Batch 1180/3470, Loss: 2.6828987756744027e-05\n","Batch 1190/3470, Loss: 3.4688426239881665e-05\n","Batch 1200/3470, Loss: 2.109981323883403e-05\n","Batch 1210/3470, Loss: 2.0846448023803532e-05\n","Batch 1220/3470, Loss: 3.64098850695882e-05\n","Batch 1230/3470, Loss: 1.9587381757446565e-05\n","Batch 1240/3470, Loss: 2.7894502636627294e-05\n","Batch 1250/3470, Loss: 2.7298417990095913e-05\n","Batch 1260/3470, Loss: 0.006730203982442617\n","Batch 1270/3470, Loss: 2.216516077169217e-05\n","Batch 1280/3470, Loss: 2.4050092179095373e-05\n","Batch 1290/3470, Loss: 2.615103767311666e-05\n","Batch 1300/3470, Loss: 2.005673013627529e-05\n","Batch 1310/3470, Loss: 1.969913500943221e-05\n","Batch 1320/3470, Loss: 2.606914313219022e-05\n","Batch 1330/3470, Loss: 2.899695755331777e-05\n","Batch 1340/3470, Loss: 2.2627078578807414e-05\n","Batch 1350/3470, Loss: 4.376894503366202e-05\n","Batch 1360/3470, Loss: 3.156733146170154e-05\n","Batch 1370/3470, Loss: 2.390844383626245e-05\n","Batch 1380/3470, Loss: 2.6776773665915243e-05\n","Batch 1390/3470, Loss: 2.5510267732897773e-05\n","Batch 1400/3470, Loss: 2.5540144633851014e-05\n","Batch 1410/3470, Loss: 2.308152397745289e-05\n","Batch 1420/3470, Loss: 1.9371311282156967e-05\n","Batch 1430/3470, Loss: 2.5160177756333724e-05\n","Batch 1440/3470, Loss: 2.187457721447572e-05\n","Batch 1450/3470, Loss: 2.4072414817055687e-05\n","Batch 1460/3470, Loss: 2.6456531486473978e-05\n","Batch 1470/3470, Loss: 2.223962110292632e-05\n","Batch 1480/3470, Loss: 2.8765971364919096e-05\n","Batch 1490/3470, Loss: 2.5867915610433556e-05\n","Batch 1500/3470, Loss: 0.0009190123528242111\n","Batch 1510/3470, Loss: 2.518996006983798e-05\n","Batch 1520/3470, Loss: 2.3312521079787984e-05\n","Batch 1530/3470, Loss: 3.6133893445366994e-05\n","Batch 1540/3470, Loss: 3.0472170692519285e-05\n","Batch 1550/3470, Loss: 2.8490376280387864e-05\n","Batch 1560/3470, Loss: 2.153186869691126e-05\n","Batch 1570/3470, Loss: 2.1002917492296547e-05\n","Batch 1580/3470, Loss: 2.569649223005399e-05\n","Batch 1590/3470, Loss: 2.6180850909440778e-05\n","Batch 1600/3470, Loss: 2.4012804715312086e-05\n","Batch 1610/3470, Loss: 2.8401056624716148e-05\n","Batch 1620/3470, Loss: 2.823714748956263e-05\n","Batch 1630/3470, Loss: 2.4735503757256083e-05\n","Batch 1640/3470, Loss: 2.3185879399534315e-05\n","Batch 1650/3470, Loss: 2.1941596060059965e-05\n","Batch 1660/3470, Loss: 3.319883762742393e-05\n","Batch 1670/3470, Loss: 2.963774386444129e-05\n","Batch 1680/3470, Loss: 2.5957304387702607e-05\n","Batch 1690/3470, Loss: 2.6493737095734105e-05\n","Batch 1700/3470, Loss: 2.1509549696929753e-05\n","Batch 1710/3470, Loss: 1.8693326637730934e-05\n","Batch 1720/3470, Loss: 2.1621290215989575e-05\n","Batch 1730/3470, Loss: 3.2535914215259254e-05\n","Batch 1740/3470, Loss: 2.751445208559744e-05\n","Batch 1750/3470, Loss: 2.806567499646917e-05\n","Batch 1760/3470, Loss: 2.5011098841787316e-05\n","Batch 1770/3470, Loss: 2.4504546672687866e-05\n","Batch 1780/3470, Loss: 2.575612234068103e-05\n","Batch 1790/3470, Loss: 2.5540042770444416e-05\n","Batch 1800/3470, Loss: 4.244276351528242e-05\n","Batch 1810/3470, Loss: 4.1482904634904116e-05\n","Batch 1820/3470, Loss: 2.5689034373499453e-05\n","Batch 1830/3470, Loss: 2.2411028112401254e-05\n","Batch 1840/3470, Loss: 3.0665902158943936e-05\n","Batch 1850/3470, Loss: 7.094944157870486e-05\n","Batch 1860/3470, Loss: 1.7165984900202602e-05\n","Batch 1870/3470, Loss: 3.191003270330839e-05\n","Batch 1880/3470, Loss: 1.9594783225329593e-05\n","Batch 1890/3470, Loss: 3.2990377803798765e-05\n","Batch 1900/3470, Loss: 3.2364489015890285e-05\n","Batch 1910/3470, Loss: 2.6486133720027283e-05\n","Batch 1920/3470, Loss: 3.1097966711968184e-05\n","Batch 1930/3470, Loss: 2.254508581245318e-05\n","Batch 1940/3470, Loss: 2.6322399207856506e-05\n","Batch 1950/3470, Loss: 2.8438231311156414e-05\n","Batch 1960/3470, Loss: 2.3275155399460346e-05\n","Batch 1970/3470, Loss: 3.109798126388341e-05\n","Batch 1980/3470, Loss: 1.8656075553735718e-05\n","Batch 1990/3470, Loss: 2.1993793779984117e-05\n","Batch 2000/3470, Loss: 1.79855360329384e-05\n","Batch 2010/3470, Loss: 2.8959793780813925e-05\n","Batch 2020/3470, Loss: 2.811790182022378e-05\n","Batch 2030/3470, Loss: 3.0449815312749706e-05\n","Batch 2040/3470, Loss: 2.711213892325759e-05\n","Batch 2050/3470, Loss: 2.5338971681776457e-05\n","Batch 2060/3470, Loss: 3.281138924648985e-05\n","Batch 2070/3470, Loss: 3.006243787240237e-05\n","Batch 2080/3470, Loss: 2.7536734705790877e-05\n","Batch 2090/3470, Loss: 1.9632043404271826e-05\n","Batch 2100/3470, Loss: 2.0079067326150835e-05\n","Batch 2110/3470, Loss: 2.4392757040914148e-05\n","Batch 2120/3470, Loss: 1.709148273221217e-05\n","Batch 2130/3470, Loss: 2.925022636190988e-05\n","Batch 2140/3470, Loss: 2.214277810708154e-05\n","Batch 2150/3470, Loss: 2.235141073470004e-05\n","Batch 2160/3470, Loss: 2.8475535145844333e-05\n","Batch 2170/3470, Loss: 2.0928409867337905e-05\n","Batch 2180/3470, Loss: 3.223046223865822e-05\n","Batch 2190/3470, Loss: 2.5257051674998365e-05\n","Batch 2200/3470, Loss: 2.0786845198017545e-05\n","Batch 2210/3470, Loss: 2.314105222467333e-05\n","Batch 2220/3470, Loss: 2.7164134735357948e-05\n","Batch 2230/3470, Loss: 2.1114676201250404e-05\n","Batch 2240/3470, Loss: 2.382656020927243e-05\n","Batch 2250/3470, Loss: 2.5279376131948084e-05\n","Batch 2260/3470, Loss: 1.7396912880940363e-05\n","Batch 2270/3470, Loss: 1.8492148228688166e-05\n","Batch 2280/3470, Loss: 3.5851226130034775e-05\n","Batch 2290/3470, Loss: 3.515842035994865e-05\n","Batch 2300/3470, Loss: 2.6016909032477997e-05\n","Batch 2310/3470, Loss: 1.955755396920722e-05\n","Batch 2320/3470, Loss: 6.659144855802879e-05\n","Batch 2330/3470, Loss: 2.0056710127391852e-05\n","Batch 2340/3470, Loss: 2.088368455588352e-05\n","Batch 2350/3470, Loss: 1.6890320694074035e-05\n","Batch 2360/3470, Loss: 1.648054603720084e-05\n","Batch 2370/3470, Loss: 2.662782571860589e-05\n","Batch 2380/3470, Loss: 1.8641103451955132e-05\n","Batch 2390/3470, Loss: 2.5391089366166852e-05\n","Batch 2400/3470, Loss: 2.604670407890808e-05\n","Batch 2410/3470, Loss: 2.6836474717129022e-05\n","Batch 2420/3470, Loss: 2.16137614188483e-05\n","Batch 2430/3470, Loss: 2.336466423003003e-05\n","Batch 2440/3470, Loss: 2.3409313143929467e-05\n","Batch 2450/3470, Loss: 2.37818640016485e-05\n","Batch 2460/3470, Loss: 0.0015680084470659494\n","Batch 2470/3470, Loss: 7.094445027178153e-05\n","Batch 2480/3470, Loss: 2.4370303435716778e-05\n","Batch 2490/3470, Loss: 2.4482165827066638e-05\n","Batch 2500/3470, Loss: 2.788700658129528e-05\n","Batch 2510/3470, Loss: 2.714187758101616e-05\n","Batch 2520/3470, Loss: 2.255988511024043e-05\n","Batch 2530/3470, Loss: 2.3945740394992754e-05\n","Batch 2540/3470, Loss: 3.7809797504451126e-05\n","Batch 2550/3470, Loss: 2.893734745157417e-05\n","Batch 2560/3470, Loss: 2.4571481844759546e-05\n","Batch 2570/3470, Loss: 2.4400256734224968e-05\n","Batch 2580/3470, Loss: 3.059876326005906e-05\n","Batch 2590/3470, Loss: 3.07477566821035e-05\n","Batch 2600/3470, Loss: 3.1060739274835214e-05\n","Batch 2610/3470, Loss: 2.0712363038910553e-05\n","Batch 2620/3470, Loss: 8.867438737070188e-05\n","Batch 2630/3470, Loss: 2.946634776890278e-05\n","Batch 2640/3470, Loss: 3.340007606311701e-05\n","Batch 2650/3470, Loss: 3.2513496989849955e-05\n","Batch 2660/3470, Loss: 2.7067368137068115e-05\n","Batch 2670/3470, Loss: 2.4444925657007843e-05\n","Batch 2680/3470, Loss: 2.6270121452398598e-05\n","Batch 2690/3470, Loss: 2.6262732717441395e-05\n","Batch 2700/3470, Loss: 2.6307518055546097e-05\n","Batch 2710/3470, Loss: 1.7866324924398214e-05\n","Batch 2720/3470, Loss: 2.309642150066793e-05\n","Batch 2730/3470, Loss: 2.488446989445947e-05\n","Batch 2740/3470, Loss: 2.8512682547443546e-05\n","Batch 2750/3470, Loss: 2.516763925086707e-05\n","Batch 2760/3470, Loss: 3.0189032258931547e-05\n","Batch 2770/3470, Loss: 2.244819734187331e-05\n","Batch 2780/3470, Loss: 2.8281774575589225e-05\n","Batch 2790/3470, Loss: 3.7192177842371166e-05\n","Batch 2800/3470, Loss: 2.0324898287071846e-05\n","Batch 2810/3470, Loss: 2.329755443497561e-05\n","Batch 2820/3470, Loss: 2.512287574063521e-05\n","Batch 2830/3470, Loss: 1.8559216186986305e-05\n","Batch 2840/3470, Loss: 2.676935946510639e-05\n","Batch 2850/3470, Loss: 2.6016927222372033e-05\n","Batch 2860/3470, Loss: 2.298465733474586e-05\n","Batch 2870/3470, Loss: 2.4638566173962317e-05\n","Batch 2880/3470, Loss: 3.084475247305818e-05\n","Batch 2890/3470, Loss: 2.4601346012786962e-05\n","Batch 2900/3470, Loss: 1.9930053895222954e-05\n","Batch 2910/3470, Loss: 2.082409082504455e-05\n","Batch 2920/3470, Loss: 2.6851317670661956e-05\n","Batch 2930/3470, Loss: 1.9103054000879638e-05\n","Batch 2940/3470, Loss: 2.2843083570478484e-05\n","Batch 2950/3470, Loss: 2.3237926143337972e-05\n","Batch 2960/3470, Loss: 2.723872967180796e-05\n","Batch 2970/3470, Loss: 1.943833012774121e-05\n","Batch 2980/3470, Loss: 2.822962596837897e-05\n","Batch 2990/3470, Loss: 0.00013872372801415622\n","Batch 3000/3470, Loss: 1.8134542187908664e-05\n","Batch 3010/3470, Loss: 3.714726699399762e-05\n","Batch 3020/3470, Loss: 1.6733858501538634e-05\n","Batch 3030/3470, Loss: 2.2276875824900344e-05\n","Batch 3040/3470, Loss: 2.217257315351162e-05\n","Batch 3050/3470, Loss: 2.2351407096721232e-05\n","Batch 3060/3470, Loss: 3.10904870275408e-05\n","Batch 3070/3470, Loss: 2.2917602109373547e-05\n","Batch 3080/3470, Loss: 3.281153112766333e-05\n","Batch 3090/3470, Loss: 2.6925792553811334e-05\n","Batch 3100/3470, Loss: 2.9823975637555122e-05\n","Batch 3110/3470, Loss: 1.8536802599555813e-05\n","Batch 3120/3470, Loss: 2.1777617803309113e-05\n","Batch 3130/3470, Loss: 3.293810004834086e-05\n","Batch 3140/3470, Loss: 2.4586424842709675e-05\n","Batch 3150/3470, Loss: 3.0822277039987966e-05\n","Batch 3160/3470, Loss: 1.9036006051464938e-05\n","Batch 3170/3470, Loss: 2.923543070210144e-05\n","Batch 3180/3470, Loss: 2.3491236788686365e-05\n","Batch 3190/3470, Loss: 2.0198209313093685e-05\n","Batch 3200/3470, Loss: 2.585299444035627e-05\n","Batch 3210/3470, Loss: 2.6411731596454047e-05\n","Batch 3220/3470, Loss: 4.030262789456174e-05\n","Batch 3230/3470, Loss: 2.217255132563878e-05\n","Batch 3240/3470, Loss: 2.8326459869276732e-05\n","Batch 3250/3470, Loss: 2.6031713787233457e-05\n","Batch 3260/3470, Loss: 2.8944736186531372e-05\n","Batch 3270/3470, Loss: 3.4554766898509115e-05\n","Batch 3280/3470, Loss: 2.2172602257342078e-05\n","Batch 3290/3470, Loss: 1.945322219398804e-05\n","Batch 3300/3470, Loss: 2.3468932340620086e-05\n","Batch 3310/3470, Loss: 3.442817251197994e-05\n","Batch 3320/3470, Loss: 2.49515323957894e-05\n","Batch 3330/3470, Loss: 2.9123608328518458e-05\n","Batch 3340/3470, Loss: 1.5526889910688624e-05\n","Batch 3350/3470, Loss: 2.3372023861156777e-05\n","Batch 3360/3470, Loss: 2.3267772121471353e-05\n","Batch 3370/3470, Loss: 1.7754564396454953e-05\n","Batch 3380/3470, Loss: 2.111461253662128e-05\n","Batch 3390/3470, Loss: 1.9721432181540877e-05\n","Batch 3400/3470, Loss: 2.7469723136164248e-05\n","Batch 3410/3470, Loss: 2.006408067245502e-05\n","Batch 3420/3470, Loss: 2.5003553673741408e-05\n","Batch 3430/3470, Loss: 1.7314998331130482e-05\n","Batch 3440/3470, Loss: 1.777692341420334e-05\n","Batch 3450/3470, Loss: 2.5256989829358645e-05\n","Batch 3460/3470, Loss: 3.1321411370299757e-05\n","Training epoch completed in: 9m 37s\n","Train loss 0.0004695698270416091 accuracy 0.9999459624979736\n","Batch 0/868, Test Loss: 1.7337264580419287e-05\n","Batch 10/868, Test Loss: 1.4908454431861173e-05\n","Batch 20/868, Test Loss: 1.7367063264828175e-05\n","Batch 30/868, Test Loss: 0.00011134384840261191\n","Batch 40/868, Test Loss: 1.2330637218838092e-05\n","Batch 50/868, Test Loss: 1.241259178641485e-05\n","Batch 60/868, Test Loss: 0.5795297026634216\n","Batch 70/868, Test Loss: 1.2405142115312628e-05\n","Batch 80/868, Test Loss: 1.728511051624082e-05\n","Batch 90/868, Test Loss: 0.0032466307748109102\n","Batch 100/868, Test Loss: 1.724041430861689e-05\n","Batch 110/868, Test Loss: 0.5419427156448364\n","Batch 120/868, Test Loss: 7.687031757086515e-05\n","Batch 130/868, Test Loss: 1.4833949535386637e-05\n","Batch 140/868, Test Loss: 2.0071525796083733e-05\n","Batch 150/868, Test Loss: 0.6179507970809937\n","Batch 160/868, Test Loss: 1.988527765206527e-05\n","Batch 170/868, Test Loss: 0.7060360312461853\n","Batch 180/868, Test Loss: 0.007301028352230787\n","Batch 190/868, Test Loss: 1.2375339792924933e-05\n","Batch 200/868, Test Loss: 1.2397691534715705e-05\n","Batch 210/868, Test Loss: 2.3289765522349626e-05\n","Batch 220/868, Test Loss: 1.735216392262373e-05\n","Batch 230/868, Test Loss: 0.01492302305996418\n","Batch 240/868, Test Loss: 1.9862925910274498e-05\n","Batch 250/868, Test Loss: 0.002775690983980894\n","Batch 260/868, Test Loss: 0.595948338508606\n","Batch 270/868, Test Loss: 0.7066428661346436\n","Batch 280/868, Test Loss: 0.26348763704299927\n","Batch 290/868, Test Loss: 0.07501110434532166\n","Batch 300/868, Test Loss: 0.39561277627944946\n","Batch 310/868, Test Loss: 0.053845662623643875\n","Batch 320/868, Test Loss: 2.7380321625969373e-05\n","Batch 330/868, Test Loss: 0.6179696917533875\n","Batch 340/868, Test Loss: 2.2381140297511593e-05\n","Batch 350/868, Test Loss: 3.95068054785952e-05\n","Batch 360/868, Test Loss: 0.0010278841946274042\n","Batch 370/868, Test Loss: 0.12844794988632202\n","Batch 380/868, Test Loss: 1.4930807083146647e-05\n","Batch 390/868, Test Loss: 1.7419217329006642e-05\n","Batch 400/868, Test Loss: 1.735961632221006e-05\n","Batch 410/868, Test Loss: 1.7478821973782033e-05\n","Batch 420/868, Test Loss: 0.44067618250846863\n","Batch 430/868, Test Loss: 2.8386089979903772e-05\n","Batch 440/868, Test Loss: 0.041101470589637756\n","Batch 450/868, Test Loss: 0.00028872030088678\n","Batch 460/868, Test Loss: 1.2405142115312628e-05\n","Batch 470/868, Test Loss: 1.742666972859297e-05\n","Batch 480/868, Test Loss: 1.4893554180162027e-05\n","Batch 490/868, Test Loss: 0.4572503864765167\n","Batch 500/868, Test Loss: 0.0005388659774325788\n","Batch 510/868, Test Loss: 1.9766077457461506e-05\n","Batch 520/868, Test Loss: 1.7344713342026807e-05\n","Batch 530/868, Test Loss: 1.4871205166855361e-05\n","Batch 540/868, Test Loss: 1.7232963728019968e-05\n","Batch 550/868, Test Loss: 0.006000284571200609\n","Batch 560/868, Test Loss: 0.6188483834266663\n","Batch 570/868, Test Loss: 0.2603875398635864\n","Batch 580/868, Test Loss: 1.2390241863613483e-05\n","Batch 590/868, Test Loss: 0.0001285901089431718\n","Batch 600/868, Test Loss: 1.2367888302833308e-05\n","Batch 610/868, Test Loss: 2.241839320049621e-05\n","Batch 620/868, Test Loss: 0.19203300774097443\n","Batch 630/868, Test Loss: 0.700320303440094\n","Batch 640/868, Test Loss: 0.7007321119308472\n","Batch 650/868, Test Loss: 1.756822894094512e-05\n","Batch 660/868, Test Loss: 0.5940224528312683\n","Batch 670/868, Test Loss: 1.8477179764886387e-05\n","Batch 680/868, Test Loss: 7.030582492006943e-05\n","Batch 690/868, Test Loss: 0.00016231306653935462\n","Batch 700/868, Test Loss: 0.7009286284446716\n","Batch 710/868, Test Loss: 1.7322365238214843e-05\n","Batch 720/868, Test Loss: 1.548213913338259e-05\n","Batch 730/868, Test Loss: 0.005701629910618067\n","Batch 740/868, Test Loss: 1.975862505787518e-05\n","Batch 750/868, Test Loss: 0.7885391116142273\n","Batch 760/868, Test Loss: 1.7300011677434668e-05\n","Batch 770/868, Test Loss: 1.7642729289946146e-05\n","Batch 780/868, Test Loss: 0.008586909621953964\n","Batch 790/868, Test Loss: 1.7411766748409718e-05\n","Batch 800/868, Test Loss: 2.2373693354893476e-05\n","Batch 810/868, Test Loss: 1.4833950444881339e-05\n","Batch 820/868, Test Loss: 2.3245356715051457e-05\n","Batch 830/868, Test Loss: 0.0008338553598150611\n","Batch 840/868, Test Loss: 0.7066535949707031\n","Batch 850/868, Test Loss: 1.4123674631118774\n","Batch 860/868, Test Loss: 3.170062700519338e-05\n","Test evaluation completed in: 0m 46s\n","Test loss 0.12726332314760075 accuracy 0.9832132564841499\n","Test Precision: 0.9831836800114114\n","Test Recall: 0.9832132564841498\n","Test F1 Score: 0.9831976617542018\n"]}],"source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split\n","import time\n","import traceback\n","\n","class ClickbaitDataset(Dataset):\n","    def __init__(self, titles, labels, tokenizer, max_len):\n","        self.titles = titles\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.titles)\n","\n","    def __getitem__(self, idx):\n","        title = self.titles[idx]\n","        label = self.labels[idx]\n","\n","        # Ensure the label is a valid integer\n","        try:\n","            label = int(label)\n","        except ValueError:\n","            raise ValueError(f\"Label {label} at index {idx} is not a valid integer.\")\n","\n","        encoding = self.tokenizer.encode_plus(\n","            title,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'title_text': title,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","def create_data_loader(titles, labels, tokenizer, max_len, batch_size):\n","    ds = ClickbaitDataset(\n","        titles=titles,\n","        labels=labels,\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","\n","    return DataLoader(ds, batch_size=batch_size, num_workers=4, pin_memory=True)\n","\n","def train_epoch(\n","    model,\n","    data_loader,\n","    loss_fn,\n","    optimizer,\n","    device,\n","    scheduler,\n","    n_examples\n","):\n","    model.train()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    for batch_idx, d in enumerate(data_loader):\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        labels = d[\"labels\"].to(device)\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        _, preds = torch.max(outputs.logits, dim=1)\n","        loss = loss_fn(outputs.logits, labels)\n","\n","        correct_predictions += torch.sum(preds == labels)\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        if batch_idx % 10 == 0:\n","            print(f'Batch {batch_idx}/{len(data_loader)}, Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Training epoch completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model.eval()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    all_labels = []\n","    all_preds = []\n","\n","    with torch.no_grad():\n","        for batch_idx, d in enumerate(data_loader):\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            labels = d[\"labels\"].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            loss = loss_fn(outputs.logits, labels)\n","\n","            correct_predictions += torch.sum(preds == labels)\n","            losses.append(loss.item())\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","\n","            if batch_idx % 10 == 0:\n","                print(f'Batch {batch_idx}/{len(data_loader)}, Test Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Test evaluation completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses), all_labels, all_preds\n","\n","def clean_dataset(df):\n","    # Remove rows where labels are NaN\n","    df = df.dropna(subset=['label'])\n","    # Convert labels to integers, and filter out rows where this conversion fails\n","    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n","    df = df.dropna(subset=['label'])\n","    df['label'] = df['label'].astype(int)\n","    return df\n","\n","def main():\n","    print(\"Starting main process\")\n","\n","    try:\n","        print('Loading the dataset.')\n","        df = pd.read_csv('/content/drive/MyDrive/clickbait/german.csv')\n","        print('Dataset loaded successfully.')\n","\n","        print('Cleaning the dataset.')\n","        df = clean_dataset(df)\n","        print('Dataset cleaned.')\n","\n","        print('Splitting the dataset into training and test sets.')\n","        df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n","        print('Dataset split into training and test sets.')\n","\n","        RANDOM_SEED = 42\n","        MAX_LEN = 128\n","        BATCH_SIZE = 16\n","        EPOCHS = 10\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f'Using device: {device}')\n","\n","        print('Loading the tokenizer and model.')\n","        tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n","        model = BertForSequenceClassification.from_pretrained('dbmdz/bert-base-german-cased', num_labels=2)\n","        model = model.to(device)\n","        print('Model and tokenizer loaded and moved to device.')\n","\n","        print('Creating data loaders.')\n","        train_titles = df_train['title'].tolist()\n","        train_labels = df_train['label'].tolist()\n","        test_titles = df_test['title'].tolist()\n","        test_labels = df_test['label'].tolist()\n","\n","        train_data_loader = create_data_loader(train_titles, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        test_data_loader = create_data_loader(test_titles, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        print('Data loaders created.')\n","\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","        total_steps = len(train_data_loader) * EPOCHS\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","        loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","        print('Optimizer, scheduler, and loss function defined.')\n","\n","        for epoch in range(EPOCHS):\n","            print(f'Starting epoch {epoch + 1}/{EPOCHS}')\n","\n","            train_acc, train_loss = train_epoch(\n","                model,\n","                train_data_loader,\n","                loss_fn,\n","                optimizer,\n","                device,\n","                scheduler,\n","                len(df_train)\n","            )\n","            print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","            test_acc, test_loss, all_labels, all_preds = eval_model(\n","                model,\n","                test_data_loader,\n","                loss_fn,\n","                device,\n","                len(df_test)\n","            )\n","            print(f'Test loss {test_loss} accuracy {test_acc}')\n","\n","            # Calculate additional metrics\n","            precision = precision_score(all_labels, all_preds, average='weighted')\n","            recall = recall_score(all_labels, all_preds, average='weighted')\n","            f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","            print(f'Test Precision: {precision}')\n","            print(f'Test Recall: {recall}')\n","            print(f'Test F1 Score: {f1}')\n","\n","    except Exception as e:\n","        print(f'An error occurred: {e}')\n","        traceback.print_exc()\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28951,"status":"ok","timestamp":1722593441445,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"},"user_tz":-180},"id":"Ic7ZMvG1IbEx","outputId":"ef2b5f58-0f39-4eaf-c9d1-0e46d416eaf2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1ZDoY_cCvGd99UeEdfZDeQTAR73hRmiF8","authorship_tag":"ABX9TyPc486VVW0tMqhenN6EzMdf"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02d092e1517f4b4fb52a33b4c388e3f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1a54def02b34b48816bbb417b314271","IPY_MODEL_50be26be971144f789f1b8b787060cf2","IPY_MODEL_aab2b9a582e64fcc86bd8b0236f7f2ee"],"layout":"IPY_MODEL_fa88061446074d35a9c73cc8099ab03b"}},"f1a54def02b34b48816bbb417b314271":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59eb56b7521241f797781e36231db5b5","placeholder":"​","style":"IPY_MODEL_f9b29fc7ba114407bfe016084d86df7a","value":"tokenizer_config.json: 100%"}},"50be26be971144f789f1b8b787060cf2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_79457734e68343e790050772a99f50f8","max":59,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e11e3ae44a548049f75a7af80517dd2","value":59}},"aab2b9a582e64fcc86bd8b0236f7f2ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf9484accf67414bb9b13eb18e129341","placeholder":"​","style":"IPY_MODEL_76e1397bd92e412a99bdac738ea477f8","value":" 59.0/59.0 [00:00&lt;00:00, 4.37kB/s]"}},"fa88061446074d35a9c73cc8099ab03b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59eb56b7521241f797781e36231db5b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9b29fc7ba114407bfe016084d86df7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79457734e68343e790050772a99f50f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e11e3ae44a548049f75a7af80517dd2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf9484accf67414bb9b13eb18e129341":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76e1397bd92e412a99bdac738ea477f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"beef9b9f41ea463b9f6e77e15a2a2801":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2b0bc26a995840c0aca2d3bec1eb0119","IPY_MODEL_541662e910c540b78499d921c303267e","IPY_MODEL_2676a26d9b2b456c97b5b033d69565f6"],"layout":"IPY_MODEL_21470fb9fb894ff79202686f97a0e20e"}},"2b0bc26a995840c0aca2d3bec1eb0119":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8774ceb09a244b438e9ab45375fdf6df","placeholder":"​","style":"IPY_MODEL_a0b95080001246e9ba30209dd1816eb4","value":"vocab.txt: 100%"}},"541662e910c540b78499d921c303267e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_76a76108e1cc43bbbf0bc2f534659b6a","max":239836,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6415f82c61384b709b3616597c4d5fc1","value":239836}},"2676a26d9b2b456c97b5b033d69565f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78bfe451044445d78cdc7687979d2035","placeholder":"​","style":"IPY_MODEL_7c21e291070248a4bd5c19db8797d960","value":" 240k/240k [00:00&lt;00:00, 1.63MB/s]"}},"21470fb9fb894ff79202686f97a0e20e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8774ceb09a244b438e9ab45375fdf6df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0b95080001246e9ba30209dd1816eb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76a76108e1cc43bbbf0bc2f534659b6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6415f82c61384b709b3616597c4d5fc1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78bfe451044445d78cdc7687979d2035":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c21e291070248a4bd5c19db8797d960":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce38598984a24f639b8b791e17d6e125":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_82574b3f3e8b483485e6d8ce68a3bf8b","IPY_MODEL_90eac4efc9924bdd932a7459531fbb4a","IPY_MODEL_7c66ef25d7224fa2bacf2370ce38b437"],"layout":"IPY_MODEL_377ff9020efb4b78a58c558e2b69bdb9"}},"82574b3f3e8b483485e6d8ce68a3bf8b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61c1437391cf419fad22ff93759c0aaa","placeholder":"​","style":"IPY_MODEL_68b90ca03e7f4785a72c6298bd42268d","value":"config.json: 100%"}},"90eac4efc9924bdd932a7459531fbb4a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc41f7d64fc7481fbb8afa84beec07c7","max":456,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b676c00631f4578bef8d462f52c81cc","value":456}},"7c66ef25d7224fa2bacf2370ce38b437":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58102bf528b34305af1864c1aeaab771","placeholder":"​","style":"IPY_MODEL_e2b17b701f2f44a39f2fb5cc5979a638","value":" 456/456 [00:00&lt;00:00, 38.4kB/s]"}},"377ff9020efb4b78a58c558e2b69bdb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61c1437391cf419fad22ff93759c0aaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68b90ca03e7f4785a72c6298bd42268d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc41f7d64fc7481fbb8afa84beec07c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b676c00631f4578bef8d462f52c81cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58102bf528b34305af1864c1aeaab771":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2b17b701f2f44a39f2fb5cc5979a638":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"472c244580824b2f83b019263019ecb4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27ad23feb6104e279004bc1733af3aac","IPY_MODEL_2403f764d76247eeb9dd27a5baabf879","IPY_MODEL_34fdb174596045b99419727e337354da"],"layout":"IPY_MODEL_788adcf6ea844825a96b5a8f61a48967"}},"27ad23feb6104e279004bc1733af3aac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1b2b5855cde4a09b87ef4d494453c09","placeholder":"​","style":"IPY_MODEL_c25c7200b37243aba2444e5bf59b3e0d","value":"model.safetensors: 100%"}},"2403f764d76247eeb9dd27a5baabf879":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ce7c7ed427b483b884f9c59a2afd82a","max":442227532,"min":0,"orientation":"horizontal","style":"IPY_MODEL_52cf9cabe72f4046926e2fe69cdbf3a8","value":442227532}},"34fdb174596045b99419727e337354da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4958be39f6ab45469bd8d5e011342718","placeholder":"​","style":"IPY_MODEL_6c05597806de4edeb8ceee08c3d2c68e","value":" 442M/442M [07:39&lt;00:00, 1.26MB/s]"}},"788adcf6ea844825a96b5a8f61a48967":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1b2b5855cde4a09b87ef4d494453c09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c25c7200b37243aba2444e5bf59b3e0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ce7c7ed427b483b884f9c59a2afd82a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52cf9cabe72f4046926e2fe69cdbf3a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4958be39f6ab45469bd8d5e011342718":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c05597806de4edeb8ceee08c3d2c68e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60924,"status":"ok","timestamp":1721761299178,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"},"user_tz":-180},"id":"X26qiUbJHNLn","outputId":"d3734a47-e7b5-4eb4-de00-1c4b38eec787"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install torch torchvision transformers pandas"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9767cf44ce11424eb05c342db7941901","ca96798e85f941a1a943b9584fd3f39f","9ab5a94db23e44b4a4257dec01c17391","d54cc7dab5e94d9a8b62f874e7772cb8","f4fdd536a2e843a789706c8b74fdc120","7f141a6fa33c40b59d5a24b6674f3938","c5872dfcd7294d5da856573e36215ac4","b7e7880648144cc7be60e412edbef8bb","16921677ac154c92b8c54be2b0a302ed","ca50b74b318b4c479e993034c800531f","9f14f6dd4800488888b931e81b2d35ee","d58f72417dbb400385aa794db657c814","7bec3de070564f6f97a75239eac67120","44905da1c1474222859f049f44c07de0","59d0aced5dfe470688d04009c5e7fa69","487f763d6e5e4913a0c24aa7d6fce4bf","71b64ccd2d0f484e857b14bbd0d03b71","073d21e8c2b14b73bfa053fcbd1a5728","21062d8abd404a1fbc17708e6bfdcef7","55ec8aab82e24b26ae2cba0cbac783be","a3dab1f579df48f0a0ca5bcb4c3a2543","70de54e4527d44b0a41d2f2f973254ae","1db9b6a669894f2bb053d6894cce7b92","b0e9b23b4613492093c49b9303409186","3da3623e7c004d379b5dbec3b9c8eeb0","46e4d75c23d547589222d86312eed842","b74dd5bfbd5b4518bb5d7d5735e817d1","9dd04a175a8c4878be305236ca7809db","6304a37a6cb2469784a196e1216cdc9d","57c9de7186e34152a422029528115811","73179f2542374297828a1e1cbdd22bcb","e5d05527e1d84b38bd1026223548e694","ffcf20d9230946a69711c84a56ab3aa6","cacbd4a001a44c7d851e3844b6766c16","8997628dba214924ae93e70723c9ebf6","7a16e59aad75424b9ea9b7de32040877","53aa52e61fc24f4bb8f9f058809871d9","db38ebba43934d8f8018288579364801","7ba247839aa74922818de5ea1538e492","4d21b4a1cce043f3a8f9ef59fb107968","13aa2fcf116544a2a946018a4384a979","eead383559f549ca9e37bca8fc810683","275b1ee961b94beeb17fef3350033917","b4eb59a88235492daeeff36142303e7f"]},"id":"pPSyyuKOEnOS","executionInfo":{"status":"ok","timestamp":1721762440959,"user_tz":-180,"elapsed":887430,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"}},"outputId":"6648cd72-cc8c-49ea-c4d2-8768e39d855e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting main process\n","Loading the dataset.\n","Dataset loaded successfully.\n","Cleaning the dataset.\n","Dataset cleaned.\n","Splitting the dataset into training and test sets.\n","Dataset split into training and test sets.\n","Using device: cuda\n","Loading the tokenizer and model.\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-6cb0d89f4911>:148: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['label'] = pd.to_numeric(df['label'], errors='coerce')\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9767cf44ce11424eb05c342db7941901"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/251k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d58f72417dbb400385aa794db657c814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1db9b6a669894f2bb053d6894cce7b92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cacbd4a001a44c7d851e3844b6766c16"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model and tokenizer loaded and moved to device.\n","Creating data loaders.\n","Data loaders created.\n","Optimizer, scheduler, and loss function defined.\n","Starting epoch 1/10\n","Batch 0/1002, Loss: 0.7004210352897644\n","Batch 10/1002, Loss: 0.444447785615921\n","Batch 20/1002, Loss: 0.593971312046051\n","Batch 30/1002, Loss: 0.5726823806762695\n","Batch 40/1002, Loss: 0.4198499321937561\n","Batch 50/1002, Loss: 0.5852428674697876\n","Batch 60/1002, Loss: 0.3933650851249695\n","Batch 70/1002, Loss: 0.19650280475616455\n","Batch 80/1002, Loss: 0.22967728972434998\n","Batch 90/1002, Loss: 0.35041287541389465\n","Batch 100/1002, Loss: 0.3824634850025177\n","Batch 110/1002, Loss: 0.1594526469707489\n","Batch 120/1002, Loss: 0.3360878825187683\n","Batch 130/1002, Loss: 0.2947680354118347\n","Batch 140/1002, Loss: 0.552290678024292\n","Batch 150/1002, Loss: 0.11938262730836868\n","Batch 160/1002, Loss: 0.12614761292934418\n","Batch 170/1002, Loss: 0.21840666234493256\n","Batch 180/1002, Loss: 0.2855603098869324\n","Batch 190/1002, Loss: 0.1474236249923706\n","Batch 200/1002, Loss: 0.36266016960144043\n","Batch 210/1002, Loss: 0.1970982402563095\n","Batch 220/1002, Loss: 0.38865959644317627\n","Batch 230/1002, Loss: 0.37587520480155945\n","Batch 240/1002, Loss: 0.14432533085346222\n","Batch 250/1002, Loss: 0.1315036118030548\n","Batch 260/1002, Loss: 0.1335497796535492\n","Batch 270/1002, Loss: 0.1548929363489151\n","Batch 280/1002, Loss: 0.21413756906986237\n","Batch 290/1002, Loss: 0.30934518575668335\n","Batch 300/1002, Loss: 0.10293102264404297\n","Batch 310/1002, Loss: 0.3614216446876526\n","Batch 320/1002, Loss: 0.35595378279685974\n","Batch 330/1002, Loss: 0.4292905032634735\n","Batch 340/1002, Loss: 0.14239650964736938\n","Batch 350/1002, Loss: 0.20941373705863953\n","Batch 360/1002, Loss: 0.3418658375740051\n","Batch 370/1002, Loss: 0.3135034143924713\n","Batch 380/1002, Loss: 0.214270681142807\n","Batch 390/1002, Loss: 0.14967027306556702\n","Batch 400/1002, Loss: 0.1366852968931198\n","Batch 410/1002, Loss: 0.026581663638353348\n","Batch 420/1002, Loss: 0.20260831713676453\n","Batch 430/1002, Loss: 0.052055831998586655\n","Batch 440/1002, Loss: 0.4969375729560852\n","Batch 450/1002, Loss: 0.35828080773353577\n","Batch 460/1002, Loss: 0.19018851220607758\n","Batch 470/1002, Loss: 0.2260112464427948\n","Batch 480/1002, Loss: 0.21265482902526855\n","Batch 490/1002, Loss: 0.09791812300682068\n","Batch 500/1002, Loss: 0.3308865427970886\n","Batch 510/1002, Loss: 0.3381192982196808\n","Batch 520/1002, Loss: 0.058428701013326645\n","Batch 530/1002, Loss: 0.044546227902173996\n","Batch 540/1002, Loss: 0.21593454480171204\n","Batch 550/1002, Loss: 0.21514436602592468\n","Batch 560/1002, Loss: 0.2463023066520691\n","Batch 570/1002, Loss: 0.08566369116306305\n","Batch 580/1002, Loss: 0.22262178361415863\n","Batch 590/1002, Loss: 0.4443865120410919\n","Batch 600/1002, Loss: 0.19981256127357483\n","Batch 610/1002, Loss: 0.23330870270729065\n","Batch 620/1002, Loss: 0.28054308891296387\n","Batch 630/1002, Loss: 0.1418202668428421\n","Batch 640/1002, Loss: 0.11511985212564468\n","Batch 650/1002, Loss: 0.3314363658428192\n","Batch 660/1002, Loss: 0.2160968780517578\n","Batch 670/1002, Loss: 0.16520212590694427\n","Batch 680/1002, Loss: 0.18854771554470062\n","Batch 690/1002, Loss: 0.41245102882385254\n","Batch 700/1002, Loss: 0.2843194603919983\n","Batch 710/1002, Loss: 0.3258233666419983\n","Batch 720/1002, Loss: 0.11796325445175171\n","Batch 730/1002, Loss: 0.07755100727081299\n","Batch 740/1002, Loss: 0.20793358981609344\n","Batch 750/1002, Loss: 0.20424483716487885\n","Batch 760/1002, Loss: 0.039127837866544724\n","Batch 770/1002, Loss: 0.036356404423713684\n","Batch 780/1002, Loss: 0.4155324697494507\n","Batch 790/1002, Loss: 0.12149089574813843\n","Batch 800/1002, Loss: 0.2115883231163025\n","Batch 810/1002, Loss: 0.3229905068874359\n","Batch 820/1002, Loss: 0.6503219604492188\n","Batch 830/1002, Loss: 0.07478896528482437\n","Batch 840/1002, Loss: 0.18069146573543549\n","Batch 850/1002, Loss: 0.18085907399654388\n","Batch 860/1002, Loss: 0.12784183025360107\n","Batch 870/1002, Loss: 0.16572608053684235\n","Batch 880/1002, Loss: 0.16114678978919983\n","Batch 890/1002, Loss: 0.624221682548523\n","Batch 900/1002, Loss: 0.14130538702011108\n","Batch 910/1002, Loss: 0.14700546860694885\n","Batch 920/1002, Loss: 0.13205359876155853\n","Batch 930/1002, Loss: 0.11108160018920898\n","Batch 940/1002, Loss: 0.17147907614707947\n","Batch 950/1002, Loss: 0.24720756709575653\n","Batch 960/1002, Loss: 0.26134300231933594\n","Batch 970/1002, Loss: 0.2866179049015045\n","Batch 980/1002, Loss: 0.2809444069862366\n","Batch 990/1002, Loss: 0.2777397036552429\n","Batch 1000/1002, Loss: 0.04995954781770706\n","Training epoch completed in: 1m 22s\n","Train loss 0.2372843111621703 accuracy 0.8953082106313951\n","Batch 0/251, Test Loss: 0.11418551951646805\n","Batch 10/251, Test Loss: 0.05573270842432976\n","Batch 20/251, Test Loss: 0.3179641366004944\n","Batch 30/251, Test Loss: 0.07009310275316238\n","Batch 40/251, Test Loss: 0.14507442712783813\n","Batch 50/251, Test Loss: 0.07314758747816086\n","Batch 60/251, Test Loss: 0.1399662345647812\n","Batch 70/251, Test Loss: 0.28100693225860596\n","Batch 80/251, Test Loss: 0.3940499722957611\n","Batch 90/251, Test Loss: 0.1905241310596466\n","Batch 100/251, Test Loss: 0.013419230468571186\n","Batch 110/251, Test Loss: 0.11864729970693588\n","Batch 120/251, Test Loss: 0.402386873960495\n","Batch 130/251, Test Loss: 0.031503308564424515\n","Batch 140/251, Test Loss: 0.059028804302215576\n","Batch 150/251, Test Loss: 0.1593477725982666\n","Batch 160/251, Test Loss: 0.5583282113075256\n","Batch 170/251, Test Loss: 0.10173726081848145\n","Batch 180/251, Test Loss: 0.1324092596769333\n","Batch 190/251, Test Loss: 0.23333866894245148\n","Batch 200/251, Test Loss: 0.6175434589385986\n","Batch 210/251, Test Loss: 0.19616103172302246\n","Batch 220/251, Test Loss: 0.1874697059392929\n","Batch 230/251, Test Loss: 0.1155402809381485\n","Batch 240/251, Test Loss: 0.03234449028968811\n","Batch 250/251, Test Loss: 0.13323856890201569\n","Test evaluation completed in: 0m 7s\n","Test loss 0.17953208272170973 accuracy 0.9236526946107784\n","Test Precision: 0.9239165858563911\n","Test Recall: 0.9236526946107785\n","Test F1 Score: 0.9236408110692085\n","Starting epoch 2/10\n","Batch 0/1002, Loss: 0.12479858100414276\n","Batch 10/1002, Loss: 0.152619406580925\n","Batch 20/1002, Loss: 0.34417176246643066\n","Batch 30/1002, Loss: 0.10488548874855042\n","Batch 40/1002, Loss: 0.3761679232120514\n","Batch 50/1002, Loss: 0.35707780718803406\n","Batch 60/1002, Loss: 0.24501824378967285\n","Batch 70/1002, Loss: 0.08438972383737564\n","Batch 80/1002, Loss: 0.3189932405948639\n","Batch 90/1002, Loss: 0.23292505741119385\n","Batch 100/1002, Loss: 0.15241895616054535\n","Batch 110/1002, Loss: 0.08112556487321854\n","Batch 120/1002, Loss: 0.2987423539161682\n","Batch 130/1002, Loss: 0.1687714010477066\n","Batch 140/1002, Loss: 0.4164019823074341\n","Batch 150/1002, Loss: 0.08772151917219162\n","Batch 160/1002, Loss: 0.15508563816547394\n","Batch 170/1002, Loss: 0.07594433426856995\n","Batch 180/1002, Loss: 0.21927335858345032\n","Batch 190/1002, Loss: 0.0813390240073204\n","Batch 200/1002, Loss: 0.22047486901283264\n","Batch 210/1002, Loss: 0.07923194766044617\n","Batch 220/1002, Loss: 0.3888785243034363\n","Batch 230/1002, Loss: 0.10116442292928696\n","Batch 240/1002, Loss: 0.06363604962825775\n","Batch 250/1002, Loss: 0.13166962563991547\n","Batch 260/1002, Loss: 0.03630577027797699\n","Batch 270/1002, Loss: 0.16883446276187897\n","Batch 280/1002, Loss: 0.03219524770975113\n","Batch 290/1002, Loss: 0.19003814458847046\n","Batch 300/1002, Loss: 0.08441523462533951\n","Batch 310/1002, Loss: 0.13359695672988892\n","Batch 320/1002, Loss: 0.13515590131282806\n","Batch 330/1002, Loss: 0.22314275801181793\n","Batch 340/1002, Loss: 0.1154155284166336\n","Batch 350/1002, Loss: 0.17940986156463623\n","Batch 360/1002, Loss: 0.25918322801589966\n","Batch 370/1002, Loss: 0.06251540780067444\n","Batch 380/1002, Loss: 0.17911672592163086\n","Batch 390/1002, Loss: 0.09558671712875366\n","Batch 400/1002, Loss: 0.10648597776889801\n","Batch 410/1002, Loss: 0.027844659984111786\n","Batch 420/1002, Loss: 0.16741323471069336\n","Batch 430/1002, Loss: 0.07022236287593842\n","Batch 440/1002, Loss: 0.4904831349849701\n","Batch 450/1002, Loss: 0.12626102566719055\n","Batch 460/1002, Loss: 0.13107168674468994\n","Batch 470/1002, Loss: 0.206650510430336\n","Batch 480/1002, Loss: 0.17224080860614777\n","Batch 490/1002, Loss: 0.026825040578842163\n","Batch 500/1002, Loss: 0.15003609657287598\n","Batch 510/1002, Loss: 0.1454954594373703\n","Batch 520/1002, Loss: 0.01103039737790823\n","Batch 530/1002, Loss: 0.03593926504254341\n","Batch 540/1002, Loss: 0.07111609727144241\n","Batch 550/1002, Loss: 0.13202477991580963\n","Batch 560/1002, Loss: 0.14775392413139343\n","Batch 570/1002, Loss: 0.024311281740665436\n","Batch 580/1002, Loss: 0.07379862666130066\n","Batch 590/1002, Loss: 0.3767443001270294\n","Batch 600/1002, Loss: 0.05338763818144798\n","Batch 610/1002, Loss: 0.05519983172416687\n","Batch 620/1002, Loss: 0.16803988814353943\n","Batch 630/1002, Loss: 0.03843798115849495\n","Batch 640/1002, Loss: 0.06604531407356262\n","Batch 650/1002, Loss: 0.2067144215106964\n","Batch 660/1002, Loss: 0.15487249195575714\n","Batch 670/1002, Loss: 0.10544728487730026\n","Batch 680/1002, Loss: 0.16127590835094452\n","Batch 690/1002, Loss: 0.1640135943889618\n","Batch 700/1002, Loss: 0.1451159417629242\n","Batch 710/1002, Loss: 0.237850159406662\n","Batch 720/1002, Loss: 0.031005190685391426\n","Batch 730/1002, Loss: 0.015966376289725304\n","Batch 740/1002, Loss: 0.14702960848808289\n","Batch 750/1002, Loss: 0.13861913979053497\n","Batch 760/1002, Loss: 0.0034214507322758436\n","Batch 770/1002, Loss: 0.006613020319491625\n","Batch 780/1002, Loss: 0.304218590259552\n","Batch 790/1002, Loss: 0.08866455405950546\n","Batch 800/1002, Loss: 0.02408214658498764\n","Batch 810/1002, Loss: 0.21478092670440674\n","Batch 820/1002, Loss: 0.5619426965713501\n","Batch 830/1002, Loss: 0.16073647141456604\n","Batch 840/1002, Loss: 0.11548339575529099\n","Batch 850/1002, Loss: 0.08578097820281982\n","Batch 860/1002, Loss: 0.08649475127458572\n","Batch 870/1002, Loss: 0.0885448083281517\n","Batch 880/1002, Loss: 0.14773058891296387\n","Batch 890/1002, Loss: 0.28070369362831116\n","Batch 900/1002, Loss: 0.06703515350818634\n","Batch 910/1002, Loss: 0.04874420911073685\n","Batch 920/1002, Loss: 0.11393799632787704\n","Batch 930/1002, Loss: 0.14285308122634888\n","Batch 940/1002, Loss: 0.2299744188785553\n","Batch 950/1002, Loss: 0.16367165744304657\n","Batch 960/1002, Loss: 0.07527557760477066\n","Batch 970/1002, Loss: 0.3073101043701172\n","Batch 980/1002, Loss: 0.16274136304855347\n","Batch 990/1002, Loss: 0.1922864466905594\n","Batch 1000/1002, Loss: 0.025984596461057663\n","Training epoch completed in: 1m 21s\n","Train loss 0.1385221729469123 accuracy 0.9417269777888696\n","Batch 0/251, Test Loss: 0.13993844389915466\n","Batch 10/251, Test Loss: 0.03942326456308365\n","Batch 20/251, Test Loss: 0.17492452263832092\n","Batch 30/251, Test Loss: 0.029457010328769684\n","Batch 40/251, Test Loss: 0.17144617438316345\n","Batch 50/251, Test Loss: 0.1407535821199417\n","Batch 60/251, Test Loss: 0.16886016726493835\n","Batch 70/251, Test Loss: 0.41973990201950073\n","Batch 80/251, Test Loss: 0.4103090465068817\n","Batch 90/251, Test Loss: 0.09744471311569214\n","Batch 100/251, Test Loss: 0.003812708891928196\n","Batch 110/251, Test Loss: 0.0426718071103096\n","Batch 120/251, Test Loss: 0.4885738492012024\n","Batch 130/251, Test Loss: 0.017105117440223694\n","Batch 140/251, Test Loss: 0.04158347472548485\n","Batch 150/251, Test Loss: 0.21182623505592346\n","Batch 160/251, Test Loss: 0.7944098114967346\n","Batch 170/251, Test Loss: 0.06038983166217804\n","Batch 180/251, Test Loss: 0.1262664496898651\n","Batch 190/251, Test Loss: 0.3092292547225952\n","Batch 200/251, Test Loss: 0.7952521443367004\n","Batch 210/251, Test Loss: 0.19225424528121948\n","Batch 220/251, Test Loss: 0.2630767524242401\n","Batch 230/251, Test Loss: 0.04625742509961128\n","Batch 240/251, Test Loss: 0.01164256501942873\n","Batch 250/251, Test Loss: 0.04356936737895012\n","Test evaluation completed in: 0m 7s\n","Test loss 0.18802878943056728 accuracy 0.9256487025948102\n","Test Precision: 0.9256487025948104\n","Test Recall: 0.9256487025948104\n","Test F1 Score: 0.9256487025948104\n","Starting epoch 3/10\n","Batch 0/1002, Loss: 0.16511693596839905\n","Batch 10/1002, Loss: 0.10837391018867493\n","Batch 20/1002, Loss: 0.3524772822856903\n","Batch 30/1002, Loss: 0.054805636405944824\n","Batch 40/1002, Loss: 0.1243179589509964\n","Batch 50/1002, Loss: 0.3228466808795929\n","Batch 60/1002, Loss: 0.1265079230070114\n","Batch 70/1002, Loss: 0.018058620393276215\n","Batch 80/1002, Loss: 0.14118270576000214\n","Batch 90/1002, Loss: 0.10583310574293137\n","Batch 100/1002, Loss: 0.11539173871278763\n","Batch 110/1002, Loss: 0.0028768847696483135\n","Batch 120/1002, Loss: 0.24399563670158386\n","Batch 130/1002, Loss: 0.03368552774190903\n","Batch 140/1002, Loss: 0.14382044970989227\n","Batch 150/1002, Loss: 0.012290716171264648\n","Batch 160/1002, Loss: 0.1476185917854309\n","Batch 170/1002, Loss: 0.08165621757507324\n","Batch 180/1002, Loss: 0.12829411029815674\n","Batch 190/1002, Loss: 0.0991610437631607\n","Batch 200/1002, Loss: 0.2715604603290558\n","Batch 210/1002, Loss: 0.04505256190896034\n","Batch 220/1002, Loss: 0.042559947818517685\n","Batch 230/1002, Loss: 0.14279301464557648\n","Batch 240/1002, Loss: 0.019862335175275803\n","Batch 250/1002, Loss: 0.12019047141075134\n","Batch 260/1002, Loss: 0.007917927578091621\n","Batch 270/1002, Loss: 0.16517777740955353\n","Batch 280/1002, Loss: 0.13309800624847412\n","Batch 290/1002, Loss: 0.07565820217132568\n","Batch 300/1002, Loss: 0.015066205523908138\n","Batch 310/1002, Loss: 0.03295911103487015\n","Batch 320/1002, Loss: 0.05068592354655266\n","Batch 330/1002, Loss: 0.17136439681053162\n","Batch 340/1002, Loss: 0.01667804829776287\n","Batch 350/1002, Loss: 0.02232462912797928\n","Batch 360/1002, Loss: 0.018251484259963036\n","Batch 370/1002, Loss: 0.088034488260746\n","Batch 380/1002, Loss: 0.008259132504463196\n","Batch 390/1002, Loss: 0.0257667638361454\n","Batch 400/1002, Loss: 0.05753009021282196\n","Batch 410/1002, Loss: 0.005560182500630617\n","Batch 420/1002, Loss: 0.17425337433815002\n","Batch 430/1002, Loss: 0.0687161237001419\n","Batch 440/1002, Loss: 0.02436027303338051\n","Batch 450/1002, Loss: 0.006705194246023893\n","Batch 460/1002, Loss: 0.08329048752784729\n","Batch 470/1002, Loss: 0.03895227238535881\n","Batch 480/1002, Loss: 0.05614105612039566\n","Batch 490/1002, Loss: 0.019670534878969193\n","Batch 500/1002, Loss: 0.05890779569745064\n","Batch 510/1002, Loss: 0.14718066155910492\n","Batch 520/1002, Loss: 0.001840273616835475\n","Batch 530/1002, Loss: 0.0052377646788954735\n","Batch 540/1002, Loss: 0.027660081163048744\n","Batch 550/1002, Loss: 0.07771903276443481\n","Batch 560/1002, Loss: 0.10095538198947906\n","Batch 570/1002, Loss: 0.1054312214255333\n","Batch 580/1002, Loss: 0.11759690195322037\n","Batch 590/1002, Loss: 0.15839612483978271\n","Batch 600/1002, Loss: 0.10880386829376221\n","Batch 610/1002, Loss: 0.08281124383211136\n","Batch 620/1002, Loss: 0.029399821534752846\n","Batch 630/1002, Loss: 0.002043041866272688\n","Batch 640/1002, Loss: 0.01783427782356739\n","Batch 650/1002, Loss: 0.012776377610862255\n","Batch 660/1002, Loss: 0.07687158137559891\n","Batch 670/1002, Loss: 0.16242821514606476\n","Batch 680/1002, Loss: 0.027986843138933182\n","Batch 690/1002, Loss: 0.10404819995164871\n","Batch 700/1002, Loss: 0.061759866774082184\n","Batch 710/1002, Loss: 0.03223784267902374\n","Batch 720/1002, Loss: 0.04136708751320839\n","Batch 730/1002, Loss: 0.001280703116208315\n","Batch 740/1002, Loss: 0.019173461943864822\n","Batch 750/1002, Loss: 0.055822912603616714\n","Batch 760/1002, Loss: 0.010045594535768032\n","Batch 770/1002, Loss: 0.0006404721061699092\n","Batch 780/1002, Loss: 0.051987189799547195\n","Batch 790/1002, Loss: 0.17294776439666748\n","Batch 800/1002, Loss: 0.0009426596225239336\n","Batch 810/1002, Loss: 0.07280091941356659\n","Batch 820/1002, Loss: 0.22131870687007904\n","Batch 830/1002, Loss: 0.04675733298063278\n","Batch 840/1002, Loss: 0.09368567913770676\n","Batch 850/1002, Loss: 0.2656265199184418\n","Batch 860/1002, Loss: 0.03944791108369827\n","Batch 870/1002, Loss: 0.14220282435417175\n","Batch 880/1002, Loss: 0.1539631187915802\n","Batch 890/1002, Loss: 0.03925025835633278\n","Batch 900/1002, Loss: 0.07433541119098663\n","Batch 910/1002, Loss: 0.004297416657209396\n","Batch 920/1002, Loss: 0.22666805982589722\n","Batch 930/1002, Loss: 0.06644254177808762\n","Batch 940/1002, Loss: 0.10903645306825638\n","Batch 950/1002, Loss: 0.0037041467148810625\n","Batch 960/1002, Loss: 0.09025131165981293\n","Batch 970/1002, Loss: 0.18158648908138275\n","Batch 980/1002, Loss: 0.05546034872531891\n","Batch 990/1002, Loss: 0.1643269956111908\n","Batch 1000/1002, Loss: 0.0295290295034647\n","Training epoch completed in: 1m 21s\n","Train loss 0.07795256548809787 accuracy 0.9704891439980036\n","Batch 0/251, Test Loss: 0.31377002596855164\n","Batch 10/251, Test Loss: 0.05353087931871414\n","Batch 20/251, Test Loss: 0.1137072965502739\n","Batch 30/251, Test Loss: 0.011079167015850544\n","Batch 40/251, Test Loss: 0.11515426635742188\n","Batch 50/251, Test Loss: 0.22023726999759674\n","Batch 60/251, Test Loss: 0.25256192684173584\n","Batch 70/251, Test Loss: 0.40082162618637085\n","Batch 80/251, Test Loss: 0.5001234412193298\n","Batch 90/251, Test Loss: 0.22388409078121185\n","Batch 100/251, Test Loss: 0.002244848757982254\n","Batch 110/251, Test Loss: 0.02123858407139778\n","Batch 120/251, Test Loss: 0.5685632824897766\n","Batch 130/251, Test Loss: 0.007273987866938114\n","Batch 140/251, Test Loss: 0.02079889178276062\n","Batch 150/251, Test Loss: 0.30145934224128723\n","Batch 160/251, Test Loss: 0.9183568954467773\n","Batch 170/251, Test Loss: 0.0878492072224617\n","Batch 180/251, Test Loss: 0.20329658687114716\n","Batch 190/251, Test Loss: 0.3336688280105591\n","Batch 200/251, Test Loss: 0.8859689831733704\n","Batch 210/251, Test Loss: 0.21790841221809387\n","Batch 220/251, Test Loss: 0.33541634678840637\n","Batch 230/251, Test Loss: 0.020342597737908363\n","Batch 240/251, Test Loss: 0.008269647136330605\n","Batch 250/251, Test Loss: 0.026176156476140022\n","Test evaluation completed in: 0m 7s\n","Test loss 0.2267175366243058 accuracy 0.9174151696606786\n","Test Precision: 0.9184781374077435\n","Test Recall: 0.9174151696606786\n","Test F1 Score: 0.9173626933280795\n","Starting epoch 4/10\n","Batch 0/1002, Loss: 0.167271226644516\n","Batch 10/1002, Loss: 0.10399371385574341\n","Batch 20/1002, Loss: 0.2493758350610733\n","Batch 30/1002, Loss: 0.08727185428142548\n","Batch 40/1002, Loss: 0.029122618958353996\n","Batch 50/1002, Loss: 0.0836060419678688\n","Batch 60/1002, Loss: 0.010988184250891209\n","Batch 70/1002, Loss: 0.009374942630529404\n","Batch 80/1002, Loss: 0.16261553764343262\n","Batch 90/1002, Loss: 0.08594875037670135\n","Batch 100/1002, Loss: 0.03415963053703308\n","Batch 110/1002, Loss: 0.0006965457578189671\n","Batch 120/1002, Loss: 0.06422179192304611\n","Batch 130/1002, Loss: 0.058411605656147\n","Batch 140/1002, Loss: 0.04503609612584114\n","Batch 150/1002, Loss: 0.012273639440536499\n","Batch 160/1002, Loss: 0.03249265253543854\n","Batch 170/1002, Loss: 0.0010127576533704996\n","Batch 180/1002, Loss: 0.01578507199883461\n","Batch 190/1002, Loss: 0.019732916727662086\n","Batch 200/1002, Loss: 0.004547011107206345\n","Batch 210/1002, Loss: 0.011669235303997993\n","Batch 220/1002, Loss: 0.03104434907436371\n","Batch 230/1002, Loss: 0.17346154153347015\n","Batch 240/1002, Loss: 0.0004980588564649224\n","Batch 250/1002, Loss: 0.03083665855228901\n","Batch 260/1002, Loss: 0.0011604760074988008\n","Batch 270/1002, Loss: 0.03398767486214638\n","Batch 280/1002, Loss: 0.0006307492149062455\n","Batch 290/1002, Loss: 0.20983390510082245\n","Batch 300/1002, Loss: 0.000612524978350848\n","Batch 310/1002, Loss: 0.12380219250917435\n","Batch 320/1002, Loss: 0.20595970749855042\n","Batch 330/1002, Loss: 0.05836368352174759\n","Batch 340/1002, Loss: 0.020509464666247368\n","Batch 350/1002, Loss: 0.01371967326849699\n","Batch 360/1002, Loss: 0.3655974864959717\n","Batch 370/1002, Loss: 0.025528810918331146\n","Batch 380/1002, Loss: 0.15646366775035858\n","Batch 390/1002, Loss: 0.031513191759586334\n","Batch 400/1002, Loss: 0.01931305229663849\n","Batch 410/1002, Loss: 0.004544316790997982\n","Batch 420/1002, Loss: 0.04625038430094719\n","Batch 430/1002, Loss: 0.05167580768465996\n","Batch 440/1002, Loss: 0.0025270050391554832\n","Batch 450/1002, Loss: 0.0018893497763201594\n","Batch 460/1002, Loss: 0.041401736438274384\n","Batch 470/1002, Loss: 0.02156970091164112\n","Batch 480/1002, Loss: 0.0051722112111747265\n","Batch 490/1002, Loss: 0.001622167183086276\n","Batch 500/1002, Loss: 0.024299610406160355\n","Batch 510/1002, Loss: 0.07512722164392471\n","Batch 520/1002, Loss: 0.0007741354056634009\n","Batch 530/1002, Loss: 0.005632148124277592\n","Batch 540/1002, Loss: 0.023560794070363045\n","Batch 550/1002, Loss: 0.011710786260664463\n","Batch 560/1002, Loss: 0.1186431497335434\n","Batch 570/1002, Loss: 0.0013052566209807992\n","Batch 580/1002, Loss: 0.025000566616654396\n","Batch 590/1002, Loss: 0.007050303276628256\n","Batch 600/1002, Loss: 0.003934404347091913\n","Batch 610/1002, Loss: 0.00917738676071167\n","Batch 620/1002, Loss: 0.4222753942012787\n","Batch 630/1002, Loss: 0.001876994501799345\n","Batch 640/1002, Loss: 0.030061611905694008\n","Batch 650/1002, Loss: 0.055313993245363235\n","Batch 660/1002, Loss: 0.1299457550048828\n","Batch 670/1002, Loss: 0.09872449189424515\n","Batch 680/1002, Loss: 0.09650083631277084\n","Batch 690/1002, Loss: 0.018737200647592545\n","Batch 700/1002, Loss: 0.013548486866056919\n","Batch 710/1002, Loss: 0.031053228303790092\n","Batch 720/1002, Loss: 0.005548667162656784\n","Batch 730/1002, Loss: 0.0025576576590538025\n","Batch 740/1002, Loss: 0.014036218635737896\n","Batch 750/1002, Loss: 0.04237262159585953\n","Batch 760/1002, Loss: 0.0004314691759645939\n","Batch 770/1002, Loss: 0.0006476609851233661\n","Batch 780/1002, Loss: 0.09096457809209824\n","Batch 790/1002, Loss: 0.04528094455599785\n","Batch 800/1002, Loss: 0.00988813117146492\n","Batch 810/1002, Loss: 0.11022573709487915\n","Batch 820/1002, Loss: 0.003401131136342883\n","Batch 830/1002, Loss: 0.0013154870830476284\n","Batch 840/1002, Loss: 0.010968123562633991\n","Batch 850/1002, Loss: 0.03060242347419262\n","Batch 860/1002, Loss: 0.277009516954422\n","Batch 870/1002, Loss: 0.07291951030492783\n","Batch 880/1002, Loss: 0.03253626450896263\n","Batch 890/1002, Loss: 0.017198726534843445\n","Batch 900/1002, Loss: 0.0039122928865253925\n","Batch 910/1002, Loss: 0.006088209338486195\n","Batch 920/1002, Loss: 0.04199165478348732\n","Batch 930/1002, Loss: 0.004053014796227217\n","Batch 940/1002, Loss: 0.016908088698983192\n","Batch 950/1002, Loss: 0.004852086305618286\n","Batch 960/1002, Loss: 0.002788207959383726\n","Batch 970/1002, Loss: 0.10569246858358383\n","Batch 980/1002, Loss: 0.019871186465024948\n","Batch 990/1002, Loss: 0.008198477327823639\n","Batch 1000/1002, Loss: 0.006178613286465406\n","Training epoch completed in: 1m 21s\n","Train loss 0.045875936487451204 accuracy 0.9837159970052409\n","Batch 0/251, Test Loss: 0.21840523183345795\n","Batch 10/251, Test Loss: 0.003929073456674814\n","Batch 20/251, Test Loss: 0.17256289720535278\n","Batch 30/251, Test Loss: 0.005655704531818628\n","Batch 40/251, Test Loss: 0.409397155046463\n","Batch 50/251, Test Loss: 0.3159896731376648\n","Batch 60/251, Test Loss: 0.3794811964035034\n","Batch 70/251, Test Loss: 0.5141206383705139\n","Batch 80/251, Test Loss: 0.5460148453712463\n","Batch 90/251, Test Loss: 0.05296702682971954\n","Batch 100/251, Test Loss: 0.0003217576304450631\n","Batch 110/251, Test Loss: 0.019403204321861267\n","Batch 120/251, Test Loss: 0.7532867789268494\n","Batch 130/251, Test Loss: 0.0018174074357375503\n","Batch 140/251, Test Loss: 0.006324297748506069\n","Batch 150/251, Test Loss: 0.13785463571548462\n","Batch 160/251, Test Loss: 1.4061863422393799\n","Batch 170/251, Test Loss: 0.12627428770065308\n","Batch 180/251, Test Loss: 0.3414055407047272\n","Batch 190/251, Test Loss: 0.38379958271980286\n","Batch 200/251, Test Loss: 1.1989234685897827\n","Batch 210/251, Test Loss: 0.3954085111618042\n","Batch 220/251, Test Loss: 0.47721707820892334\n","Batch 230/251, Test Loss: 0.050927311182022095\n","Batch 240/251, Test Loss: 0.0007490264833904803\n","Batch 250/251, Test Loss: 0.006973632611334324\n","Test evaluation completed in: 0m 7s\n","Test loss 0.2767950796233017 accuracy 0.9231536926147704\n","Test Precision: 0.9233965964673101\n","Test Recall: 0.9231536926147704\n","Test F1 Score: 0.9231426692965155\n","Starting epoch 5/10\n","Batch 0/1002, Loss: 0.10196717083454132\n","Batch 10/1002, Loss: 0.06303822249174118\n","Batch 20/1002, Loss: 0.21955455839633942\n","Batch 30/1002, Loss: 0.03840235248208046\n","Batch 40/1002, Loss: 0.04341263696551323\n","Batch 50/1002, Loss: 0.03365783393383026\n","Batch 60/1002, Loss: 0.006966185290366411\n","Batch 70/1002, Loss: 0.0029193214140832424\n","Batch 80/1002, Loss: 0.043326571583747864\n","Batch 90/1002, Loss: 0.034424908459186554\n","Batch 100/1002, Loss: 0.005045407917350531\n","Batch 110/1002, Loss: 0.011041127145290375\n","Batch 120/1002, Loss: 0.024300172924995422\n","Batch 130/1002, Loss: 0.0005350414430722594\n","Batch 140/1002, Loss: 0.019955217838287354\n","Batch 150/1002, Loss: 0.012486176565289497\n","Batch 160/1002, Loss: 0.005387842655181885\n","Batch 170/1002, Loss: 0.0013051290297880769\n","Batch 180/1002, Loss: 0.025557102635502815\n","Batch 190/1002, Loss: 0.005629625637084246\n","Batch 200/1002, Loss: 0.0016430870164185762\n","Batch 210/1002, Loss: 0.005116643849760294\n","Batch 220/1002, Loss: 0.047795090824365616\n","Batch 230/1002, Loss: 0.01224942784756422\n","Batch 240/1002, Loss: 0.00025636525242589414\n","Batch 250/1002, Loss: 0.000920570979360491\n","Batch 260/1002, Loss: 0.0020607481710612774\n","Batch 270/1002, Loss: 0.001344975084066391\n","Batch 280/1002, Loss: 0.00040803314186632633\n","Batch 290/1002, Loss: 0.08670417964458466\n","Batch 300/1002, Loss: 0.0003593744186218828\n","Batch 310/1002, Loss: 0.15839821100234985\n","Batch 320/1002, Loss: 0.0007391452672891319\n","Batch 330/1002, Loss: 0.22507700324058533\n","Batch 340/1002, Loss: 0.012950927019119263\n","Batch 350/1002, Loss: 0.03652007132768631\n","Batch 360/1002, Loss: 0.008449182845652103\n","Batch 370/1002, Loss: 0.22642692923545837\n","Batch 380/1002, Loss: 0.0009721645037643611\n","Batch 390/1002, Loss: 0.027186350896954536\n","Batch 400/1002, Loss: 0.034469086676836014\n","Batch 410/1002, Loss: 0.00040505462675355375\n","Batch 420/1002, Loss: 0.03714989498257637\n","Batch 430/1002, Loss: 0.009802253916859627\n","Batch 440/1002, Loss: 0.004414774943143129\n","Batch 450/1002, Loss: 0.038641415536403656\n","Batch 460/1002, Loss: 0.03701300919055939\n","Batch 470/1002, Loss: 0.10124389827251434\n","Batch 480/1002, Loss: 0.013215295970439911\n","Batch 490/1002, Loss: 0.0006894212565384805\n","Batch 500/1002, Loss: 0.032709844410419464\n","Batch 510/1002, Loss: 0.007984587922692299\n","Batch 520/1002, Loss: 0.0003107006778009236\n","Batch 530/1002, Loss: 0.003977680578827858\n","Batch 540/1002, Loss: 0.0009320955141447484\n","Batch 550/1002, Loss: 0.0420842282474041\n","Batch 560/1002, Loss: 0.027256691828370094\n","Batch 570/1002, Loss: 0.0011430119629949331\n","Batch 580/1002, Loss: 0.026046667248010635\n","Batch 590/1002, Loss: 0.006040248554199934\n","Batch 600/1002, Loss: 0.06346984207630157\n","Batch 610/1002, Loss: 0.0027575362473726273\n","Batch 620/1002, Loss: 0.21961340308189392\n","Batch 630/1002, Loss: 0.008775182068347931\n","Batch 640/1002, Loss: 0.14499402046203613\n","Batch 650/1002, Loss: 0.0011461912654340267\n","Batch 660/1002, Loss: 0.012414932250976562\n","Batch 670/1002, Loss: 0.21383175253868103\n","Batch 680/1002, Loss: 0.0010802989127114415\n","Batch 690/1002, Loss: 0.0004904701490886509\n","Batch 700/1002, Loss: 0.004614230711013079\n","Batch 710/1002, Loss: 0.04618428274989128\n","Batch 720/1002, Loss: 0.004644378088414669\n","Batch 730/1002, Loss: 0.015443278476595879\n","Batch 740/1002, Loss: 0.005881951190531254\n","Batch 750/1002, Loss: 0.004786127246916294\n","Batch 760/1002, Loss: 0.0005033854395151138\n","Batch 770/1002, Loss: 0.00034090597182512283\n","Batch 780/1002, Loss: 0.008686864748597145\n","Batch 790/1002, Loss: 0.0012561808107420802\n","Batch 800/1002, Loss: 0.0008474611677229404\n","Batch 810/1002, Loss: 0.012133193202316761\n","Batch 820/1002, Loss: 0.06264903396368027\n","Batch 830/1002, Loss: 0.00039071307401172817\n","Batch 840/1002, Loss: 0.012135081924498081\n","Batch 850/1002, Loss: 0.0011846997076645494\n","Batch 860/1002, Loss: 0.003232508897781372\n","Batch 870/1002, Loss: 0.0014569751219823956\n","Batch 880/1002, Loss: 0.0046829781495034695\n","Batch 890/1002, Loss: 0.013376639224588871\n","Batch 900/1002, Loss: 0.07686436921358109\n","Batch 910/1002, Loss: 0.00072694435948506\n","Batch 920/1002, Loss: 0.18433961272239685\n","Batch 930/1002, Loss: 0.004395422991365194\n","Batch 940/1002, Loss: 0.08107578754425049\n","Batch 950/1002, Loss: 0.0007926615071482956\n","Batch 960/1002, Loss: 0.006373370997607708\n","Batch 970/1002, Loss: 0.0011176456464454532\n","Batch 980/1002, Loss: 0.060228168964385986\n","Batch 990/1002, Loss: 0.0061669787392020226\n","Batch 1000/1002, Loss: 0.0025747991167008877\n","Training epoch completed in: 1m 21s\n","Train loss 0.028965795975159424 accuracy 0.9892063888195658\n","Batch 0/251, Test Loss: 0.46941033005714417\n","Batch 10/251, Test Loss: 0.004864008631557226\n","Batch 20/251, Test Loss: 0.03412783890962601\n","Batch 30/251, Test Loss: 0.00268185930326581\n","Batch 40/251, Test Loss: 0.3593607544898987\n","Batch 50/251, Test Loss: 0.28841105103492737\n","Batch 60/251, Test Loss: 0.4454039931297302\n","Batch 70/251, Test Loss: 0.5675309896469116\n","Batch 80/251, Test Loss: 0.5864235758781433\n","Batch 90/251, Test Loss: 0.5174910426139832\n","Batch 100/251, Test Loss: 0.00014791620196774602\n","Batch 110/251, Test Loss: 0.014632395468652248\n","Batch 120/251, Test Loss: 1.0095375776290894\n","Batch 130/251, Test Loss: 0.0007473650039173663\n","Batch 140/251, Test Loss: 0.03125650808215141\n","Batch 150/251, Test Loss: 0.32228440046310425\n","Batch 160/251, Test Loss: 1.7708265781402588\n","Batch 170/251, Test Loss: 0.13120020925998688\n","Batch 180/251, Test Loss: 0.36241358518600464\n","Batch 190/251, Test Loss: 0.3674723505973816\n","Batch 200/251, Test Loss: 1.1224125623703003\n","Batch 210/251, Test Loss: 0.6309378147125244\n","Batch 220/251, Test Loss: 0.5207933783531189\n","Batch 230/251, Test Loss: 0.0009993737330660224\n","Batch 240/251, Test Loss: 0.0009932256070896983\n","Batch 250/251, Test Loss: 0.0009240983636118472\n","Test evaluation completed in: 0m 7s\n","Test loss 0.33152779900659973 accuracy 0.9196606786427145\n","Test Precision: 0.9205860480437614\n","Test Recall: 0.9196606786427146\n","Test F1 Score: 0.9196164638708337\n","Starting epoch 6/10\n","Batch 0/1002, Loss: 0.018757034093141556\n","Batch 10/1002, Loss: 0.0163313839584589\n","Batch 20/1002, Loss: 0.06260072439908981\n","Batch 30/1002, Loss: 0.005437966901808977\n","Batch 40/1002, Loss: 0.018873929977416992\n","Batch 50/1002, Loss: 0.12955500185489655\n","Batch 60/1002, Loss: 0.0033623226918280125\n","Batch 70/1002, Loss: 0.011259579099714756\n","Batch 80/1002, Loss: 0.0030481477733701468\n","Batch 90/1002, Loss: 0.1264127790927887\n","Batch 100/1002, Loss: 0.003478549886494875\n","Batch 110/1002, Loss: 0.00034461854374967515\n","Batch 120/1002, Loss: 0.007442650385200977\n","Batch 130/1002, Loss: 0.0006523496122099459\n","Batch 140/1002, Loss: 0.03935840725898743\n","Batch 150/1002, Loss: 0.0013007386587560177\n","Batch 160/1002, Loss: 0.12757159769535065\n","Batch 170/1002, Loss: 0.0003202596271876246\n","Batch 180/1002, Loss: 0.002581428736448288\n","Batch 190/1002, Loss: 0.21199627220630646\n","Batch 200/1002, Loss: 0.002782069845125079\n","Batch 210/1002, Loss: 0.04072818532586098\n","Batch 220/1002, Loss: 0.0014168020570650697\n","Batch 230/1002, Loss: 0.0037014929112046957\n","Batch 240/1002, Loss: 0.00015460899157915264\n","Batch 250/1002, Loss: 0.000752636231482029\n","Batch 260/1002, Loss: 0.0005538755794987082\n","Batch 270/1002, Loss: 0.0179092139005661\n","Batch 280/1002, Loss: 0.008424545638263226\n","Batch 290/1002, Loss: 0.05221013352274895\n","Batch 300/1002, Loss: 0.0008530920022167265\n","Batch 310/1002, Loss: 0.005419909954071045\n","Batch 320/1002, Loss: 0.00035382952773943543\n","Batch 330/1002, Loss: 0.06414394825696945\n","Batch 340/1002, Loss: 0.006335881073027849\n","Batch 350/1002, Loss: 0.0010785094927996397\n","Batch 360/1002, Loss: 0.0015769917517900467\n","Batch 370/1002, Loss: 0.00040572084253653884\n","Batch 380/1002, Loss: 0.0030368727166205645\n","Batch 390/1002, Loss: 0.018121033906936646\n","Batch 400/1002, Loss: 0.0018141331383958459\n","Batch 410/1002, Loss: 0.00012677483027800918\n","Batch 420/1002, Loss: 0.0035592117346823215\n","Batch 430/1002, Loss: 0.002522560302168131\n","Batch 440/1002, Loss: 0.0006854770472273231\n","Batch 450/1002, Loss: 0.011678347364068031\n","Batch 460/1002, Loss: 0.001261756755411625\n","Batch 470/1002, Loss: 0.006189978215843439\n","Batch 480/1002, Loss: 0.2630870044231415\n","Batch 490/1002, Loss: 0.0002277697203680873\n","Batch 500/1002, Loss: 0.0460088849067688\n","Batch 510/1002, Loss: 0.010239656083285809\n","Batch 520/1002, Loss: 0.00013491706340573728\n","Batch 530/1002, Loss: 0.001075745327398181\n","Batch 540/1002, Loss: 0.0007449915865436196\n","Batch 550/1002, Loss: 0.0008533922955393791\n","Batch 560/1002, Loss: 0.0006878301501274109\n","Batch 570/1002, Loss: 0.0002139720309060067\n","Batch 580/1002, Loss: 0.002196627203375101\n","Batch 590/1002, Loss: 0.0025099299382418394\n","Batch 600/1002, Loss: 0.0027925404720008373\n","Batch 610/1002, Loss: 0.00032206112518906593\n","Batch 620/1002, Loss: 0.04786783084273338\n","Batch 630/1002, Loss: 0.00022222258849069476\n","Batch 640/1002, Loss: 0.014528141357004642\n","Batch 650/1002, Loss: 0.13977345824241638\n","Batch 660/1002, Loss: 0.1394319385290146\n","Batch 670/1002, Loss: 0.020263418555259705\n","Batch 680/1002, Loss: 0.0005137048428878188\n","Batch 690/1002, Loss: 0.0013899420155212283\n","Batch 700/1002, Loss: 0.002690425608307123\n","Batch 710/1002, Loss: 0.00386504246853292\n","Batch 720/1002, Loss: 0.0008885671850293875\n","Batch 730/1002, Loss: 0.00042145149200223386\n","Batch 740/1002, Loss: 0.16038794815540314\n","Batch 750/1002, Loss: 0.009874829091131687\n","Batch 760/1002, Loss: 0.0001563539117341861\n","Batch 770/1002, Loss: 0.00023741467157378793\n","Batch 780/1002, Loss: 0.0015580132603645325\n","Batch 790/1002, Loss: 0.0017207725904881954\n","Batch 800/1002, Loss: 0.01628989726305008\n","Batch 810/1002, Loss: 0.003592319320887327\n","Batch 820/1002, Loss: 0.0007910678396001458\n","Batch 830/1002, Loss: 0.0007783757755532861\n","Batch 840/1002, Loss: 0.0002756624890025705\n","Batch 850/1002, Loss: 0.0006360746338032186\n","Batch 860/1002, Loss: 0.0016795456176623702\n","Batch 870/1002, Loss: 0.02804979868233204\n","Batch 880/1002, Loss: 0.0001990526943700388\n","Batch 890/1002, Loss: 0.003903775243088603\n","Batch 900/1002, Loss: 0.0002955313539132476\n","Batch 910/1002, Loss: 0.0002473129134159535\n","Batch 920/1002, Loss: 0.0043508498929440975\n","Batch 930/1002, Loss: 0.054365381598472595\n","Batch 940/1002, Loss: 0.0005643119802698493\n","Batch 950/1002, Loss: 0.0024271870497614145\n","Batch 960/1002, Loss: 0.03819705545902252\n","Batch 970/1002, Loss: 0.0077895731665194035\n","Batch 980/1002, Loss: 0.017878426238894463\n","Batch 990/1002, Loss: 0.003893597749993205\n","Batch 1000/1002, Loss: 0.0022023131605237722\n","Training epoch completed in: 1m 21s\n","Train loss 0.01713893214329579 accuracy 0.9936985275767408\n","Batch 0/251, Test Loss: 0.5676319003105164\n","Batch 10/251, Test Loss: 0.0015512211248278618\n","Batch 20/251, Test Loss: 0.015529805794358253\n","Batch 30/251, Test Loss: 0.00046373100485652685\n","Batch 40/251, Test Loss: 0.176833376288414\n","Batch 50/251, Test Loss: 0.48981890082359314\n","Batch 60/251, Test Loss: 0.5000836253166199\n","Batch 70/251, Test Loss: 0.621062159538269\n","Batch 80/251, Test Loss: 0.6173001527786255\n","Batch 90/251, Test Loss: 0.6189488768577576\n","Batch 100/251, Test Loss: 6.665717955911532e-05\n","Batch 110/251, Test Loss: 0.016458556056022644\n","Batch 120/251, Test Loss: 0.9069204330444336\n","Batch 130/251, Test Loss: 0.00018132288823835552\n","Batch 140/251, Test Loss: 0.04935070127248764\n","Batch 150/251, Test Loss: 0.24737507104873657\n","Batch 160/251, Test Loss: 2.1759283542633057\n","Batch 170/251, Test Loss: 0.22720056772232056\n","Batch 180/251, Test Loss: 0.3195193111896515\n","Batch 190/251, Test Loss: 0.42715853452682495\n","Batch 200/251, Test Loss: 1.436493158340454\n","Batch 210/251, Test Loss: 0.5492985248565674\n","Batch 220/251, Test Loss: 0.7900203466415405\n","Batch 230/251, Test Loss: 0.06854889541864395\n","Batch 240/251, Test Loss: 0.00039460172411054373\n","Batch 250/251, Test Loss: 0.0005858057411387563\n","Test evaluation completed in: 0m 7s\n","Test loss 0.38094242444985216 accuracy 0.9241516966067863\n","Test Precision: 0.9241554987823645\n","Test Recall: 0.9241516966067864\n","Test F1 Score: 0.9241515266283151\n","Starting epoch 7/10\n","Batch 0/1002, Loss: 0.07643540948629379\n","Batch 10/1002, Loss: 0.01596250757575035\n","Batch 20/1002, Loss: 0.2231241762638092\n","Batch 30/1002, Loss: 0.0006169015541672707\n","Batch 40/1002, Loss: 0.005785598885267973\n","Batch 50/1002, Loss: 0.0244325939565897\n","Batch 60/1002, Loss: 0.003060793038457632\n","Batch 70/1002, Loss: 0.00041069986764341593\n","Batch 80/1002, Loss: 0.006282016169279814\n","Batch 90/1002, Loss: 0.0043714530766010284\n","Batch 100/1002, Loss: 0.0026756704319268465\n","Batch 110/1002, Loss: 0.00017440087685827166\n","Batch 120/1002, Loss: 0.0008699906757101417\n","Batch 130/1002, Loss: 0.0001363539631711319\n","Batch 140/1002, Loss: 0.0007332550594583154\n","Batch 150/1002, Loss: 0.0017345297383144498\n","Batch 160/1002, Loss: 0.0005250847316347063\n","Batch 170/1002, Loss: 0.00014970460324548185\n","Batch 180/1002, Loss: 0.0013195440405979753\n","Batch 190/1002, Loss: 0.0005824007675983012\n","Batch 200/1002, Loss: 0.0025702733546495438\n","Batch 210/1002, Loss: 0.00014543734141625464\n","Batch 220/1002, Loss: 0.01285303384065628\n","Batch 230/1002, Loss: 0.00382430967874825\n","Batch 240/1002, Loss: 0.0001313930406467989\n","Batch 250/1002, Loss: 0.00033963765599764884\n","Batch 260/1002, Loss: 0.0013163742842152715\n","Batch 270/1002, Loss: 0.010632004588842392\n","Batch 280/1002, Loss: 0.0008995658135972917\n","Batch 290/1002, Loss: 0.10665413737297058\n","Batch 300/1002, Loss: 0.0023892796598374844\n","Batch 310/1002, Loss: 0.0003233431198168546\n","Batch 320/1002, Loss: 0.0001792477269191295\n","Batch 330/1002, Loss: 0.0018563250778242946\n","Batch 340/1002, Loss: 0.0002668966189958155\n","Batch 350/1002, Loss: 0.00012459958088584244\n","Batch 360/1002, Loss: 0.00014227179053705186\n","Batch 370/1002, Loss: 0.00013210736506152898\n","Batch 380/1002, Loss: 0.0001329090737272054\n","Batch 390/1002, Loss: 0.007506161462515593\n","Batch 400/1002, Loss: 0.00015523577167186886\n","Batch 410/1002, Loss: 9.853348456090316e-05\n","Batch 420/1002, Loss: 0.0031302517745643854\n","Batch 430/1002, Loss: 0.017963426187634468\n","Batch 440/1002, Loss: 0.0004312003729864955\n","Batch 450/1002, Loss: 0.0002561751171015203\n","Batch 460/1002, Loss: 0.0003627216792665422\n","Batch 470/1002, Loss: 0.01231319084763527\n","Batch 480/1002, Loss: 0.0024266571272164583\n","Batch 490/1002, Loss: 0.00014209056098479778\n","Batch 500/1002, Loss: 0.021292269229888916\n","Batch 510/1002, Loss: 0.0013208577875047922\n","Batch 520/1002, Loss: 0.00011104010627605021\n","Batch 530/1002, Loss: 0.057543326169252396\n","Batch 540/1002, Loss: 0.00018015046953223646\n","Batch 550/1002, Loss: 0.001031111110933125\n","Batch 560/1002, Loss: 0.018278464674949646\n","Batch 570/1002, Loss: 0.00012116548896301538\n","Batch 580/1002, Loss: 0.02200310118496418\n","Batch 590/1002, Loss: 0.007329255808144808\n","Batch 600/1002, Loss: 0.00016200366371776909\n","Batch 610/1002, Loss: 0.00023367996618617326\n","Batch 620/1002, Loss: 0.0006986954831518233\n","Batch 630/1002, Loss: 0.0006820456474088132\n","Batch 640/1002, Loss: 0.04225815832614899\n","Batch 650/1002, Loss: 0.0002634470001794398\n","Batch 660/1002, Loss: 0.1272096037864685\n","Batch 670/1002, Loss: 0.009617974981665611\n","Batch 680/1002, Loss: 0.0011461252579465508\n","Batch 690/1002, Loss: 0.00016566249541938305\n","Batch 700/1002, Loss: 0.001458359183743596\n","Batch 710/1002, Loss: 0.014721809886395931\n","Batch 720/1002, Loss: 0.00025098209152929485\n","Batch 730/1002, Loss: 0.00016681117995176464\n","Batch 740/1002, Loss: 0.0032791991252452135\n","Batch 750/1002, Loss: 0.23849008977413177\n","Batch 760/1002, Loss: 0.00010500644566491246\n","Batch 770/1002, Loss: 0.00010950034629786387\n","Batch 780/1002, Loss: 0.0018366988515481353\n","Batch 790/1002, Loss: 0.0030641467310488224\n","Batch 800/1002, Loss: 0.00013688011677004397\n","Batch 810/1002, Loss: 0.0016243919963017106\n","Batch 820/1002, Loss: 0.0014516982482746243\n","Batch 830/1002, Loss: 0.00015551246178802103\n","Batch 840/1002, Loss: 0.001876047346740961\n","Batch 850/1002, Loss: 0.00170744932256639\n","Batch 860/1002, Loss: 0.0006110790418460965\n","Batch 870/1002, Loss: 0.00024957669666036963\n","Batch 880/1002, Loss: 0.0004705609753727913\n","Batch 890/1002, Loss: 0.0033553249668329954\n","Batch 900/1002, Loss: 0.0004382732149679214\n","Batch 910/1002, Loss: 0.00021158548770472407\n","Batch 920/1002, Loss: 0.005938393995165825\n","Batch 930/1002, Loss: 0.0017940070247277617\n","Batch 940/1002, Loss: 0.00021452469809446484\n","Batch 950/1002, Loss: 0.0006267020362429321\n","Batch 960/1002, Loss: 0.0028773401863873005\n","Batch 970/1002, Loss: 0.00047372703556902707\n","Batch 980/1002, Loss: 0.019709963351488113\n","Batch 990/1002, Loss: 0.0016970373690128326\n","Batch 1000/1002, Loss: 0.0009486727067269385\n","Training epoch completed in: 1m 21s\n","Train loss 0.01091643839283856 accuracy 0.9963813326678314\n","Batch 0/251, Test Loss: 0.5604869723320007\n","Batch 10/251, Test Loss: 0.00011987212201347575\n","Batch 20/251, Test Loss: 0.6082940697669983\n","Batch 30/251, Test Loss: 0.00037172369775362313\n","Batch 40/251, Test Loss: 0.0027015991508960724\n","Batch 50/251, Test Loss: 0.5421303510665894\n","Batch 60/251, Test Loss: 0.4776139557361603\n","Batch 70/251, Test Loss: 0.6396811604499817\n","Batch 80/251, Test Loss: 0.630664050579071\n","Batch 90/251, Test Loss: 0.5216183066368103\n","Batch 100/251, Test Loss: 5.89166593272239e-05\n","Batch 110/251, Test Loss: 0.04698522016406059\n","Batch 120/251, Test Loss: 1.1734782457351685\n","Batch 130/251, Test Loss: 0.00010324384493287653\n","Batch 140/251, Test Loss: 0.2107967585325241\n","Batch 150/251, Test Loss: 0.07457254081964493\n","Batch 160/251, Test Loss: 2.3354718685150146\n","Batch 170/251, Test Loss: 0.03934524208307266\n","Batch 180/251, Test Loss: 0.008947388269007206\n","Batch 190/251, Test Loss: 0.38790538907051086\n","Batch 200/251, Test Loss: 1.522331714630127\n","Batch 210/251, Test Loss: 0.5028138756752014\n","Batch 220/251, Test Loss: 1.0706737041473389\n","Batch 230/251, Test Loss: 0.17496344447135925\n","Batch 240/251, Test Loss: 0.0001824660284910351\n","Batch 250/251, Test Loss: 0.04037453606724739\n","Test evaluation completed in: 0m 7s\n","Test loss 0.4203607644052377 accuracy 0.9214071856287425\n","Test Precision: 0.9222399918165612\n","Test Recall: 0.9214071856287425\n","Test F1 Score: 0.9213684133231748\n","Starting epoch 8/10\n","Batch 0/1002, Loss: 0.003415182698518038\n","Batch 10/1002, Loss: 0.08266793936491013\n","Batch 20/1002, Loss: 0.005481898318976164\n","Batch 30/1002, Loss: 0.01147395744919777\n","Batch 40/1002, Loss: 0.012701918371021748\n","Batch 50/1002, Loss: 0.018752656877040863\n","Batch 60/1002, Loss: 0.11134369671344757\n","Batch 70/1002, Loss: 0.00012664553651120514\n","Batch 80/1002, Loss: 0.0003559750330168754\n","Batch 90/1002, Loss: 0.23196828365325928\n","Batch 100/1002, Loss: 0.0008028919110074639\n","Batch 110/1002, Loss: 0.00014219325385056436\n","Batch 120/1002, Loss: 0.0008549850317649543\n","Batch 130/1002, Loss: 9.418481931788847e-05\n","Batch 140/1002, Loss: 0.0012315775966271758\n","Batch 150/1002, Loss: 0.0001943698589457199\n","Batch 160/1002, Loss: 0.0004064429667778313\n","Batch 170/1002, Loss: 0.00013899111945647746\n","Batch 180/1002, Loss: 0.0002006887225434184\n","Batch 190/1002, Loss: 0.0004132053873036057\n","Batch 200/1002, Loss: 0.009264932945370674\n","Batch 210/1002, Loss: 0.0007051476859487593\n","Batch 220/1002, Loss: 0.00540580227971077\n","Batch 230/1002, Loss: 0.0737607479095459\n","Batch 240/1002, Loss: 0.0001840350014390424\n","Batch 250/1002, Loss: 0.0002255328290630132\n","Batch 260/1002, Loss: 0.002113763242959976\n","Batch 270/1002, Loss: 0.0038405531086027622\n","Batch 280/1002, Loss: 0.00013385352212935686\n","Batch 290/1002, Loss: 0.1260858178138733\n","Batch 300/1002, Loss: 0.0002410771558061242\n","Batch 310/1002, Loss: 8.542384603060782e-05\n","Batch 320/1002, Loss: 0.00034005657653324306\n","Batch 330/1002, Loss: 0.19405773282051086\n","Batch 340/1002, Loss: 0.00019515959138516337\n","Batch 350/1002, Loss: 0.00020021418458782136\n","Batch 360/1002, Loss: 0.002037006663158536\n","Batch 370/1002, Loss: 0.0002322283835383132\n","Batch 380/1002, Loss: 0.0012039401335641742\n","Batch 390/1002, Loss: 0.01540381833910942\n","Batch 400/1002, Loss: 6.929467781446874e-05\n","Batch 410/1002, Loss: 0.000660467951092869\n","Batch 420/1002, Loss: 0.06398364901542664\n","Batch 430/1002, Loss: 0.029200967401266098\n","Batch 440/1002, Loss: 0.0008932570344768465\n","Batch 450/1002, Loss: 0.00030584546038880944\n","Batch 460/1002, Loss: 0.0009907770436257124\n","Batch 470/1002, Loss: 0.0006955782882869244\n","Batch 480/1002, Loss: 0.0005500330589711666\n","Batch 490/1002, Loss: 0.00010835893044713885\n","Batch 500/1002, Loss: 0.00823723990470171\n","Batch 510/1002, Loss: 0.000635818752925843\n","Batch 520/1002, Loss: 8.385888213524595e-05\n","Batch 530/1002, Loss: 0.0002632742980495095\n","Batch 540/1002, Loss: 0.00015565067587886006\n","Batch 550/1002, Loss: 0.0007662597927264869\n","Batch 560/1002, Loss: 0.20405153930187225\n","Batch 570/1002, Loss: 0.00015830509073566645\n","Batch 580/1002, Loss: 0.02616906352341175\n","Batch 590/1002, Loss: 0.0168417040258646\n","Batch 600/1002, Loss: 0.0001863173965830356\n","Batch 610/1002, Loss: 0.0005744868540205061\n","Batch 620/1002, Loss: 0.0007691328646615148\n","Batch 630/1002, Loss: 0.00041484099347144365\n","Batch 640/1002, Loss: 0.0016365995397791266\n","Batch 650/1002, Loss: 0.0001720261643640697\n","Batch 660/1002, Loss: 0.008554935455322266\n","Batch 670/1002, Loss: 0.24699899554252625\n","Batch 680/1002, Loss: 0.009098105132579803\n","Batch 690/1002, Loss: 0.0001280889700865373\n","Batch 700/1002, Loss: 0.004011824727058411\n","Batch 710/1002, Loss: 0.005807369481772184\n","Batch 720/1002, Loss: 0.033256545662879944\n","Batch 730/1002, Loss: 7.98364490037784e-05\n","Batch 740/1002, Loss: 0.0001299752329941839\n","Batch 750/1002, Loss: 0.004130681045353413\n","Batch 760/1002, Loss: 0.0001336155692115426\n","Batch 770/1002, Loss: 7.026197999948636e-05\n","Batch 780/1002, Loss: 0.0007168465526774526\n","Batch 790/1002, Loss: 0.002432251116260886\n","Batch 800/1002, Loss: 0.00012373033678159118\n","Batch 810/1002, Loss: 0.000794199644587934\n","Batch 820/1002, Loss: 0.002307093935087323\n","Batch 830/1002, Loss: 0.0001027185790007934\n","Batch 840/1002, Loss: 0.0010752236703410745\n","Batch 850/1002, Loss: 0.00040940518374554813\n","Batch 860/1002, Loss: 0.001838047057390213\n","Batch 870/1002, Loss: 0.0002992548979818821\n","Batch 880/1002, Loss: 0.00022459252795670182\n","Batch 890/1002, Loss: 0.0027366841677576303\n","Batch 900/1002, Loss: 0.00010277928231516853\n","Batch 910/1002, Loss: 0.00011671793618006632\n","Batch 920/1002, Loss: 0.015727411955595016\n","Batch 930/1002, Loss: 0.08959394693374634\n","Batch 940/1002, Loss: 0.00015084748156368732\n","Batch 950/1002, Loss: 0.0008489865576848388\n","Batch 960/1002, Loss: 0.0010359479347243905\n","Batch 970/1002, Loss: 0.00013388357183430344\n","Batch 980/1002, Loss: 0.07042354345321655\n","Batch 990/1002, Loss: 0.03337806090712547\n","Batch 1000/1002, Loss: 0.010060321539640427\n","Training epoch completed in: 1m 21s\n","Train loss 0.008421635186255707 accuracy 0.9973795857249813\n","Batch 0/251, Test Loss: 0.6159593462944031\n","Batch 10/251, Test Loss: 0.0011252890108153224\n","Batch 20/251, Test Loss: 0.12568087875843048\n","Batch 30/251, Test Loss: 0.00015252476441673934\n","Batch 40/251, Test Loss: 0.2694854736328125\n","Batch 50/251, Test Loss: 0.37762269377708435\n","Batch 60/251, Test Loss: 0.3690814971923828\n","Batch 70/251, Test Loss: 0.678482174873352\n","Batch 80/251, Test Loss: 0.6382827162742615\n","Batch 90/251, Test Loss: 0.696126401424408\n","Batch 100/251, Test Loss: 4.393466224428266e-05\n","Batch 110/251, Test Loss: 0.0005463364068418741\n","Batch 120/251, Test Loss: 1.3794536590576172\n","Batch 130/251, Test Loss: 0.00010209813626715913\n","Batch 140/251, Test Loss: 0.000359917466994375\n","Batch 150/251, Test Loss: 0.33392488956451416\n","Batch 160/251, Test Loss: 2.0994696617126465\n","Batch 170/251, Test Loss: 0.0075670937076210976\n","Batch 180/251, Test Loss: 0.3475417196750641\n","Batch 190/251, Test Loss: 0.7206312417984009\n","Batch 200/251, Test Loss: 1.7467525005340576\n","Batch 210/251, Test Loss: 0.5979188680648804\n","Batch 220/251, Test Loss: 0.9296830892562866\n","Batch 230/251, Test Loss: 0.023355448618531227\n","Batch 240/251, Test Loss: 0.00030446366872638464\n","Batch 250/251, Test Loss: 0.00013843493070453405\n","Test evaluation completed in: 0m 7s\n","Test loss 0.42444108170191946 accuracy 0.9251497005988023\n","Test Precision: 0.9253365259580115\n","Test Recall: 0.9251497005988024\n","Test F1 Score: 0.9251414803608451\n","Starting epoch 9/10\n","Batch 0/1002, Loss: 0.002429251093417406\n","Batch 10/1002, Loss: 0.0010352588724344969\n","Batch 20/1002, Loss: 0.015654532238841057\n","Batch 30/1002, Loss: 0.000272379198577255\n","Batch 40/1002, Loss: 0.01877911575138569\n","Batch 50/1002, Loss: 0.03332674503326416\n","Batch 60/1002, Loss: 0.00012579458416439593\n","Batch 70/1002, Loss: 0.0001908307895064354\n","Batch 80/1002, Loss: 0.0008593006641604006\n","Batch 90/1002, Loss: 0.042123857885599136\n","Batch 100/1002, Loss: 0.004418048541992903\n","Batch 110/1002, Loss: 0.00010110781295225024\n","Batch 120/1002, Loss: 0.0033414517529308796\n","Batch 130/1002, Loss: 6.696282071061432e-05\n","Batch 140/1002, Loss: 0.00046266207937151194\n","Batch 150/1002, Loss: 0.00047562335385009646\n","Batch 160/1002, Loss: 0.0009031390072777867\n","Batch 170/1002, Loss: 7.244401058414951e-05\n","Batch 180/1002, Loss: 0.0002446788130328059\n","Batch 190/1002, Loss: 0.00013475125888362527\n","Batch 200/1002, Loss: 8.791803702479228e-05\n","Batch 210/1002, Loss: 0.00020757502352353185\n","Batch 220/1002, Loss: 0.003260563127696514\n","Batch 230/1002, Loss: 0.00047073361929506063\n","Batch 240/1002, Loss: 5.9400848840596154e-05\n","Batch 250/1002, Loss: 0.0003308740851934999\n","Batch 260/1002, Loss: 0.0075169699266552925\n","Batch 270/1002, Loss: 0.0030579103622585535\n","Batch 280/1002, Loss: 9.020765719469637e-05\n","Batch 290/1002, Loss: 0.026517951861023903\n","Batch 300/1002, Loss: 0.0001145685018855147\n","Batch 310/1002, Loss: 0.00018451926007401198\n","Batch 320/1002, Loss: 0.00018919089052360505\n","Batch 330/1002, Loss: 0.00960956234484911\n","Batch 340/1002, Loss: 5.79920488235075e-05\n","Batch 350/1002, Loss: 8.522954885847867e-05\n","Batch 360/1002, Loss: 8.850863378029317e-05\n","Batch 370/1002, Loss: 8.49580392241478e-05\n","Batch 380/1002, Loss: 0.00012230082938913256\n","Batch 390/1002, Loss: 0.0005865022540092468\n","Batch 400/1002, Loss: 9.745470015332103e-05\n","Batch 410/1002, Loss: 8.260863978648558e-05\n","Batch 420/1002, Loss: 0.0007918777992017567\n","Batch 430/1002, Loss: 0.00014705165813211352\n","Batch 440/1002, Loss: 0.00017166048928629607\n","Batch 450/1002, Loss: 0.00013859063619747758\n","Batch 460/1002, Loss: 0.010341349057853222\n","Batch 470/1002, Loss: 0.00017755941371433437\n","Batch 480/1002, Loss: 0.0005255955038592219\n","Batch 490/1002, Loss: 7.526129047619179e-05\n","Batch 500/1002, Loss: 0.00013608783774543554\n","Batch 510/1002, Loss: 0.0004843531351070851\n","Batch 520/1002, Loss: 5.4938194807618856e-05\n","Batch 530/1002, Loss: 0.00013305162428878248\n","Batch 540/1002, Loss: 0.001784654799848795\n","Batch 550/1002, Loss: 0.01089368388056755\n","Batch 560/1002, Loss: 0.000696320494171232\n","Batch 570/1002, Loss: 6.803568248869851e-05\n","Batch 580/1002, Loss: 0.00014746830856893212\n","Batch 590/1002, Loss: 0.0007590002496726811\n","Batch 600/1002, Loss: 0.00046522385673597455\n","Batch 610/1002, Loss: 0.00011975511006312445\n","Batch 620/1002, Loss: 0.0006878584972582757\n","Batch 630/1002, Loss: 0.0001879022311186418\n","Batch 640/1002, Loss: 0.012545911595225334\n","Batch 650/1002, Loss: 0.00017131889762822539\n","Batch 660/1002, Loss: 0.02049189805984497\n","Batch 670/1002, Loss: 0.012931103818118572\n","Batch 680/1002, Loss: 6.401728751370683e-05\n","Batch 690/1002, Loss: 0.00010060415661428124\n","Batch 700/1002, Loss: 0.0004687958862632513\n","Batch 710/1002, Loss: 0.006154324859380722\n","Batch 720/1002, Loss: 0.00011012516188202426\n","Batch 730/1002, Loss: 7.149053271859884e-05\n","Batch 740/1002, Loss: 0.00010516208567423746\n","Batch 750/1002, Loss: 0.0013751683291047812\n","Batch 760/1002, Loss: 5.484160647029057e-05\n","Batch 770/1002, Loss: 4.629663089872338e-05\n","Batch 780/1002, Loss: 0.005076614674180746\n","Batch 790/1002, Loss: 0.00035842449869960546\n","Batch 800/1002, Loss: 6.121899787103757e-05\n","Batch 810/1002, Loss: 0.0002889779570978135\n","Batch 820/1002, Loss: 0.03478409722447395\n","Batch 830/1002, Loss: 6.334896170301363e-05\n","Batch 840/1002, Loss: 0.00014350285346154124\n","Batch 850/1002, Loss: 0.0007539614452980459\n","Batch 860/1002, Loss: 0.00024351633328478783\n","Batch 870/1002, Loss: 0.0001329789956798777\n","Batch 880/1002, Loss: 6.705164560116827e-05\n","Batch 890/1002, Loss: 0.10272781550884247\n","Batch 900/1002, Loss: 8.819814684102312e-05\n","Batch 910/1002, Loss: 8.972884097602218e-05\n","Batch 920/1002, Loss: 0.0002475129731465131\n","Batch 930/1002, Loss: 0.00010030078556155786\n","Batch 940/1002, Loss: 0.00013339411816559732\n","Batch 950/1002, Loss: 0.00036490909405983984\n","Batch 960/1002, Loss: 0.0008015937055461109\n","Batch 970/1002, Loss: 0.00031751825008541346\n","Batch 980/1002, Loss: 0.180371955037117\n","Batch 990/1002, Loss: 0.0025926579255610704\n","Batch 1000/1002, Loss: 0.0003957639855798334\n","Training epoch completed in: 1m 21s\n","Train loss 0.004123480175436641 accuracy 0.9985026204142751\n","Batch 0/251, Test Loss: 0.6485862731933594\n","Batch 10/251, Test Loss: 0.00017592500080354512\n","Batch 20/251, Test Loss: 0.14609204232692719\n","Batch 30/251, Test Loss: 7.161667599575594e-05\n","Batch 40/251, Test Loss: 0.4774969816207886\n","Batch 50/251, Test Loss: 0.43915462493896484\n","Batch 60/251, Test Loss: 0.436565101146698\n","Batch 70/251, Test Loss: 0.6855834722518921\n","Batch 80/251, Test Loss: 0.657335102558136\n","Batch 90/251, Test Loss: 0.9535845518112183\n","Batch 100/251, Test Loss: 3.2878586353035644e-05\n","Batch 110/251, Test Loss: 0.0002727604587562382\n","Batch 120/251, Test Loss: 1.395020842552185\n","Batch 130/251, Test Loss: 7.200666004791856e-05\n","Batch 140/251, Test Loss: 0.018149971961975098\n","Batch 150/251, Test Loss: 0.5035662055015564\n","Batch 160/251, Test Loss: 2.307792901992798\n","Batch 170/251, Test Loss: 0.04070945456624031\n","Batch 180/251, Test Loss: 0.47831520438194275\n","Batch 190/251, Test Loss: 0.6056091785430908\n","Batch 200/251, Test Loss: 1.855698823928833\n","Batch 210/251, Test Loss: 0.8281586766242981\n","Batch 220/251, Test Loss: 1.0609327554702759\n","Batch 230/251, Test Loss: 0.07034890353679657\n","Batch 240/251, Test Loss: 0.00025975171593017876\n","Batch 250/251, Test Loss: 0.0001983675902010873\n","Test evaluation completed in: 0m 7s\n","Test loss 0.47265548157507925 accuracy 0.9256487025948102\n","Test Precision: 0.9257318132551244\n","Test Recall: 0.9256487025948104\n","Test F1 Score: 0.9256450737333846\n","Starting epoch 10/10\n","Batch 0/1002, Loss: 0.0006062338361516595\n","Batch 10/1002, Loss: 0.0001568121078889817\n","Batch 20/1002, Loss: 0.09235267341136932\n","Batch 30/1002, Loss: 0.0003575712034944445\n","Batch 40/1002, Loss: 0.003349080216139555\n","Batch 50/1002, Loss: 0.00032080605160444975\n","Batch 60/1002, Loss: 0.0005900449468754232\n","Batch 70/1002, Loss: 0.0001329984952462837\n","Batch 80/1002, Loss: 0.0001583258854225278\n","Batch 90/1002, Loss: 0.003669295459985733\n","Batch 100/1002, Loss: 0.009183156304061413\n","Batch 110/1002, Loss: 0.0001305042242165655\n","Batch 120/1002, Loss: 0.0005845397245138884\n","Batch 130/1002, Loss: 4.950022048433311e-05\n","Batch 140/1002, Loss: 0.001697368803434074\n","Batch 150/1002, Loss: 0.0021866026800125837\n","Batch 160/1002, Loss: 0.00032387126702815294\n","Batch 170/1002, Loss: 5.9073063312098384e-05\n","Batch 180/1002, Loss: 0.00022773131786379963\n","Batch 190/1002, Loss: 0.0001273669913643971\n","Batch 200/1002, Loss: 0.0003174078301526606\n","Batch 210/1002, Loss: 0.0009652182343415916\n","Batch 220/1002, Loss: 0.0017252841498702765\n","Batch 230/1002, Loss: 0.0007786728674545884\n","Batch 240/1002, Loss: 6.933458644198254e-05\n","Batch 250/1002, Loss: 0.00029157690005376935\n","Batch 260/1002, Loss: 0.00011734063446056098\n","Batch 270/1002, Loss: 0.0005820681690238416\n","Batch 280/1002, Loss: 5.380621587391943e-05\n","Batch 290/1002, Loss: 0.06811457127332687\n","Batch 300/1002, Loss: 0.0004478213668335229\n","Batch 310/1002, Loss: 7.131313759600744e-05\n","Batch 320/1002, Loss: 6.95162671036087e-05\n","Batch 330/1002, Loss: 0.00036366048152558506\n","Batch 340/1002, Loss: 8.316833554999903e-05\n","Batch 350/1002, Loss: 6.597957690246403e-05\n","Batch 360/1002, Loss: 7.155739149311557e-05\n","Batch 370/1002, Loss: 5.636048808810301e-05\n","Batch 380/1002, Loss: 5.672688712365925e-05\n","Batch 390/1002, Loss: 0.0006852107471786439\n","Batch 400/1002, Loss: 5.904223507968709e-05\n","Batch 410/1002, Loss: 4.735439142677933e-05\n","Batch 420/1002, Loss: 0.0006412211805582047\n","Batch 430/1002, Loss: 0.00022516777971759439\n","Batch 440/1002, Loss: 0.0004314456309657544\n","Batch 450/1002, Loss: 0.00013009714893996716\n","Batch 460/1002, Loss: 0.0001577297953190282\n","Batch 470/1002, Loss: 0.00036571311648003757\n","Batch 480/1002, Loss: 0.00023739904281683266\n","Batch 490/1002, Loss: 0.00011017783253919333\n","Batch 500/1002, Loss: 0.0004129369626753032\n","Batch 510/1002, Loss: 6.083132029743865e-05\n","Batch 520/1002, Loss: 5.136937397764996e-05\n","Batch 530/1002, Loss: 0.00020733600831590593\n","Batch 540/1002, Loss: 7.532553718192503e-05\n","Batch 550/1002, Loss: 0.0006801728741265833\n","Batch 560/1002, Loss: 0.0004524154355749488\n","Batch 570/1002, Loss: 4.1282699385192245e-05\n","Batch 580/1002, Loss: 9.98329560388811e-05\n","Batch 590/1002, Loss: 0.0010344780748710036\n","Batch 600/1002, Loss: 8.759411139180884e-05\n","Batch 610/1002, Loss: 0.00012140201579313725\n","Batch 620/1002, Loss: 0.015727108344435692\n","Batch 630/1002, Loss: 8.755188900977373e-05\n","Batch 640/1002, Loss: 0.057606760412454605\n","Batch 650/1002, Loss: 0.00013996018969919533\n","Batch 660/1002, Loss: 0.0010571706807240844\n","Batch 670/1002, Loss: 0.05665946751832962\n","Batch 680/1002, Loss: 0.0001329025544691831\n","Batch 690/1002, Loss: 5.5973880080273375e-05\n","Batch 700/1002, Loss: 0.00018396886298432946\n","Batch 710/1002, Loss: 0.0006010427023284137\n","Batch 720/1002, Loss: 0.0001374682178720832\n","Batch 730/1002, Loss: 6.149370165076107e-05\n","Batch 740/1002, Loss: 0.00030112682725302875\n","Batch 750/1002, Loss: 0.0008012381731532514\n","Batch 760/1002, Loss: 5.750020136474632e-05\n","Batch 770/1002, Loss: 3.892833410645835e-05\n","Batch 780/1002, Loss: 0.0005372204468585551\n","Batch 790/1002, Loss: 0.000412104360293597\n","Batch 800/1002, Loss: 8.285355579573661e-05\n","Batch 810/1002, Loss: 0.0003267650317866355\n","Batch 820/1002, Loss: 9.893332025967538e-05\n","Batch 830/1002, Loss: 5.032629633205943e-05\n","Batch 840/1002, Loss: 4.212422209093347e-05\n","Batch 850/1002, Loss: 0.00023938708181958646\n","Batch 860/1002, Loss: 0.00018419412663206458\n","Batch 870/1002, Loss: 9.200036583933979e-05\n","Batch 880/1002, Loss: 5.2211158617865294e-05\n","Batch 890/1002, Loss: 0.00011479464592412114\n","Batch 900/1002, Loss: 0.00046541166375391185\n","Batch 910/1002, Loss: 0.00011421514500398189\n","Batch 920/1002, Loss: 0.00023645015608053654\n","Batch 930/1002, Loss: 5.012588371755555e-05\n","Batch 940/1002, Loss: 0.0002487779420334846\n","Batch 950/1002, Loss: 0.00022262120910454541\n","Batch 960/1002, Loss: 0.00041798321763053536\n","Batch 970/1002, Loss: 7.728546916041523e-05\n","Batch 980/1002, Loss: 0.009894044138491154\n","Batch 990/1002, Loss: 0.0020157769322395325\n","Batch 1000/1002, Loss: 0.0025399283040314913\n","Training epoch completed in: 1m 21s\n","Train loss 0.0031708343861150106 accuracy 0.9987521836785626\n","Batch 0/251, Test Loss: 0.6464700698852539\n","Batch 10/251, Test Loss: 0.00017735117580741644\n","Batch 20/251, Test Loss: 0.3280528783798218\n","Batch 30/251, Test Loss: 6.34396928944625e-05\n","Batch 40/251, Test Loss: 0.49491947889328003\n","Batch 50/251, Test Loss: 0.6923856139183044\n","Batch 60/251, Test Loss: 0.4822331964969635\n","Batch 70/251, Test Loss: 0.703924834728241\n","Batch 80/251, Test Loss: 0.6583669185638428\n","Batch 90/251, Test Loss: 0.873299777507782\n","Batch 100/251, Test Loss: 2.9891096346545964e-05\n","Batch 110/251, Test Loss: 0.00010073673911392689\n","Batch 120/251, Test Loss: 1.4717350006103516\n","Batch 130/251, Test Loss: 5.868249718332663e-05\n","Batch 140/251, Test Loss: 0.0009782384149730206\n","Batch 150/251, Test Loss: 0.5756790637969971\n","Batch 160/251, Test Loss: 2.2195425033569336\n","Batch 170/251, Test Loss: 0.015089953318238258\n","Batch 180/251, Test Loss: 0.5303241610527039\n","Batch 190/251, Test Loss: 0.7321923971176147\n","Batch 200/251, Test Loss: 2.1381516456604004\n","Batch 210/251, Test Loss: 0.6770519614219666\n","Batch 220/251, Test Loss: 1.0001335144042969\n","Batch 230/251, Test Loss: 0.06814613193273544\n","Batch 240/251, Test Loss: 0.0002225120842922479\n","Batch 250/251, Test Loss: 0.00012374595098663121\n","Test evaluation completed in: 0m 7s\n","Test loss 0.4766611662974345 accuracy 0.9256487025948102\n","Test Precision: 0.9257712597699547\n","Test Recall: 0.9256487025948104\n","Test F1 Score: 0.92564335175175\n"]}],"source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split\n","import time\n","import traceback\n","\n","class ClickbaitDataset(Dataset):\n","    def __init__(self, titles, labels, tokenizer, max_len):\n","        self.titles = titles\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.titles)\n","\n","    def __getitem__(self, idx):\n","        title = self.titles[idx]\n","        label = self.labels[idx]\n","\n","        # Ensure the label is a valid integer\n","        try:\n","            label = int(label)\n","        except ValueError:\n","            raise ValueError(f\"Label {label} at index {idx} is not a valid integer.\")\n","\n","        encoding = self.tokenizer.encode_plus(\n","            title,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'title_text': title,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","def create_data_loader(titles, labels, tokenizer, max_len, batch_size):\n","    ds = ClickbaitDataset(\n","        titles=titles,\n","        labels=labels,\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","\n","    return DataLoader(ds, batch_size=batch_size, num_workers=4, pin_memory=True)\n","\n","def train_epoch(\n","    model,\n","    data_loader,\n","    loss_fn,\n","    optimizer,\n","    device,\n","    scheduler,\n","    n_examples\n","):\n","    model.train()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    for batch_idx, d in enumerate(data_loader):\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        labels = d[\"labels\"].to(device)\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        _, preds = torch.max(outputs.logits, dim=1)\n","        loss = loss_fn(outputs.logits, labels)\n","\n","        correct_predictions += torch.sum(preds == labels)\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        if batch_idx % 10 == 0:\n","            print(f'Batch {batch_idx}/{len(data_loader)}, Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Training epoch completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model.eval()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    all_labels = []\n","    all_preds = []\n","\n","    with torch.no_grad():\n","        for batch_idx, d in enumerate(data_loader):\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            labels = d[\"labels\"].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            loss = loss_fn(outputs.logits, labels)\n","\n","            correct_predictions += torch.sum(preds == labels)\n","            losses.append(loss.item())\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","\n","            if batch_idx % 10 == 0:\n","                print(f'Batch {batch_idx}/{len(data_loader)}, Test Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Test evaluation completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses), all_labels, all_preds\n","\n","def clean_dataset(df):\n","    # Remove rows where labels are NaN\n","    df = df.dropna(subset=['label'])\n","    # Convert labels to integers, and filter out rows where this conversion fails\n","    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n","    df = df.dropna(subset=['label'])\n","    df['label'] = df['label'].astype(int)\n","    return df\n","\n","def main():\n","    print(\"Starting main process\")\n","\n","    try:\n","        print('Loading the dataset.')\n","        df = pd.read_csv('/content/drive/MyDrive/clickbait/turkish.csv')\n","        print('Dataset loaded successfully.')\n","\n","        print('Cleaning the dataset.')\n","        df = clean_dataset(df)\n","        print('Dataset cleaned.')\n","\n","        print('Splitting the dataset into training and test sets.')\n","        df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n","        print('Dataset split into training and test sets.')\n","\n","        RANDOM_SEED = 42\n","        MAX_LEN = 128\n","        BATCH_SIZE = 16\n","        EPOCHS = 10\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f'Using device: {device}')\n","\n","        print('Loading the tokenizer and model.')\n","        tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-turkish-cased')\n","        model = BertForSequenceClassification.from_pretrained('dbmdz/bert-base-turkish-cased', num_labels=2)\n","        model = model.to(device)\n","        print('Model and tokenizer loaded and moved to device.')\n","\n","        print('Creating data loaders.')\n","        train_titles = df_train['title'].tolist()\n","        train_labels = df_train['label'].tolist()\n","        test_titles = df_test['title'].tolist()\n","        test_labels = df_test['label'].tolist()\n","\n","        train_data_loader = create_data_loader(train_titles, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        test_data_loader = create_data_loader(test_titles, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        print('Data loaders created.')\n","\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","        total_steps = len(train_data_loader) * EPOCHS\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","        loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","        print('Optimizer, scheduler, and loss function defined.')\n","\n","        for epoch in range(EPOCHS):\n","            print(f'Starting epoch {epoch + 1}/{EPOCHS}')\n","\n","            train_acc, train_loss = train_epoch(\n","                model,\n","                train_data_loader,\n","                loss_fn,\n","                optimizer,\n","                device,\n","                scheduler,\n","                len(df_train)\n","            )\n","            print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","            test_acc, test_loss, all_labels, all_preds = eval_model(\n","                model,\n","                test_data_loader,\n","                loss_fn,\n","                device,\n","                len(df_test)\n","            )\n","            print(f'Test loss {test_loss} accuracy {test_acc}')\n","\n","            # Calculate additional metrics\n","            precision = precision_score(all_labels, all_preds, average='weighted')\n","            recall = recall_score(all_labels, all_preds, average='weighted')\n","            f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","            print(f'Test Precision: {precision}')\n","            print(f'Test Recall: {recall}')\n","            print(f'Test F1 Score: {f1}')\n","\n","    except Exception as e:\n","        print(f'An error occurred: {e}')\n","        traceback.print_exc()\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32194,"status":"ok","timestamp":1721761180908,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"},"user_tz":-180},"id":"Ic7ZMvG1IbEx","outputId":"eadc1573-b6a6-4459-f061-2bf034dec19c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"mount_file_id":"1ZDoY_cCvGd99UeEdfZDeQTAR73hRmiF8","authorship_tag":"ABX9TyMqwKVYmqxrARU9dV45QEDn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9767cf44ce11424eb05c342db7941901":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ca96798e85f941a1a943b9584fd3f39f","IPY_MODEL_9ab5a94db23e44b4a4257dec01c17391","IPY_MODEL_d54cc7dab5e94d9a8b62f874e7772cb8"],"layout":"IPY_MODEL_f4fdd536a2e843a789706c8b74fdc120"}},"ca96798e85f941a1a943b9584fd3f39f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f141a6fa33c40b59d5a24b6674f3938","placeholder":"​","style":"IPY_MODEL_c5872dfcd7294d5da856573e36215ac4","value":"tokenizer_config.json: 100%"}},"9ab5a94db23e44b4a4257dec01c17391":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7e7880648144cc7be60e412edbef8bb","max":60,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16921677ac154c92b8c54be2b0a302ed","value":60}},"d54cc7dab5e94d9a8b62f874e7772cb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca50b74b318b4c479e993034c800531f","placeholder":"​","style":"IPY_MODEL_9f14f6dd4800488888b931e81b2d35ee","value":" 60.0/60.0 [00:00&lt;00:00, 5.20kB/s]"}},"f4fdd536a2e843a789706c8b74fdc120":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f141a6fa33c40b59d5a24b6674f3938":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5872dfcd7294d5da856573e36215ac4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7e7880648144cc7be60e412edbef8bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16921677ac154c92b8c54be2b0a302ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca50b74b318b4c479e993034c800531f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f14f6dd4800488888b931e81b2d35ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d58f72417dbb400385aa794db657c814":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7bec3de070564f6f97a75239eac67120","IPY_MODEL_44905da1c1474222859f049f44c07de0","IPY_MODEL_59d0aced5dfe470688d04009c5e7fa69"],"layout":"IPY_MODEL_487f763d6e5e4913a0c24aa7d6fce4bf"}},"7bec3de070564f6f97a75239eac67120":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71b64ccd2d0f484e857b14bbd0d03b71","placeholder":"​","style":"IPY_MODEL_073d21e8c2b14b73bfa053fcbd1a5728","value":"vocab.txt: 100%"}},"44905da1c1474222859f049f44c07de0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_21062d8abd404a1fbc17708e6bfdcef7","max":251003,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55ec8aab82e24b26ae2cba0cbac783be","value":251003}},"59d0aced5dfe470688d04009c5e7fa69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3dab1f579df48f0a0ca5bcb4c3a2543","placeholder":"​","style":"IPY_MODEL_70de54e4527d44b0a41d2f2f973254ae","value":" 251k/251k [00:00&lt;00:00, 1.15MB/s]"}},"487f763d6e5e4913a0c24aa7d6fce4bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71b64ccd2d0f484e857b14bbd0d03b71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"073d21e8c2b14b73bfa053fcbd1a5728":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21062d8abd404a1fbc17708e6bfdcef7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55ec8aab82e24b26ae2cba0cbac783be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a3dab1f579df48f0a0ca5bcb4c3a2543":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70de54e4527d44b0a41d2f2f973254ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1db9b6a669894f2bb053d6894cce7b92":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0e9b23b4613492093c49b9303409186","IPY_MODEL_3da3623e7c004d379b5dbec3b9c8eeb0","IPY_MODEL_46e4d75c23d547589222d86312eed842"],"layout":"IPY_MODEL_b74dd5bfbd5b4518bb5d7d5735e817d1"}},"b0e9b23b4613492093c49b9303409186":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dd04a175a8c4878be305236ca7809db","placeholder":"​","style":"IPY_MODEL_6304a37a6cb2469784a196e1216cdc9d","value":"config.json: 100%"}},"3da3623e7c004d379b5dbec3b9c8eeb0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57c9de7186e34152a422029528115811","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73179f2542374297828a1e1cbdd22bcb","value":385}},"46e4d75c23d547589222d86312eed842":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5d05527e1d84b38bd1026223548e694","placeholder":"​","style":"IPY_MODEL_ffcf20d9230946a69711c84a56ab3aa6","value":" 385/385 [00:00&lt;00:00, 34.5kB/s]"}},"b74dd5bfbd5b4518bb5d7d5735e817d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dd04a175a8c4878be305236ca7809db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6304a37a6cb2469784a196e1216cdc9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57c9de7186e34152a422029528115811":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73179f2542374297828a1e1cbdd22bcb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5d05527e1d84b38bd1026223548e694":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffcf20d9230946a69711c84a56ab3aa6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cacbd4a001a44c7d851e3844b6766c16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8997628dba214924ae93e70723c9ebf6","IPY_MODEL_7a16e59aad75424b9ea9b7de32040877","IPY_MODEL_53aa52e61fc24f4bb8f9f058809871d9"],"layout":"IPY_MODEL_db38ebba43934d8f8018288579364801"}},"8997628dba214924ae93e70723c9ebf6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ba247839aa74922818de5ea1538e492","placeholder":"​","style":"IPY_MODEL_4d21b4a1cce043f3a8f9ef59fb107968","value":"pytorch_model.bin: 100%"}},"7a16e59aad75424b9ea9b7de32040877":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_13aa2fcf116544a2a946018a4384a979","max":445018508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eead383559f549ca9e37bca8fc810683","value":445018508}},"53aa52e61fc24f4bb8f9f058809871d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_275b1ee961b94beeb17fef3350033917","placeholder":"​","style":"IPY_MODEL_b4eb59a88235492daeeff36142303e7f","value":" 445M/445M [00:01&lt;00:00, 432MB/s]"}},"db38ebba43934d8f8018288579364801":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ba247839aa74922818de5ea1538e492":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d21b4a1cce043f3a8f9ef59fb107968":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13aa2fcf116544a2a946018a4384a979":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eead383559f549ca9e37bca8fc810683":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"275b1ee961b94beeb17fef3350033917":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4eb59a88235492daeeff36142303e7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
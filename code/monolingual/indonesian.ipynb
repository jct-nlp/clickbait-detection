{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51636,"status":"ok","timestamp":1722709671241,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"},"user_tz":-180},"id":"X26qiUbJHNLn","outputId":"80765726-269a-49ea-935b-d8b243ce084d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install torch torchvision transformers pandas"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["97d957532d0f4c70a4e56dd8295215f5","108742a5c1804587a42c67f1f5cd5b25","f88d0d1339354e02a68d3587f81e6d3a","84e06baaa33044b4bf547024b1cf4455","2aee488fddc34e10ae84acb72b555434","279d4352b4724d3084b474b757f6fcf7","8b6a614782d446a4acdd065bf86a1529","37a080f2bb434efe97b76c508acc4cb0","3b6b258f043b495eb409c4e980c17523","ea64ab0c934b4416b09a4723589a0544","87664342f5dd436b90a08a686d4fd46c","cd0fdf0a3ee74e53bc310fa48ff92782","81af3bc6d0704076a1d5d2cc2c44940a","8903894ae680484aa75aeb641719e075","9db5d8adf2eb489481b280c89470af6f","c80cba1873514377b2c38ca4b4b224ca","1e2a7b42d19a4311b171dc0f5822fbb2","2781bffc44874388a85163ee12bbcf20","4054b309d82642f6b56fd914b5718b18","48a79c174d28424d94aa306ccd17db92","0091de15ec7f45c3b315a1943473f7eb","a465983ec2ab43669a8c516c71eecfbd","fe78caf8ab5247a7a77dc89003a68ae1","bbc785e3a57b404e959a96cb02fe2e66","a44f0105445849fd85981ab03801af04","1245da1554ec4a4db888daa70cc245af","dc921392f44544d5b7eb45b679aaba73","1e3dbc0ac0c54d31918ae129092e679a","0990f662ab3a4ae8acb8a6878108b9eb","3a6c2370e37b469d95deda19d74ce6ed","441e1ff055914185a41061b5bfddf7e6","7d7c430539d44b389d38b0349e5fdaaf","74fd7749b4894eaa81f10fb31066ff48","4c3436b219354819a60ba343f9aace72","7b556f8d201d4b47ae1553dda1e3c9f7","397a2a61425c4cd4a3746511ebcab358","6affde2b2b1a406fb304fc3e0e358df3","27181b41ba344b3cb2b9277d27e7f088","e34db46b3a3648ba86181061a1ae416a","f29029ff3c8449888ceff9579e9253fe","b6a9b0d81ac445dda1e5f473695f1850","555f2ad394b14dd7bd0c4263af61529c","0012d805dbe34577b69606dad6c2e0ac","b629cb8232184808b45011f69f2f7d08","669a9ae19cdb476eb692b1eca8ce3f7b","c1647848fa404cddbb034c1bc5b08e7d","da92f09bfb2946d8aa6ece7d942886ca","2aa0a48fc0f74494b0b991fad67280ac","c3b1be20f4154872be1024ccdced54c4","758eec21d81c42b3897dda9dbc7af4f5","6b1b20fc316148e8baf9b72032a03ac0","46ceb213a6804359b38a8c4ad05899fd","768c5e0b9849438c83fed7f52e1709d6","5c3011f356d0481bbc891ea923dbc60d","f554fd0a643743d3bb5df1857d6a2dd2"]},"id":"pPSyyuKOEnOS","executionInfo":{"status":"ok","timestamp":1722711156142,"user_tz":-180,"elapsed":1389206,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"}},"outputId":"45b13976-5276-485b-e4b9-99211dfdc3b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting main process\n","Loading the dataset.\n","Dataset loaded successfully.\n","Cleaning the dataset.\n","Dataset cleaned.\n","Splitting the dataset into training and test sets.\n","Dataset split into training and test sets.\n","Using device: cuda\n","Loading the tokenizer and model.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97d957532d0f4c70a4e56dd8295215f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd0fdf0a3ee74e53bc310fa48ff92782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe78caf8ab5247a7a77dc89003a68ae1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c3436b219354819a60ba343f9aace72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"669a9ae19cdb476eb692b1eca8ce3f7b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model and tokenizer loaded and moved to device.\n","Creating data loaders.\n","Data loaders created.\n","Optimizer, scheduler, and loss function defined.\n","Starting epoch 1/10\n","Batch 0/750, Loss: 0.6719750761985779\n","Batch 10/750, Loss: 0.543975830078125\n","Batch 20/750, Loss: 0.4164557456970215\n","Batch 30/750, Loss: 0.4579352140426636\n","Batch 40/750, Loss: 0.582927942276001\n","Batch 50/750, Loss: 0.39499735832214355\n","Batch 60/750, Loss: 0.2040656954050064\n","Batch 70/750, Loss: 0.43764761090278625\n","Batch 80/750, Loss: 0.4249236583709717\n","Batch 90/750, Loss: 0.5391281247138977\n","Batch 100/750, Loss: 0.45382338762283325\n","Batch 110/750, Loss: 0.3951532244682312\n","Batch 120/750, Loss: 0.4378443956375122\n","Batch 130/750, Loss: 0.5251945853233337\n","Batch 140/750, Loss: 0.40104496479034424\n","Batch 150/750, Loss: 0.74666827917099\n","Batch 160/750, Loss: 0.4587329924106598\n","Batch 170/750, Loss: 0.46013686060905457\n","Batch 180/750, Loss: 0.466461718082428\n","Batch 190/750, Loss: 0.3960007131099701\n","Batch 200/750, Loss: 0.38034236431121826\n","Batch 210/750, Loss: 0.34987136721611023\n","Batch 220/750, Loss: 0.48483505845069885\n","Batch 230/750, Loss: 0.5167908668518066\n","Batch 240/750, Loss: 0.24141521751880646\n","Batch 250/750, Loss: 0.32692354917526245\n","Batch 260/750, Loss: 0.4540855884552002\n","Batch 270/750, Loss: 0.730521559715271\n","Batch 280/750, Loss: 0.39897620677948\n","Batch 290/750, Loss: 0.47960057854652405\n","Batch 300/750, Loss: 0.3983044922351837\n","Batch 310/750, Loss: 0.6048843264579773\n","Batch 320/750, Loss: 0.30348843336105347\n","Batch 330/750, Loss: 0.20490895211696625\n","Batch 340/750, Loss: 0.3141525685787201\n","Batch 350/750, Loss: 0.2722756862640381\n","Batch 360/750, Loss: 0.21066200733184814\n","Batch 370/750, Loss: 0.4165840446949005\n","Batch 380/750, Loss: 0.30064791440963745\n","Batch 390/750, Loss: 0.4247385859489441\n","Batch 400/750, Loss: 0.3556171655654907\n","Batch 410/750, Loss: 0.12208560109138489\n","Batch 420/750, Loss: 0.5406755208969116\n","Batch 430/750, Loss: 0.32591545581817627\n","Batch 440/750, Loss: 0.5134639143943787\n","Batch 450/750, Loss: 0.5441638231277466\n","Batch 460/750, Loss: 0.21673814952373505\n","Batch 470/750, Loss: 0.23211055994033813\n","Batch 480/750, Loss: 0.22366639971733093\n","Batch 490/750, Loss: 0.536318302154541\n","Batch 500/750, Loss: 0.40990835428237915\n","Batch 510/750, Loss: 0.7448922991752625\n","Batch 520/750, Loss: 0.23309016227722168\n","Batch 530/750, Loss: 0.41317471861839294\n","Batch 540/750, Loss: 0.3233015835285187\n","Batch 550/750, Loss: 0.16401642560958862\n","Batch 560/750, Loss: 0.26957014203071594\n","Batch 570/750, Loss: 0.5812489986419678\n","Batch 580/750, Loss: 0.3329465389251709\n","Batch 590/750, Loss: 0.5332428216934204\n","Batch 600/750, Loss: 0.23888415098190308\n","Batch 610/750, Loss: 0.6192213892936707\n","Batch 620/750, Loss: 0.663214385509491\n","Batch 630/750, Loss: 0.4871385991573334\n","Batch 640/750, Loss: 0.2950323224067688\n","Batch 650/750, Loss: 0.461138516664505\n","Batch 660/750, Loss: 0.3168148398399353\n","Batch 670/750, Loss: 0.36602073907852173\n","Batch 680/750, Loss: 0.3379039466381073\n","Batch 690/750, Loss: 0.37243732810020447\n","Batch 700/750, Loss: 0.3591930568218231\n","Batch 710/750, Loss: 0.17897340655326843\n","Batch 720/750, Loss: 0.41977134346961975\n","Batch 730/750, Loss: 0.5373473763465881\n","Batch 740/750, Loss: 0.3563553988933563\n","Training epoch completed in: 2m 4s\n","Train loss 0.42557926606138546 accuracy 0.8086666666666666\n","Batch 0/188, Test Loss: 0.6578388214111328\n","Batch 10/188, Test Loss: 0.37044501304626465\n","Batch 20/188, Test Loss: 0.6302309632301331\n","Batch 30/188, Test Loss: 0.3540324568748474\n","Batch 40/188, Test Loss: 0.2685673236846924\n","Batch 50/188, Test Loss: 0.5242670178413391\n","Batch 60/188, Test Loss: 0.3831142485141754\n","Batch 70/188, Test Loss: 0.5185106992721558\n","Batch 80/188, Test Loss: 0.31253042817115784\n","Batch 90/188, Test Loss: 0.3699333667755127\n","Batch 100/188, Test Loss: 0.1563657820224762\n","Batch 110/188, Test Loss: 0.4767979085445404\n","Batch 120/188, Test Loss: 0.19310376048088074\n","Batch 130/188, Test Loss: 0.40687987208366394\n","Batch 140/188, Test Loss: 0.17507204413414001\n","Batch 150/188, Test Loss: 0.34220871329307556\n","Batch 160/188, Test Loss: 0.5944782495498657\n","Batch 170/188, Test Loss: 0.6835812926292419\n","Batch 180/188, Test Loss: 0.4446794390678406\n","Test evaluation completed in: 0m 10s\n","Test loss 0.4060334957184944 accuracy 0.82\n","Test Precision: 0.8356205815869814\n","Test Recall: 0.82\n","Test F1 Score: 0.8113778163991049\n","Starting epoch 2/10\n","Batch 0/750, Loss: 0.3085038363933563\n","Batch 10/750, Loss: 0.570963978767395\n","Batch 20/750, Loss: 0.2669508457183838\n","Batch 30/750, Loss: 0.23220431804656982\n","Batch 40/750, Loss: 0.319518506526947\n","Batch 50/750, Loss: 0.1959485113620758\n","Batch 60/750, Loss: 0.12695010006427765\n","Batch 70/750, Loss: 0.40438756346702576\n","Batch 80/750, Loss: 0.11642181873321533\n","Batch 90/750, Loss: 0.583576500415802\n","Batch 100/750, Loss: 0.3084941804409027\n","Batch 110/750, Loss: 0.31847110390663147\n","Batch 120/750, Loss: 0.264492392539978\n","Batch 130/750, Loss: 0.4698467254638672\n","Batch 140/750, Loss: 0.33662307262420654\n","Batch 150/750, Loss: 0.6371005177497864\n","Batch 160/750, Loss: 0.35387560725212097\n","Batch 170/750, Loss: 0.33398351073265076\n","Batch 180/750, Loss: 0.34231576323509216\n","Batch 190/750, Loss: 0.17320390045642853\n","Batch 200/750, Loss: 0.20566584169864655\n","Batch 210/750, Loss: 0.33800840377807617\n","Batch 220/750, Loss: 0.2370925396680832\n","Batch 230/750, Loss: 0.4062809348106384\n","Batch 240/750, Loss: 0.10716752707958221\n","Batch 250/750, Loss: 0.2800387740135193\n","Batch 260/750, Loss: 0.258136510848999\n","Batch 270/750, Loss: 0.7282007932662964\n","Batch 280/750, Loss: 0.32240575551986694\n","Batch 290/750, Loss: 0.3444441556930542\n","Batch 300/750, Loss: 0.24157927930355072\n","Batch 310/750, Loss: 0.2973562479019165\n","Batch 320/750, Loss: 0.22239874303340912\n","Batch 330/750, Loss: 0.03855081647634506\n","Batch 340/750, Loss: 0.18654778599739075\n","Batch 350/750, Loss: 0.21741542220115662\n","Batch 360/750, Loss: 0.16778422892093658\n","Batch 370/750, Loss: 0.3718641996383667\n","Batch 380/750, Loss: 0.32224416732788086\n","Batch 390/750, Loss: 0.19468086957931519\n","Batch 400/750, Loss: 0.21911785006523132\n","Batch 410/750, Loss: 0.06823951005935669\n","Batch 420/750, Loss: 0.4497761130332947\n","Batch 430/750, Loss: 0.10856519639492035\n","Batch 440/750, Loss: 0.3328733444213867\n","Batch 450/750, Loss: 0.532843291759491\n","Batch 460/750, Loss: 0.14198021590709686\n","Batch 470/750, Loss: 0.14705915749073029\n","Batch 480/750, Loss: 0.24919728934764862\n","Batch 490/750, Loss: 0.2913946509361267\n","Batch 500/750, Loss: 0.3518785834312439\n","Batch 510/750, Loss: 0.6548840999603271\n","Batch 520/750, Loss: 0.12995389103889465\n","Batch 530/750, Loss: 0.2661696970462799\n","Batch 540/750, Loss: 0.21141621470451355\n","Batch 550/750, Loss: 0.1443428248167038\n","Batch 560/750, Loss: 0.1918407380580902\n","Batch 570/750, Loss: 0.41235509514808655\n","Batch 580/750, Loss: 0.21259844303131104\n","Batch 590/750, Loss: 0.4115643799304962\n","Batch 600/750, Loss: 0.11383601278066635\n","Batch 610/750, Loss: 0.5308272242546082\n","Batch 620/750, Loss: 0.549410879611969\n","Batch 630/750, Loss: 0.3423227369785309\n","Batch 640/750, Loss: 0.1183169037103653\n","Batch 650/750, Loss: 0.29625001549720764\n","Batch 660/750, Loss: 0.2270345389842987\n","Batch 670/750, Loss: 0.1208992674946785\n","Batch 680/750, Loss: 0.16923512518405914\n","Batch 690/750, Loss: 0.32242274284362793\n","Batch 700/750, Loss: 0.287906289100647\n","Batch 710/750, Loss: 0.0800589993596077\n","Batch 720/750, Loss: 0.2772280275821686\n","Batch 730/750, Loss: 0.47943127155303955\n","Batch 740/750, Loss: 0.2747894823551178\n","Training epoch completed in: 2m 7s\n","Train loss 0.30510747197767096 accuracy 0.8776666666666666\n","Batch 0/188, Test Loss: 0.647663414478302\n","Batch 10/188, Test Loss: 0.3121475875377655\n","Batch 20/188, Test Loss: 0.622977077960968\n","Batch 30/188, Test Loss: 0.47683852910995483\n","Batch 40/188, Test Loss: 0.28779566287994385\n","Batch 50/188, Test Loss: 0.5216760039329529\n","Batch 60/188, Test Loss: 0.4736563563346863\n","Batch 70/188, Test Loss: 0.4732474386692047\n","Batch 80/188, Test Loss: 0.21739883720874786\n","Batch 90/188, Test Loss: 0.31660014390945435\n","Batch 100/188, Test Loss: 0.0863112360239029\n","Batch 110/188, Test Loss: 0.3879028558731079\n","Batch 120/188, Test Loss: 0.2081957757472992\n","Batch 130/188, Test Loss: 0.4854699671268463\n","Batch 140/188, Test Loss: 0.11712409555912018\n","Batch 150/188, Test Loss: 0.3017095923423767\n","Batch 160/188, Test Loss: 0.7222210168838501\n","Batch 170/188, Test Loss: 0.7774119973182678\n","Batch 180/188, Test Loss: 0.43569862842559814\n","Test evaluation completed in: 0m 10s\n","Test loss 0.41115101414950606 accuracy 0.8313333333333333\n","Test Precision: 0.8375460204587953\n","Test Recall: 0.8313333333333334\n","Test F1 Score: 0.8263081360207877\n","Starting epoch 3/10\n","Batch 0/750, Loss: 0.3137817978858948\n","Batch 10/750, Loss: 0.4600532054901123\n","Batch 20/750, Loss: 0.14365535974502563\n","Batch 30/750, Loss: 0.11314044147729874\n","Batch 40/750, Loss: 0.27366697788238525\n","Batch 50/750, Loss: 0.07686226814985275\n","Batch 60/750, Loss: 0.03349017724394798\n","Batch 70/750, Loss: 0.2000071108341217\n","Batch 80/750, Loss: 0.05053291842341423\n","Batch 90/750, Loss: 0.3657691180706024\n","Batch 100/750, Loss: 0.09792916476726532\n","Batch 110/750, Loss: 0.35911113023757935\n","Batch 120/750, Loss: 0.15010280907154083\n","Batch 130/750, Loss: 0.20199716091156006\n","Batch 140/750, Loss: 0.28798601031303406\n","Batch 150/750, Loss: 0.30764931440353394\n","Batch 160/750, Loss: 0.11036168783903122\n","Batch 170/750, Loss: 0.3384682834148407\n","Batch 180/750, Loss: 0.4378398358821869\n","Batch 190/750, Loss: 0.04795277491211891\n","Batch 200/750, Loss: 0.056383274495601654\n","Batch 210/750, Loss: 0.1463313102722168\n","Batch 220/750, Loss: 0.2604678273200989\n","Batch 230/750, Loss: 0.09725935012102127\n","Batch 240/750, Loss: 0.07758163660764694\n","Batch 250/750, Loss: 0.23857702314853668\n","Batch 260/750, Loss: 0.07549622654914856\n","Batch 270/750, Loss: 0.24884214997291565\n","Batch 280/750, Loss: 0.04580265283584595\n","Batch 290/750, Loss: 0.24843233823776245\n","Batch 300/750, Loss: 0.06670351326465607\n","Batch 310/750, Loss: 0.22813518345355988\n","Batch 320/750, Loss: 0.12044783681631088\n","Batch 330/750, Loss: 0.03708403557538986\n","Batch 340/750, Loss: 0.13656318187713623\n","Batch 350/750, Loss: 0.10355792939662933\n","Batch 360/750, Loss: 0.11132773011922836\n","Batch 370/750, Loss: 0.19286416471004486\n","Batch 380/750, Loss: 0.21089302003383636\n","Batch 390/750, Loss: 0.1753006875514984\n","Batch 400/750, Loss: 0.15397393703460693\n","Batch 410/750, Loss: 0.03548477962613106\n","Batch 420/750, Loss: 0.5272647142410278\n","Batch 430/750, Loss: 0.09568480402231216\n","Batch 440/750, Loss: 0.12432853132486343\n","Batch 450/750, Loss: 0.42599111795425415\n","Batch 460/750, Loss: 0.1281021535396576\n","Batch 470/750, Loss: 0.04948863387107849\n","Batch 480/750, Loss: 0.173170268535614\n","Batch 490/750, Loss: 0.14100340008735657\n","Batch 500/750, Loss: 0.12616349756717682\n","Batch 510/750, Loss: 0.30428460240364075\n","Batch 520/750, Loss: 0.03284718841314316\n","Batch 530/750, Loss: 0.188650980591774\n","Batch 540/750, Loss: 0.09170400351285934\n","Batch 550/750, Loss: 0.042839568108320236\n","Batch 560/750, Loss: 0.06610497832298279\n","Batch 570/750, Loss: 0.47672387957572937\n","Batch 580/750, Loss: 0.1676749289035797\n","Batch 590/750, Loss: 0.07669579982757568\n","Batch 600/750, Loss: 0.10089249163866043\n","Batch 610/750, Loss: 0.49016454815864563\n","Batch 620/750, Loss: 0.2209269106388092\n","Batch 630/750, Loss: 0.2682201862335205\n","Batch 640/750, Loss: 0.05422346293926239\n","Batch 650/750, Loss: 0.27173569798469543\n","Batch 660/750, Loss: 0.020744524896144867\n","Batch 670/750, Loss: 0.020929398015141487\n","Batch 680/750, Loss: 0.032404616475105286\n","Batch 690/750, Loss: 0.046252742409706116\n","Batch 700/750, Loss: 0.06988165527582169\n","Batch 710/750, Loss: 0.012932329438626766\n","Batch 720/750, Loss: 0.19193600118160248\n","Batch 730/750, Loss: 0.345171183347702\n","Batch 740/750, Loss: 0.108505018055439\n","Training epoch completed in: 2m 7s\n","Train loss 0.18796861345445115 accuracy 0.9273333333333333\n","Batch 0/188, Test Loss: 0.6567592620849609\n","Batch 10/188, Test Loss: 0.5188141465187073\n","Batch 20/188, Test Loss: 0.5222007036209106\n","Batch 30/188, Test Loss: 0.4579647481441498\n","Batch 40/188, Test Loss: 0.3961641490459442\n","Batch 50/188, Test Loss: 0.6244550347328186\n","Batch 60/188, Test Loss: 0.5812769532203674\n","Batch 70/188, Test Loss: 0.2029651403427124\n","Batch 80/188, Test Loss: 0.21842360496520996\n","Batch 90/188, Test Loss: 0.4677525758743286\n","Batch 100/188, Test Loss: 0.1904781013727188\n","Batch 110/188, Test Loss: 0.2063126116991043\n","Batch 120/188, Test Loss: 0.34914419054985046\n","Batch 130/188, Test Loss: 0.2605200409889221\n","Batch 140/188, Test Loss: 0.21666169166564941\n","Batch 150/188, Test Loss: 0.2925775647163391\n","Batch 160/188, Test Loss: 1.0618603229522705\n","Batch 170/188, Test Loss: 0.7545301914215088\n","Batch 180/188, Test Loss: 0.38848474621772766\n","Test evaluation completed in: 0m 10s\n","Test loss 0.5024036623022341 accuracy 0.826\n","Test Precision: 0.825226289664122\n","Test Recall: 0.826\n","Test F1 Score: 0.8254296144533432\n","Starting epoch 4/10\n","Batch 0/750, Loss: 0.11465146392583847\n","Batch 10/750, Loss: 0.23751354217529297\n","Batch 20/750, Loss: 0.04214932769536972\n","Batch 30/750, Loss: 0.15146639943122864\n","Batch 40/750, Loss: 0.029788030311465263\n","Batch 50/750, Loss: 0.011001006700098515\n","Batch 60/750, Loss: 0.06765938550233841\n","Batch 70/750, Loss: 0.08619385212659836\n","Batch 80/750, Loss: 0.15292488038539886\n","Batch 90/750, Loss: 0.3057381212711334\n","Batch 100/750, Loss: 0.09070593118667603\n","Batch 110/750, Loss: 0.03251789137721062\n","Batch 120/750, Loss: 0.27375417947769165\n","Batch 130/750, Loss: 0.040724050253629684\n","Batch 140/750, Loss: 0.08860525488853455\n","Batch 150/750, Loss: 0.14150024950504303\n","Batch 160/750, Loss: 0.031103093177080154\n","Batch 170/750, Loss: 0.07450472563505173\n","Batch 180/750, Loss: 0.29719680547714233\n","Batch 190/750, Loss: 0.05574195086956024\n","Batch 200/750, Loss: 0.1229245662689209\n","Batch 210/750, Loss: 0.04570555314421654\n","Batch 220/750, Loss: 0.08384650200605392\n","Batch 230/750, Loss: 0.2757311165332794\n","Batch 240/750, Loss: 0.029630687087774277\n","Batch 250/750, Loss: 0.026594189926981926\n","Batch 260/750, Loss: 0.011914298869669437\n","Batch 270/750, Loss: 0.0332551971077919\n","Batch 280/750, Loss: 0.014550311490893364\n","Batch 290/750, Loss: 0.2336244136095047\n","Batch 300/750, Loss: 0.03533346205949783\n","Batch 310/750, Loss: 0.05727524682879448\n","Batch 320/750, Loss: 0.008937953040003777\n","Batch 330/750, Loss: 0.00851146224886179\n","Batch 340/750, Loss: 0.026767157018184662\n","Batch 350/750, Loss: 0.056478921324014664\n","Batch 360/750, Loss: 0.07271675020456314\n","Batch 370/750, Loss: 0.041689638048410416\n","Batch 380/750, Loss: 0.1910366266965866\n","Batch 390/750, Loss: 0.029941769316792488\n","Batch 400/750, Loss: 0.11742926388978958\n","Batch 410/750, Loss: 0.03230833634734154\n","Batch 420/750, Loss: 0.36568936705589294\n","Batch 430/750, Loss: 0.039518821984529495\n","Batch 440/750, Loss: 0.10454750806093216\n","Batch 450/750, Loss: 0.3845030963420868\n","Batch 460/750, Loss: 0.08219555765390396\n","Batch 470/750, Loss: 0.02787402644753456\n","Batch 480/750, Loss: 0.2527478039264679\n","Batch 490/750, Loss: 0.06310980021953583\n","Batch 500/750, Loss: 0.05052939057350159\n","Batch 510/750, Loss: 0.3382163345813751\n","Batch 520/750, Loss: 0.020757680758833885\n","Batch 530/750, Loss: 0.11141269654035568\n","Batch 540/750, Loss: 0.10666432231664658\n","Batch 550/750, Loss: 0.20194463431835175\n","Batch 560/750, Loss: 0.0274665504693985\n","Batch 570/750, Loss: 0.5316310524940491\n","Batch 580/750, Loss: 0.07375767827033997\n","Batch 590/750, Loss: 0.16253291070461273\n","Batch 600/750, Loss: 0.020173773169517517\n","Batch 610/750, Loss: 0.3153126537799835\n","Batch 620/750, Loss: 0.07052408903837204\n","Batch 630/750, Loss: 0.05781881883740425\n","Batch 640/750, Loss: 0.017978614196181297\n","Batch 650/750, Loss: 0.27633023262023926\n","Batch 660/750, Loss: 0.012340283021330833\n","Batch 670/750, Loss: 0.05780797824263573\n","Batch 680/750, Loss: 0.011707587167620659\n","Batch 690/750, Loss: 0.05533690005540848\n","Batch 700/750, Loss: 0.013944548554718494\n","Batch 710/750, Loss: 0.02999391406774521\n","Batch 720/750, Loss: 0.05694234371185303\n","Batch 730/750, Loss: 0.4223170876502991\n","Batch 740/750, Loss: 0.0246201790869236\n","Training epoch completed in: 2m 7s\n","Train loss 0.11446383359531562 accuracy 0.9596666666666667\n","Batch 0/188, Test Loss: 0.7802552580833435\n","Batch 10/188, Test Loss: 0.6021807789802551\n","Batch 20/188, Test Loss: 0.6985694766044617\n","Batch 30/188, Test Loss: 0.7672086954116821\n","Batch 40/188, Test Loss: 0.3771418035030365\n","Batch 50/188, Test Loss: 0.8375653028488159\n","Batch 60/188, Test Loss: 0.7732762694358826\n","Batch 70/188, Test Loss: 0.5673170685768127\n","Batch 80/188, Test Loss: 0.2450304478406906\n","Batch 90/188, Test Loss: 0.5033560991287231\n","Batch 100/188, Test Loss: 0.23746299743652344\n","Batch 110/188, Test Loss: 0.4505380690097809\n","Batch 120/188, Test Loss: 0.15436206758022308\n","Batch 130/188, Test Loss: 0.3015674650669098\n","Batch 140/188, Test Loss: 0.12979669868946075\n","Batch 150/188, Test Loss: 0.3366532623767853\n","Batch 160/188, Test Loss: 1.2715175151824951\n","Batch 170/188, Test Loss: 0.9517976641654968\n","Batch 180/188, Test Loss: 0.36594921350479126\n","Test evaluation completed in: 0m 10s\n","Test loss 0.5813659211680134 accuracy 0.8296666666666667\n","Test Precision: 0.8289561445735235\n","Test Recall: 0.8296666666666667\n","Test F1 Score: 0.828027095246218\n","Starting epoch 5/10\n","Batch 0/750, Loss: 0.02631501480937004\n","Batch 10/750, Loss: 0.11830315738916397\n","Batch 20/750, Loss: 0.018859263509511948\n","Batch 30/750, Loss: 0.07371016591787338\n","Batch 40/750, Loss: 0.023089034482836723\n","Batch 50/750, Loss: 0.0040947250090539455\n","Batch 60/750, Loss: 0.005037697963416576\n","Batch 70/750, Loss: 0.05442136526107788\n","Batch 80/750, Loss: 0.05678492784500122\n","Batch 90/750, Loss: 0.32172438502311707\n","Batch 100/750, Loss: 0.2513978183269501\n","Batch 110/750, Loss: 0.017780225723981857\n","Batch 120/750, Loss: 0.024958396330475807\n","Batch 130/750, Loss: 0.04015888646245003\n","Batch 140/750, Loss: 0.09708978235721588\n","Batch 150/750, Loss: 0.012599245645105839\n","Batch 160/750, Loss: 0.0075816684402525425\n","Batch 170/750, Loss: 0.010508148930966854\n","Batch 180/750, Loss: 0.04634081944823265\n","Batch 190/750, Loss: 0.034083042293787\n","Batch 200/750, Loss: 0.009106290526688099\n","Batch 210/750, Loss: 0.015986235812306404\n","Batch 220/750, Loss: 0.005589084699749947\n","Batch 230/750, Loss: 0.041280265897512436\n","Batch 240/750, Loss: 0.04659528285264969\n","Batch 250/750, Loss: 0.016565464437007904\n","Batch 260/750, Loss: 0.03928147628903389\n","Batch 270/750, Loss: 0.036125898361206055\n","Batch 280/750, Loss: 0.021733814850449562\n","Batch 290/750, Loss: 0.040419116616249084\n","Batch 300/750, Loss: 0.007943741045892239\n","Batch 310/750, Loss: 0.007657918147742748\n","Batch 320/750, Loss: 0.039432086050510406\n","Batch 330/750, Loss: 0.007780585438013077\n","Batch 340/750, Loss: 0.18537698686122894\n","Batch 350/750, Loss: 0.0446961484849453\n","Batch 360/750, Loss: 0.04388407990336418\n","Batch 370/750, Loss: 0.056731633841991425\n","Batch 380/750, Loss: 0.011277012526988983\n","Batch 390/750, Loss: 0.18578043580055237\n","Batch 400/750, Loss: 0.03320234268903732\n","Batch 410/750, Loss: 0.02140703611075878\n","Batch 420/750, Loss: 0.3408505618572235\n","Batch 430/750, Loss: 0.027643511071801186\n","Batch 440/750, Loss: 0.08070524036884308\n","Batch 450/750, Loss: 0.08915793895721436\n","Batch 460/750, Loss: 0.020888756960630417\n","Batch 470/750, Loss: 0.0652126744389534\n","Batch 480/750, Loss: 0.1303572803735733\n","Batch 490/750, Loss: 0.03808829188346863\n","Batch 500/750, Loss: 0.012238706462085247\n","Batch 510/750, Loss: 0.02985064499080181\n","Batch 520/750, Loss: 0.013172196224331856\n","Batch 530/750, Loss: 0.029225891456007957\n","Batch 540/750, Loss: 0.003138432279229164\n","Batch 550/750, Loss: 0.00264516519382596\n","Batch 560/750, Loss: 0.013606302440166473\n","Batch 570/750, Loss: 0.03398262336850166\n","Batch 580/750, Loss: 0.016286280006170273\n","Batch 590/750, Loss: 0.0050021689385175705\n","Batch 600/750, Loss: 0.011494985781610012\n","Batch 610/750, Loss: 0.2032388150691986\n","Batch 620/750, Loss: 0.02425643801689148\n","Batch 630/750, Loss: 0.019758010283112526\n","Batch 640/750, Loss: 0.009265461005270481\n","Batch 650/750, Loss: 0.1871330589056015\n","Batch 660/750, Loss: 0.00788877159357071\n","Batch 670/750, Loss: 0.032765649259090424\n","Batch 680/750, Loss: 0.010652893222868443\n","Batch 690/750, Loss: 0.0037287496961653233\n","Batch 700/750, Loss: 0.010542217642068863\n","Batch 710/750, Loss: 0.0032111096661537886\n","Batch 720/750, Loss: 0.017286071553826332\n","Batch 730/750, Loss: 0.616475522518158\n","Batch 740/750, Loss: 0.0018348356243222952\n","Training epoch completed in: 2m 7s\n","Train loss 0.06607696330733598 accuracy 0.9778333333333333\n","Batch 0/188, Test Loss: 0.915518045425415\n","Batch 10/188, Test Loss: 0.6900343894958496\n","Batch 20/188, Test Loss: 1.175654411315918\n","Batch 30/188, Test Loss: 1.0194499492645264\n","Batch 40/188, Test Loss: 0.6747815608978271\n","Batch 50/188, Test Loss: 1.3989739418029785\n","Batch 60/188, Test Loss: 1.0998268127441406\n","Batch 70/188, Test Loss: 0.46387219429016113\n","Batch 80/188, Test Loss: 0.39425432682037354\n","Batch 90/188, Test Loss: 0.9023070335388184\n","Batch 100/188, Test Loss: 0.19253498315811157\n","Batch 110/188, Test Loss: 0.324379563331604\n","Batch 120/188, Test Loss: 0.47196513414382935\n","Batch 130/188, Test Loss: 0.3276436924934387\n","Batch 140/188, Test Loss: 0.5022855997085571\n","Batch 150/188, Test Loss: 0.3594329059123993\n","Batch 160/188, Test Loss: 1.4856889247894287\n","Batch 170/188, Test Loss: 0.9655982851982117\n","Batch 180/188, Test Loss: 0.3818454146385193\n","Test evaluation completed in: 0m 10s\n","Test loss 0.7478151041122668 accuracy 0.819\n","Test Precision: 0.8194648175366726\n","Test Recall: 0.819\n","Test F1 Score: 0.8192056125277113\n","Starting epoch 6/10\n","Batch 0/750, Loss: 0.014831393957138062\n","Batch 10/750, Loss: 0.1076667308807373\n","Batch 20/750, Loss: 0.004880319349467754\n","Batch 30/750, Loss: 0.006338859908282757\n","Batch 40/750, Loss: 0.016429541632533073\n","Batch 50/750, Loss: 0.17298635840415955\n","Batch 60/750, Loss: 0.004502688068896532\n","Batch 70/750, Loss: 0.0036005089059472084\n","Batch 80/750, Loss: 0.004386567510664463\n","Batch 90/750, Loss: 0.04543701931834221\n","Batch 100/750, Loss: 0.005622155964374542\n","Batch 110/750, Loss: 0.02513418346643448\n","Batch 120/750, Loss: 0.034526124596595764\n","Batch 130/750, Loss: 0.003551170462742448\n","Batch 140/750, Loss: 0.0035040259826928377\n","Batch 150/750, Loss: 0.006095872726291418\n","Batch 160/750, Loss: 0.007375963497906923\n","Batch 170/750, Loss: 0.23324677348136902\n","Batch 180/750, Loss: 0.026369350031018257\n","Batch 190/750, Loss: 0.012820110656321049\n","Batch 200/750, Loss: 0.020029230043292046\n","Batch 210/750, Loss: 0.010609416291117668\n","Batch 220/750, Loss: 0.07970958203077316\n","Batch 230/750, Loss: 0.020235169678926468\n","Batch 240/750, Loss: 0.008688039146363735\n","Batch 250/750, Loss: 0.003611526917666197\n","Batch 260/750, Loss: 0.0024712844751775265\n","Batch 270/750, Loss: 0.0044417427852749825\n","Batch 280/750, Loss: 0.029829829931259155\n","Batch 290/750, Loss: 0.12244082987308502\n","Batch 300/750, Loss: 0.037908900529146194\n","Batch 310/750, Loss: 0.005052833817899227\n","Batch 320/750, Loss: 0.018262797966599464\n","Batch 330/750, Loss: 0.048930488526821136\n","Batch 340/750, Loss: 0.0016404156340286136\n","Batch 350/750, Loss: 0.0018602353520691395\n","Batch 360/750, Loss: 0.006248401943594217\n","Batch 370/750, Loss: 0.0009704986587166786\n","Batch 380/750, Loss: 0.029734008014202118\n","Batch 390/750, Loss: 0.0039061009883880615\n","Batch 400/750, Loss: 0.051737572997808456\n","Batch 410/750, Loss: 0.057071834802627563\n","Batch 420/750, Loss: 0.2521666884422302\n","Batch 430/750, Loss: 0.011884665116667747\n","Batch 440/750, Loss: 0.013799401000142097\n","Batch 450/750, Loss: 0.003092718543484807\n","Batch 460/750, Loss: 0.0056180329993367195\n","Batch 470/750, Loss: 0.0017455604393035173\n","Batch 480/750, Loss: 0.008804132230579853\n","Batch 490/750, Loss: 0.006415459793061018\n","Batch 500/750, Loss: 0.0036705187521874905\n","Batch 510/750, Loss: 0.0068268366158008575\n","Batch 520/750, Loss: 0.001445310190320015\n","Batch 530/750, Loss: 0.052903663367033005\n","Batch 540/750, Loss: 0.0009983947966247797\n","Batch 550/750, Loss: 0.002307765418663621\n","Batch 560/750, Loss: 0.024228893220424652\n","Batch 570/750, Loss: 0.005459526088088751\n","Batch 580/750, Loss: 0.011935805901885033\n","Batch 590/750, Loss: 0.00220694812014699\n","Batch 600/750, Loss: 0.003380898851901293\n","Batch 610/750, Loss: 0.10199081897735596\n","Batch 620/750, Loss: 0.045397400856018066\n","Batch 630/750, Loss: 0.004061534069478512\n","Batch 640/750, Loss: 0.015418993309140205\n","Batch 650/750, Loss: 0.11387374997138977\n","Batch 660/750, Loss: 0.00423083733767271\n","Batch 670/750, Loss: 0.0150523092597723\n","Batch 680/750, Loss: 0.0044775730930268764\n","Batch 690/750, Loss: 0.057695191353559494\n","Batch 700/750, Loss: 0.006001970265060663\n","Batch 710/750, Loss: 0.005212258081883192\n","Batch 720/750, Loss: 0.0034840828739106655\n","Batch 730/750, Loss: 0.4652312994003296\n","Batch 740/750, Loss: 0.0025231875479221344\n","Training epoch completed in: 2m 7s\n","Train loss 0.03972887048575406 accuracy 0.9865833333333333\n","Batch 0/188, Test Loss: 1.2123610973358154\n","Batch 10/188, Test Loss: 1.3972092866897583\n","Batch 20/188, Test Loss: 1.2741223573684692\n","Batch 30/188, Test Loss: 0.9298356771469116\n","Batch 40/188, Test Loss: 0.9809476137161255\n","Batch 50/188, Test Loss: 1.5525248050689697\n","Batch 60/188, Test Loss: 0.7882705330848694\n","Batch 70/188, Test Loss: 0.7928751707077026\n","Batch 80/188, Test Loss: 0.4074226915836334\n","Batch 90/188, Test Loss: 0.5216690897941589\n","Batch 100/188, Test Loss: 0.824094831943512\n","Batch 110/188, Test Loss: 0.3684934973716736\n","Batch 120/188, Test Loss: 0.2626950442790985\n","Batch 130/188, Test Loss: 0.292911559343338\n","Batch 140/188, Test Loss: 0.26496535539627075\n","Batch 150/188, Test Loss: 0.3407576084136963\n","Batch 160/188, Test Loss: 1.364720344543457\n","Batch 170/188, Test Loss: 1.0145258903503418\n","Batch 180/188, Test Loss: 0.4249587953090668\n","Test evaluation completed in: 0m 10s\n","Test loss 0.8201760591176199 accuracy 0.8243333333333333\n","Test Precision: 0.8233305170992535\n","Test Recall: 0.8243333333333334\n","Test F1 Score: 0.8232055641883967\n","Starting epoch 7/10\n","Batch 0/750, Loss: 0.01585378497838974\n","Batch 10/750, Loss: 0.03880322724580765\n","Batch 20/750, Loss: 0.00376275391317904\n","Batch 30/750, Loss: 0.0021615021396428347\n","Batch 40/750, Loss: 0.035565223544836044\n","Batch 50/750, Loss: 0.0019935383461415768\n","Batch 60/750, Loss: 0.0008937782258726656\n","Batch 70/750, Loss: 0.009196482598781586\n","Batch 80/750, Loss: 0.03066013567149639\n","Batch 90/750, Loss: 0.07390228658914566\n","Batch 100/750, Loss: 0.0022547636181116104\n","Batch 110/750, Loss: 0.002430710243061185\n","Batch 120/750, Loss: 0.01015505287796259\n","Batch 130/750, Loss: 0.014611161313951015\n","Batch 140/750, Loss: 0.031921181827783585\n","Batch 150/750, Loss: 0.009074435569345951\n","Batch 160/750, Loss: 0.3453236222267151\n","Batch 170/750, Loss: 0.0039018606767058372\n","Batch 180/750, Loss: 0.034635722637176514\n","Batch 190/750, Loss: 0.0015310572925955057\n","Batch 200/750, Loss: 0.00668412446975708\n","Batch 210/750, Loss: 0.005295605398714542\n","Batch 220/750, Loss: 0.0039445688016712666\n","Batch 230/750, Loss: 0.002783146221190691\n","Batch 240/750, Loss: 0.006583142094314098\n","Batch 250/750, Loss: 0.004964618943631649\n","Batch 260/750, Loss: 0.015850136056542397\n","Batch 270/750, Loss: 0.017875932157039642\n","Batch 280/750, Loss: 0.03434083238244057\n","Batch 290/750, Loss: 0.00415540486574173\n","Batch 300/750, Loss: 0.0007100519142113626\n","Batch 310/750, Loss: 0.002920744242146611\n","Batch 320/750, Loss: 0.0019371436210349202\n","Batch 330/750, Loss: 0.013911280781030655\n","Batch 340/750, Loss: 0.0016984879039227962\n","Batch 350/750, Loss: 0.009708935394883156\n","Batch 360/750, Loss: 0.00392347015440464\n","Batch 370/750, Loss: 0.0010079714702442288\n","Batch 380/750, Loss: 0.00412343442440033\n","Batch 390/750, Loss: 0.0007826555520296097\n","Batch 400/750, Loss: 0.0020939060486853123\n","Batch 410/750, Loss: 0.0014402868691831827\n","Batch 420/750, Loss: 0.006144647020846605\n","Batch 430/750, Loss: 0.0012042884482070804\n","Batch 440/750, Loss: 0.0055210585705935955\n","Batch 450/750, Loss: 0.0010902947979047894\n","Batch 460/750, Loss: 0.02610933966934681\n","Batch 470/750, Loss: 0.0007934103487059474\n","Batch 480/750, Loss: 0.011401927098631859\n","Batch 490/750, Loss: 0.001819593133404851\n","Batch 500/750, Loss: 0.0011039506644010544\n","Batch 510/750, Loss: 0.0020890177693217993\n","Batch 520/750, Loss: 0.0005157936247996986\n","Batch 530/750, Loss: 0.0025793230161070824\n","Batch 540/750, Loss: 0.008536586537957191\n","Batch 550/750, Loss: 0.0008352053700946271\n","Batch 560/750, Loss: 0.0009228707058355212\n","Batch 570/750, Loss: 0.001606747042387724\n","Batch 580/750, Loss: 0.013599295169115067\n","Batch 590/750, Loss: 0.0007332524983212352\n","Batch 600/750, Loss: 0.0042713419534265995\n","Batch 610/750, Loss: 0.027147281914949417\n","Batch 620/750, Loss: 0.0020576794631779194\n","Batch 630/750, Loss: 0.0032907556742429733\n","Batch 640/750, Loss: 0.02318948693573475\n","Batch 650/750, Loss: 0.08225135505199432\n","Batch 660/750, Loss: 0.004091646056622267\n","Batch 670/750, Loss: 0.0014004181139171124\n","Batch 680/750, Loss: 0.0020909642335027456\n","Batch 690/750, Loss: 0.016710586845874786\n","Batch 700/750, Loss: 0.0010765272891148925\n","Batch 710/750, Loss: 0.0005733153666369617\n","Batch 720/750, Loss: 0.005918731912970543\n","Batch 730/750, Loss: 0.6286062002182007\n","Batch 740/750, Loss: 0.0017337050521746278\n","Training epoch completed in: 2m 7s\n","Train loss 0.024863375883898697 accuracy 0.9918333333333333\n","Batch 0/188, Test Loss: 1.316893458366394\n","Batch 10/188, Test Loss: 0.9194782972335815\n","Batch 20/188, Test Loss: 1.3000166416168213\n","Batch 30/188, Test Loss: 1.0958172082901\n","Batch 40/188, Test Loss: 0.43860965967178345\n","Batch 50/188, Test Loss: 2.0009961128234863\n","Batch 60/188, Test Loss: 0.9636664986610413\n","Batch 70/188, Test Loss: 0.8652084469795227\n","Batch 80/188, Test Loss: 0.6985357999801636\n","Batch 90/188, Test Loss: 0.627930223941803\n","Batch 100/188, Test Loss: 0.8827015161514282\n","Batch 110/188, Test Loss: 0.3476429283618927\n","Batch 120/188, Test Loss: 0.3323335647583008\n","Batch 130/188, Test Loss: 0.41896653175354004\n","Batch 140/188, Test Loss: 0.25673604011535645\n","Batch 150/188, Test Loss: 0.5557554364204407\n","Batch 160/188, Test Loss: 1.5318635702133179\n","Batch 170/188, Test Loss: 1.2630996704101562\n","Batch 180/188, Test Loss: 0.5308950543403625\n","Test evaluation completed in: 0m 10s\n","Test loss 0.8689588795743011 accuracy 0.821\n","Test Precision: 0.8200437087259623\n","Test Recall: 0.821\n","Test F1 Score: 0.8201929208753963\n","Starting epoch 8/10\n","Batch 0/750, Loss: 0.019805746152997017\n","Batch 10/750, Loss: 0.011015893891453743\n","Batch 20/750, Loss: 0.009726624935865402\n","Batch 30/750, Loss: 0.003213676158338785\n","Batch 40/750, Loss: 0.0029650707729160786\n","Batch 50/750, Loss: 0.0004919542698189616\n","Batch 60/750, Loss: 0.0004954857286065817\n","Batch 70/750, Loss: 0.004121401812881231\n","Batch 80/750, Loss: 0.003851980436593294\n","Batch 90/750, Loss: 0.0057142144069075584\n","Batch 100/750, Loss: 0.0039059945847839117\n","Batch 110/750, Loss: 0.009416148997843266\n","Batch 120/750, Loss: 0.0015157535672187805\n","Batch 130/750, Loss: 0.004116591066122055\n","Batch 140/750, Loss: 0.0018776850774884224\n","Batch 150/750, Loss: 0.01180849876254797\n","Batch 160/750, Loss: 0.0020804544910788536\n","Batch 170/750, Loss: 0.007887276820838451\n","Batch 180/750, Loss: 0.00795652624219656\n","Batch 190/750, Loss: 0.07210994511842728\n","Batch 200/750, Loss: 0.009688597172498703\n","Batch 210/750, Loss: 0.004149144981056452\n","Batch 220/750, Loss: 0.001000604941509664\n","Batch 230/750, Loss: 0.0015876172110438347\n","Batch 240/750, Loss: 0.0010723829036578536\n","Batch 250/750, Loss: 0.0008969877962954342\n","Batch 260/750, Loss: 0.004697563126683235\n","Batch 270/750, Loss: 0.0034648783039301634\n","Batch 280/750, Loss: 0.037060003727674484\n","Batch 290/750, Loss: 0.0008018945809453726\n","Batch 300/750, Loss: 0.001882525160908699\n","Batch 310/750, Loss: 0.0010028344113379717\n","Batch 320/750, Loss: 0.0016667493619024754\n","Batch 330/750, Loss: 0.005519493483006954\n","Batch 340/750, Loss: 0.0005487191956490278\n","Batch 350/750, Loss: 0.0014012288302183151\n","Batch 360/750, Loss: 0.0006500787567347288\n","Batch 370/750, Loss: 0.0006124009378254414\n","Batch 380/750, Loss: 0.01217721775174141\n","Batch 390/750, Loss: 0.0027938373386859894\n","Batch 400/750, Loss: 0.02618636004626751\n","Batch 410/750, Loss: 0.0006719650118611753\n","Batch 420/750, Loss: 0.007018785458058119\n","Batch 430/750, Loss: 0.000527379394043237\n","Batch 440/750, Loss: 0.007428515236824751\n","Batch 450/750, Loss: 0.0005703018396161497\n","Batch 460/750, Loss: 0.00043394940439611673\n","Batch 470/750, Loss: 0.0004175804788246751\n","Batch 480/750, Loss: 0.0004935052711516619\n","Batch 490/750, Loss: 0.0010213282657787204\n","Batch 500/750, Loss: 0.002642128150910139\n","Batch 510/750, Loss: 0.08155272156000137\n","Batch 520/750, Loss: 0.0008640925516374409\n","Batch 530/750, Loss: 0.0010027446551248431\n","Batch 540/750, Loss: 0.0005080826231278479\n","Batch 550/750, Loss: 0.0008828357676975429\n","Batch 560/750, Loss: 0.0040300157852470875\n","Batch 570/750, Loss: 0.0016682668356224895\n","Batch 580/750, Loss: 0.0019224061397835612\n","Batch 590/750, Loss: 0.0008471041219308972\n","Batch 600/750, Loss: 0.0009157199528999627\n","Batch 610/750, Loss: 0.007907510735094547\n","Batch 620/750, Loss: 0.002762772375717759\n","Batch 630/750, Loss: 0.0026817494072020054\n","Batch 640/750, Loss: 0.0014663732144981623\n","Batch 650/750, Loss: 0.08230294287204742\n","Batch 660/750, Loss: 0.0007725005270913243\n","Batch 670/750, Loss: 0.0005249723326414824\n","Batch 680/750, Loss: 0.00436911778524518\n","Batch 690/750, Loss: 0.0005538060213439167\n","Batch 700/750, Loss: 0.025686850771307945\n","Batch 710/750, Loss: 0.004988398868590593\n","Batch 720/750, Loss: 0.05472978577017784\n","Batch 730/750, Loss: 0.4919022023677826\n","Batch 740/750, Loss: 0.0005266896332614124\n","Training epoch completed in: 2m 7s\n","Train loss 0.017071746051117467 accuracy 0.9950833333333333\n","Batch 0/188, Test Loss: 1.718940019607544\n","Batch 10/188, Test Loss: 0.9010730981826782\n","Batch 20/188, Test Loss: 0.9404661655426025\n","Batch 30/188, Test Loss: 1.0228495597839355\n","Batch 40/188, Test Loss: 0.4028511047363281\n","Batch 50/188, Test Loss: 1.9727905988693237\n","Batch 60/188, Test Loss: 1.1734317541122437\n","Batch 70/188, Test Loss: 0.9699233174324036\n","Batch 80/188, Test Loss: 0.8317614793777466\n","Batch 90/188, Test Loss: 0.3985031843185425\n","Batch 100/188, Test Loss: 0.9746851921081543\n","Batch 110/188, Test Loss: 0.6061645746231079\n","Batch 120/188, Test Loss: 0.3879759609699249\n","Batch 130/188, Test Loss: 0.44243454933166504\n","Batch 140/188, Test Loss: 0.07870420813560486\n","Batch 150/188, Test Loss: 0.6257574558258057\n","Batch 160/188, Test Loss: 1.4753005504608154\n","Batch 170/188, Test Loss: 1.5945621728897095\n","Batch 180/188, Test Loss: 0.765045702457428\n","Test evaluation completed in: 0m 10s\n","Test loss 0.9064383595894546 accuracy 0.8246666666666667\n","Test Precision: 0.823679439499248\n","Test Recall: 0.8246666666666667\n","Test F1 Score: 0.823397518200959\n","Starting epoch 9/10\n","Batch 0/750, Loss: 0.002363529521971941\n","Batch 10/750, Loss: 0.011708050034940243\n","Batch 20/750, Loss: 0.023495329543948174\n","Batch 30/750, Loss: 0.0007358375587500632\n","Batch 40/750, Loss: 0.011294538155198097\n","Batch 50/750, Loss: 0.00029564680880866945\n","Batch 60/750, Loss: 0.00037819542922079563\n","Batch 70/750, Loss: 0.0012809641193598509\n","Batch 80/750, Loss: 0.0008762535871937871\n","Batch 90/750, Loss: 0.004435770213603973\n","Batch 100/750, Loss: 0.005935638211667538\n","Batch 110/750, Loss: 0.12411943823099136\n","Batch 120/750, Loss: 0.0008318532491102815\n","Batch 130/750, Loss: 0.0012007236946374178\n","Batch 140/750, Loss: 0.0007646688027307391\n","Batch 150/750, Loss: 0.002038239035755396\n","Batch 160/750, Loss: 0.0007686380995437503\n","Batch 170/750, Loss: 0.0027993719559162855\n","Batch 180/750, Loss: 0.02909267321228981\n","Batch 190/750, Loss: 0.0009516951395198703\n","Batch 200/750, Loss: 0.0017410016153007746\n","Batch 210/750, Loss: 0.0005508768372237682\n","Batch 220/750, Loss: 0.000764509488362819\n","Batch 230/750, Loss: 0.0009790299227461219\n","Batch 240/750, Loss: 0.000409188331104815\n","Batch 250/750, Loss: 0.0004975346964783967\n","Batch 260/750, Loss: 0.00105579337105155\n","Batch 270/750, Loss: 0.0008263036143034697\n","Batch 280/750, Loss: 0.0026099805254489183\n","Batch 290/750, Loss: 0.0010985047556459904\n","Batch 300/750, Loss: 0.0008720416808500886\n","Batch 310/750, Loss: 0.0011345472885295749\n","Batch 320/750, Loss: 0.0016278413822874427\n","Batch 330/750, Loss: 0.0024446933530271053\n","Batch 340/750, Loss: 0.00044494267785921693\n","Batch 350/750, Loss: 0.001043077209033072\n","Batch 360/750, Loss: 0.0011453564511612058\n","Batch 370/750, Loss: 0.00032395985908806324\n","Batch 380/750, Loss: 0.0009547090739943087\n","Batch 390/750, Loss: 0.0004446392704267055\n","Batch 400/750, Loss: 0.0013537982013076544\n","Batch 410/750, Loss: 0.0006337611703202128\n","Batch 420/750, Loss: 0.05549347400665283\n","Batch 430/750, Loss: 0.0004098584468010813\n","Batch 440/750, Loss: 0.0011344430968165398\n","Batch 450/750, Loss: 0.0006338334642350674\n","Batch 460/750, Loss: 0.0003835934039670974\n","Batch 470/750, Loss: 0.0005408338038250804\n","Batch 480/750, Loss: 0.0005236775614321232\n","Batch 490/750, Loss: 0.12842802703380585\n","Batch 500/750, Loss: 0.0017626453191041946\n","Batch 510/750, Loss: 0.0007540347869507968\n","Batch 520/750, Loss: 0.0006365095032379031\n","Batch 530/750, Loss: 0.002571677789092064\n","Batch 540/750, Loss: 0.0008811700972728431\n","Batch 550/750, Loss: 0.00047780474415048957\n","Batch 560/750, Loss: 0.001542376121506095\n","Batch 570/750, Loss: 0.05041960999369621\n","Batch 580/750, Loss: 0.0014480594545602798\n","Batch 590/750, Loss: 0.00033201047335751355\n","Batch 600/750, Loss: 0.0005534940864890814\n","Batch 610/750, Loss: 0.004538373090326786\n","Batch 620/750, Loss: 0.0011394487228244543\n","Batch 630/750, Loss: 0.024300653487443924\n","Batch 640/750, Loss: 0.0016564906109124422\n","Batch 650/750, Loss: 0.030501563102006912\n","Batch 660/750, Loss: 0.0007392746629193425\n","Batch 670/750, Loss: 0.0007826617220416665\n","Batch 680/750, Loss: 0.04675201326608658\n","Batch 690/750, Loss: 0.0003280978708062321\n","Batch 700/750, Loss: 0.0002844100526999682\n","Batch 710/750, Loss: 0.06693088263273239\n","Batch 720/750, Loss: 0.0006208020495250821\n","Batch 730/750, Loss: 0.48732900619506836\n","Batch 740/750, Loss: 0.0007299210992641747\n","Training epoch completed in: 2m 7s\n","Train loss 0.010190652498917189 accuracy 0.9963333333333333\n","Batch 0/188, Test Loss: 1.7588527202606201\n","Batch 10/188, Test Loss: 0.9517083764076233\n","Batch 20/188, Test Loss: 1.3429151773452759\n","Batch 30/188, Test Loss: 1.1911925077438354\n","Batch 40/188, Test Loss: 0.5048049092292786\n","Batch 50/188, Test Loss: 2.421710252761841\n","Batch 60/188, Test Loss: 1.4962658882141113\n","Batch 70/188, Test Loss: 0.7454965114593506\n","Batch 80/188, Test Loss: 0.7166601419448853\n","Batch 90/188, Test Loss: 0.583418607711792\n","Batch 100/188, Test Loss: 1.022417664527893\n","Batch 110/188, Test Loss: 0.4965560734272003\n","Batch 120/188, Test Loss: 0.6325497031211853\n","Batch 130/188, Test Loss: 0.7731049656867981\n","Batch 140/188, Test Loss: 0.37234368920326233\n","Batch 150/188, Test Loss: 0.7032933831214905\n","Batch 160/188, Test Loss: 1.6394424438476562\n","Batch 170/188, Test Loss: 1.3436106443405151\n","Batch 180/188, Test Loss: 0.8274229764938354\n","Test evaluation completed in: 0m 10s\n","Test loss 0.9925930819008499 accuracy 0.823\n","Test Precision: 0.82351505299566\n","Test Recall: 0.823\n","Test F1 Score: 0.8232238877470831\n","Starting epoch 10/10\n","Batch 0/750, Loss: 0.0008024650742299855\n","Batch 10/750, Loss: 0.0006400936981663108\n","Batch 20/750, Loss: 0.0007489041308872402\n","Batch 30/750, Loss: 0.00038291828241199255\n","Batch 40/750, Loss: 0.0006162482313811779\n","Batch 50/750, Loss: 0.00019336694094818085\n","Batch 60/750, Loss: 0.0002462041738908738\n","Batch 70/750, Loss: 0.0003062478790525347\n","Batch 80/750, Loss: 0.0007155674393288791\n","Batch 90/750, Loss: 0.0012533393455669284\n","Batch 100/750, Loss: 0.019351776689291\n","Batch 110/750, Loss: 0.166818305850029\n","Batch 120/750, Loss: 0.0005107191391289234\n","Batch 130/750, Loss: 0.0006070764502510428\n","Batch 140/750, Loss: 0.0004539749934338033\n","Batch 150/750, Loss: 0.0008716816082596779\n","Batch 160/750, Loss: 0.0024344613775610924\n","Batch 170/750, Loss: 0.0013325582258403301\n","Batch 180/750, Loss: 0.0038859741762280464\n","Batch 190/750, Loss: 0.0010316953994333744\n","Batch 200/750, Loss: 0.0007664281292818487\n","Batch 210/750, Loss: 0.00043095441651530564\n","Batch 220/750, Loss: 0.00042898443643935025\n","Batch 230/750, Loss: 0.0007852677954360843\n","Batch 240/750, Loss: 0.00032548533636145294\n","Batch 250/750, Loss: 0.0005297239404171705\n","Batch 260/750, Loss: 0.0003549005195964128\n","Batch 270/750, Loss: 0.005826269742101431\n","Batch 280/750, Loss: 0.0006238593487069011\n","Batch 290/750, Loss: 0.0006755847716704011\n","Batch 300/750, Loss: 0.00019828221411444247\n","Batch 310/750, Loss: 0.0006880465080030262\n","Batch 320/750, Loss: 0.0005169655196368694\n","Batch 330/750, Loss: 0.0006725545972585678\n","Batch 340/750, Loss: 0.0002369833382545039\n","Batch 350/750, Loss: 0.0007635225192643702\n","Batch 360/750, Loss: 0.0007171263569034636\n","Batch 370/750, Loss: 0.0008465800201520324\n","Batch 380/750, Loss: 0.0014344719238579273\n","Batch 390/750, Loss: 0.00035597948590293527\n","Batch 400/750, Loss: 0.004388677887618542\n","Batch 410/750, Loss: 0.0005415149498730898\n","Batch 420/750, Loss: 0.001096941065043211\n","Batch 430/750, Loss: 0.004556803498417139\n","Batch 440/750, Loss: 0.003079071408137679\n","Batch 450/750, Loss: 0.0005276465672068298\n","Batch 460/750, Loss: 0.00037773826625198126\n","Batch 470/750, Loss: 0.0006522664334625006\n","Batch 480/750, Loss: 0.000471092906082049\n","Batch 490/750, Loss: 0.001154100289568305\n","Batch 500/750, Loss: 0.003109134268015623\n","Batch 510/750, Loss: 0.002282829489558935\n","Batch 520/750, Loss: 0.0003052538959309459\n","Batch 530/750, Loss: 0.00219414453022182\n","Batch 540/750, Loss: 0.00023375290038529783\n","Batch 550/750, Loss: 0.0005417856737039983\n","Batch 560/750, Loss: 0.000511247490067035\n","Batch 570/750, Loss: 0.00038813642458990216\n","Batch 580/750, Loss: 0.0008850672165863216\n","Batch 590/750, Loss: 0.00027009882614947855\n","Batch 600/750, Loss: 0.0003616703615989536\n","Batch 610/750, Loss: 0.004175017587840557\n","Batch 620/750, Loss: 0.001370093086734414\n","Batch 630/750, Loss: 0.0006277309730648994\n","Batch 640/750, Loss: 0.00026005416293628514\n","Batch 650/750, Loss: 0.034093212336301804\n","Batch 660/750, Loss: 0.0002712510176934302\n","Batch 670/750, Loss: 0.0007174736238084733\n","Batch 680/750, Loss: 0.004710520152002573\n","Batch 690/750, Loss: 0.00026594361406750977\n","Batch 700/750, Loss: 0.00033012143103405833\n","Batch 710/750, Loss: 0.012431339360773563\n","Batch 720/750, Loss: 0.0003897327114827931\n","Batch 730/750, Loss: 0.48417413234710693\n","Batch 740/750, Loss: 0.001032535103149712\n","Training epoch completed in: 2m 7s\n","Train loss 0.00573855397363271 accuracy 0.9985833333333333\n","Batch 0/188, Test Loss: 1.8913466930389404\n","Batch 10/188, Test Loss: 1.0443140268325806\n","Batch 20/188, Test Loss: 1.3769382238388062\n","Batch 30/188, Test Loss: 1.1340433359146118\n","Batch 40/188, Test Loss: 0.5040035247802734\n","Batch 50/188, Test Loss: 2.2558326721191406\n","Batch 60/188, Test Loss: 1.6251615285873413\n","Batch 70/188, Test Loss: 0.8118877410888672\n","Batch 80/188, Test Loss: 0.6903939247131348\n","Batch 90/188, Test Loss: 0.5088874697685242\n","Batch 100/188, Test Loss: 1.051026701927185\n","Batch 110/188, Test Loss: 0.5492978096008301\n","Batch 120/188, Test Loss: 0.631354808807373\n","Batch 130/188, Test Loss: 0.6627153754234314\n","Batch 140/188, Test Loss: 0.40831589698791504\n","Batch 150/188, Test Loss: 0.753149151802063\n","Batch 160/188, Test Loss: 1.7122197151184082\n","Batch 170/188, Test Loss: 1.3956491947174072\n","Batch 180/188, Test Loss: 0.8647910952568054\n","Test evaluation completed in: 0m 10s\n","Test loss 1.0185383569667155 accuracy 0.8226666666666667\n","Test Precision: 0.8228713914154238\n","Test Recall: 0.8226666666666667\n","Test F1 Score: 0.8227630557727896\n"]}],"source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split\n","import time\n","import traceback\n","\n","class ClickbaitDataset(Dataset):\n","    def __init__(self, titles, labels, tokenizer, max_len):\n","        self.titles = titles\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.titles)\n","\n","    def __getitem__(self, idx):\n","        title = self.titles[idx]\n","        label = self.labels[idx]\n","\n","        # Ensure the label is a valid integer\n","        try:\n","            label = int(label)\n","        except ValueError:\n","            raise ValueError(f\"Label {label} at index {idx} is not a valid integer.\")\n","\n","        encoding = self.tokenizer.encode_plus(\n","            title,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        return {\n","            'title_text': title,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","def create_data_loader(titles, labels, tokenizer, max_len, batch_size):\n","    ds = ClickbaitDataset(\n","        titles=titles,\n","        labels=labels,\n","        tokenizer=tokenizer,\n","        max_len=max_len\n","    )\n","\n","    return DataLoader(ds, batch_size=batch_size, num_workers=4, pin_memory=True)\n","\n","def train_epoch(\n","    model,\n","    data_loader,\n","    loss_fn,\n","    optimizer,\n","    device,\n","    scheduler,\n","    n_examples\n","):\n","    model.train()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    for batch_idx, d in enumerate(data_loader):\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        labels = d[\"labels\"].to(device)\n","\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        _, preds = torch.max(outputs.logits, dim=1)\n","        loss = loss_fn(outputs.logits, labels)\n","\n","        correct_predictions += torch.sum(preds == labels)\n","        losses.append(loss.item())\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        if batch_idx % 10 == 0:\n","            print(f'Batch {batch_idx}/{len(data_loader)}, Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Training epoch completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model.eval()\n","\n","    losses = []\n","    correct_predictions = 0\n","    start_time = time.time()\n","\n","    all_labels = []\n","    all_preds = []\n","\n","    with torch.no_grad():\n","        for batch_idx, d in enumerate(data_loader):\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            labels = d[\"labels\"].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            loss = loss_fn(outputs.logits, labels)\n","\n","            correct_predictions += torch.sum(preds == labels)\n","            losses.append(loss.item())\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds.cpu().numpy())\n","\n","            if batch_idx % 10 == 0:\n","                print(f'Batch {batch_idx}/{len(data_loader)}, Test Loss: {loss.item()}')\n","\n","    total_time = time.time() - start_time\n","    print(f'Test evaluation completed in: {total_time // 60:.0f}m {total_time % 60:.0f}s')\n","\n","    return correct_predictions.double() / n_examples, np.mean(losses), all_labels, all_preds\n","\n","def clean_dataset(df):\n","    # Remove rows where labels are NaN\n","    df = df.dropna(subset=['label'])\n","    # Convert labels to integers, and filter out rows where this conversion fails\n","    df['label'] = pd.to_numeric(df['label'], errors='coerce')\n","    df = df.dropna(subset=['label'])\n","    df['label'] = df['label'].astype(int)\n","    return df\n","\n","def main():\n","    print(\"Starting main process\")\n","\n","    try:\n","        print('Loading the dataset.')\n","        df = pd.read_csv('/content/drive/MyDrive/clickbait/indonesian.csv')\n","        print('Dataset loaded successfully.')\n","\n","        print('Cleaning the dataset.')\n","        df = clean_dataset(df)\n","        print('Dataset cleaned.')\n","\n","        print('Splitting the dataset into training and test sets.')\n","        df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n","        print('Dataset split into training and test sets.')\n","\n","        RANDOM_SEED = 42\n","        MAX_LEN = 128\n","        BATCH_SIZE = 16\n","        EPOCHS = 10\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f'Using device: {device}')\n","\n","        print('Loading the tokenizer and model.')\n","        tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n","        model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', num_labels=2)\n","        model = model.to(device)\n","        print('Model and tokenizer loaded and moved to device.')\n","\n","        print('Creating data loaders.')\n","        train_titles = df_train['title'].tolist()\n","        train_labels = df_train['label'].tolist()\n","        test_titles = df_test['title'].tolist()\n","        test_labels = df_test['label'].tolist()\n","\n","        train_data_loader = create_data_loader(train_titles, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        test_data_loader = create_data_loader(test_titles, test_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n","        print('Data loaders created.')\n","\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","        total_steps = len(train_data_loader) * EPOCHS\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","        loss_fn = torch.nn.CrossEntropyLoss().to(device)\n","        print('Optimizer, scheduler, and loss function defined.')\n","\n","        for epoch in range(EPOCHS):\n","            print(f'Starting epoch {epoch + 1}/{EPOCHS}')\n","\n","            train_acc, train_loss = train_epoch(\n","                model,\n","                train_data_loader,\n","                loss_fn,\n","                optimizer,\n","                device,\n","                scheduler,\n","                len(df_train)\n","            )\n","            print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","            test_acc, test_loss, all_labels, all_preds = eval_model(\n","                model,\n","                test_data_loader,\n","                loss_fn,\n","                device,\n","                len(df_test)\n","            )\n","            print(f'Test loss {test_loss} accuracy {test_acc}')\n","\n","            # Calculate additional metrics\n","            precision = precision_score(all_labels, all_preds, average='weighted')\n","            recall = recall_score(all_labels, all_preds, average='weighted')\n","            f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","            print(f'Test Precision: {precision}')\n","            print(f'Test Recall: {recall}')\n","            print(f'Test F1 Score: {f1}')\n","\n","    except Exception as e:\n","        print(f'An error occurred: {e}')\n","        traceback.print_exc()\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37215,"status":"ok","timestamp":1722709552800,"user":{"displayName":"Miri Zlotnik","userId":"09592984354898968731"},"user_tz":-180},"id":"Ic7ZMvG1IbEx","outputId":"92581bf0-c038-45f1-e2e8-eff25d84ba5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1ZDoY_cCvGd99UeEdfZDeQTAR73hRmiF8","authorship_tag":"ABX9TyNUZEUaQ0CuyCJOqaE+WVw/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"97d957532d0f4c70a4e56dd8295215f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_108742a5c1804587a42c67f1f5cd5b25","IPY_MODEL_f88d0d1339354e02a68d3587f81e6d3a","IPY_MODEL_84e06baaa33044b4bf547024b1cf4455"],"layout":"IPY_MODEL_2aee488fddc34e10ae84acb72b555434"}},"108742a5c1804587a42c67f1f5cd5b25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_279d4352b4724d3084b474b757f6fcf7","placeholder":"​","style":"IPY_MODEL_8b6a614782d446a4acdd065bf86a1529","value":"tokenizer_config.json: 100%"}},"f88d0d1339354e02a68d3587f81e6d3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_37a080f2bb434efe97b76c508acc4cb0","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3b6b258f043b495eb409c4e980c17523","value":2}},"84e06baaa33044b4bf547024b1cf4455":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea64ab0c934b4416b09a4723589a0544","placeholder":"​","style":"IPY_MODEL_87664342f5dd436b90a08a686d4fd46c","value":" 2.00/2.00 [00:00&lt;00:00, 162B/s]"}},"2aee488fddc34e10ae84acb72b555434":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"279d4352b4724d3084b474b757f6fcf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b6a614782d446a4acdd065bf86a1529":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37a080f2bb434efe97b76c508acc4cb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b6b258f043b495eb409c4e980c17523":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea64ab0c934b4416b09a4723589a0544":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87664342f5dd436b90a08a686d4fd46c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd0fdf0a3ee74e53bc310fa48ff92782":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81af3bc6d0704076a1d5d2cc2c44940a","IPY_MODEL_8903894ae680484aa75aeb641719e075","IPY_MODEL_9db5d8adf2eb489481b280c89470af6f"],"layout":"IPY_MODEL_c80cba1873514377b2c38ca4b4b224ca"}},"81af3bc6d0704076a1d5d2cc2c44940a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e2a7b42d19a4311b171dc0f5822fbb2","placeholder":"​","style":"IPY_MODEL_2781bffc44874388a85163ee12bbcf20","value":"vocab.txt: 100%"}},"8903894ae680484aa75aeb641719e075":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4054b309d82642f6b56fd914b5718b18","max":229167,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48a79c174d28424d94aa306ccd17db92","value":229167}},"9db5d8adf2eb489481b280c89470af6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0091de15ec7f45c3b315a1943473f7eb","placeholder":"​","style":"IPY_MODEL_a465983ec2ab43669a8c516c71eecfbd","value":" 229k/229k [00:00&lt;00:00, 5.93MB/s]"}},"c80cba1873514377b2c38ca4b4b224ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e2a7b42d19a4311b171dc0f5822fbb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2781bffc44874388a85163ee12bbcf20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4054b309d82642f6b56fd914b5718b18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48a79c174d28424d94aa306ccd17db92":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0091de15ec7f45c3b315a1943473f7eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a465983ec2ab43669a8c516c71eecfbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe78caf8ab5247a7a77dc89003a68ae1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bbc785e3a57b404e959a96cb02fe2e66","IPY_MODEL_a44f0105445849fd85981ab03801af04","IPY_MODEL_1245da1554ec4a4db888daa70cc245af"],"layout":"IPY_MODEL_dc921392f44544d5b7eb45b679aaba73"}},"bbc785e3a57b404e959a96cb02fe2e66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e3dbc0ac0c54d31918ae129092e679a","placeholder":"​","style":"IPY_MODEL_0990f662ab3a4ae8acb8a6878108b9eb","value":"special_tokens_map.json: 100%"}},"a44f0105445849fd85981ab03801af04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a6c2370e37b469d95deda19d74ce6ed","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_441e1ff055914185a41061b5bfddf7e6","value":112}},"1245da1554ec4a4db888daa70cc245af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d7c430539d44b389d38b0349e5fdaaf","placeholder":"​","style":"IPY_MODEL_74fd7749b4894eaa81f10fb31066ff48","value":" 112/112 [00:00&lt;00:00, 9.04kB/s]"}},"dc921392f44544d5b7eb45b679aaba73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e3dbc0ac0c54d31918ae129092e679a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0990f662ab3a4ae8acb8a6878108b9eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a6c2370e37b469d95deda19d74ce6ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"441e1ff055914185a41061b5bfddf7e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d7c430539d44b389d38b0349e5fdaaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74fd7749b4894eaa81f10fb31066ff48":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c3436b219354819a60ba343f9aace72":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b556f8d201d4b47ae1553dda1e3c9f7","IPY_MODEL_397a2a61425c4cd4a3746511ebcab358","IPY_MODEL_6affde2b2b1a406fb304fc3e0e358df3"],"layout":"IPY_MODEL_27181b41ba344b3cb2b9277d27e7f088"}},"7b556f8d201d4b47ae1553dda1e3c9f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e34db46b3a3648ba86181061a1ae416a","placeholder":"​","style":"IPY_MODEL_f29029ff3c8449888ceff9579e9253fe","value":"config.json: 100%"}},"397a2a61425c4cd4a3746511ebcab358":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6a9b0d81ac445dda1e5f473695f1850","max":1534,"min":0,"orientation":"horizontal","style":"IPY_MODEL_555f2ad394b14dd7bd0c4263af61529c","value":1534}},"6affde2b2b1a406fb304fc3e0e358df3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0012d805dbe34577b69606dad6c2e0ac","placeholder":"​","style":"IPY_MODEL_b629cb8232184808b45011f69f2f7d08","value":" 1.53k/1.53k [00:00&lt;00:00, 126kB/s]"}},"27181b41ba344b3cb2b9277d27e7f088":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e34db46b3a3648ba86181061a1ae416a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f29029ff3c8449888ceff9579e9253fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6a9b0d81ac445dda1e5f473695f1850":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"555f2ad394b14dd7bd0c4263af61529c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0012d805dbe34577b69606dad6c2e0ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b629cb8232184808b45011f69f2f7d08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"669a9ae19cdb476eb692b1eca8ce3f7b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1647848fa404cddbb034c1bc5b08e7d","IPY_MODEL_da92f09bfb2946d8aa6ece7d942886ca","IPY_MODEL_2aa0a48fc0f74494b0b991fad67280ac"],"layout":"IPY_MODEL_c3b1be20f4154872be1024ccdced54c4"}},"c1647848fa404cddbb034c1bc5b08e7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_758eec21d81c42b3897dda9dbc7af4f5","placeholder":"​","style":"IPY_MODEL_6b1b20fc316148e8baf9b72032a03ac0","value":"pytorch_model.bin: 100%"}},"da92f09bfb2946d8aa6ece7d942886ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46ceb213a6804359b38a8c4ad05899fd","max":497810400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_768c5e0b9849438c83fed7f52e1709d6","value":497810400}},"2aa0a48fc0f74494b0b991fad67280ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c3011f356d0481bbc891ea923dbc60d","placeholder":"​","style":"IPY_MODEL_f554fd0a643743d3bb5df1857d6a2dd2","value":" 498M/498M [00:07&lt;00:00, 69.0MB/s]"}},"c3b1be20f4154872be1024ccdced54c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"758eec21d81c42b3897dda9dbc7af4f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b1b20fc316148e8baf9b72032a03ac0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46ceb213a6804359b38a8c4ad05899fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"768c5e0b9849438c83fed7f52e1709d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c3011f356d0481bbc891ea923dbc60d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f554fd0a643743d3bb5df1857d6a2dd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
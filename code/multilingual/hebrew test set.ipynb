{
  "cells": [
    {
      "cell_type": "code",
      "id": "xvlPpyif8fdoK5TbF7HWZXos",
      "metadata": {
        "tags": [],
        "id": "xvlPpyif8fdoK5TbF7HWZXos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726704854866,
          "user_tz": -180,
          "elapsed": 21591528,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "513ddcee-6ec8-45a7-c709-f083095d108c"
      },
      "source": [
        "from itertools import combinations\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load and clean dataset functions\n",
        "def load_data(filepath):\n",
        "    if filepath.endswith('.csv'):\n",
        "        return pd.read_csv(filepath)\n",
        "    elif filepath.endswith('.xlsx'):\n",
        "        return pd.read_excel(filepath)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {filepath}\")\n",
        "\n",
        "def clean_dataset(df):\n",
        "    if 'title' not in df.columns or 'label' not in df.columns:\n",
        "        raise ValueError(\"The dataframe must have 'title' and 'label' columns.\")\n",
        "\n",
        "    df = df.dropna(subset=['title', 'label'])  # Ensure no NaNs in title or label\n",
        "    df = df[df['title'].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]  # Ensure non-empty strings\n",
        "    df = df[df['label'].apply(lambda x: isinstance(x, (int, np.integer)) or str(x).isdigit())]  # Ensure valid labels\n",
        "\n",
        "    df['title'] = df['title'].astype(str)\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    return df\n",
        "\n",
        "# Dataset wrapper to handle input data\n",
        "class DatasetWrapper(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def create_data_loader(texts, labels, tokenizer, max_len, batch_size):\n",
        "    print(f\"Texts to tokenize: {texts[:5]}\")  # Print the first 5 texts for debugging\n",
        "    print(f\"Labels: {labels[:5]}\")            # Print the corresponding labels\n",
        "\n",
        "    # Filter out any potential empty strings in the texts\n",
        "    valid_data = [(text, label) for text, label in zip(texts, labels) if text.strip()]\n",
        "\n",
        "    # Separate valid texts and labels\n",
        "    valid_texts, valid_labels = zip(*valid_data) if valid_data else ([], [])\n",
        "\n",
        "    if len(valid_texts) == 0:\n",
        "        raise ValueError(\"No valid texts found after filtering.\")\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        valid_texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    dataset = DatasetWrapper(encoding, valid_labels)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, dataset_len):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for data in data_loader:\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_mask'].to(device)\n",
        "        labels = data['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / dataset_len, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, dataset_len):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            input_ids = data['input_ids'].to(device)\n",
        "            attention_mask = data['attention_mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    return correct_predictions.double() / dataset_len, np.mean(losses), all_labels, all_preds\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    RANDOM_SEED = 42\n",
        "    MAX_LEN = 128\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 3\n",
        "    TEST_SPLIT = 0.2\n",
        "    PATIENCE = 3  # Early stopping patience\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    languages = ['hebrew', 'arabic', 'bangla', 'chinese', 'english', 'german', 'indonesian', 'romanian', 'turkish']\n",
        "    file_paths = {\n",
        "        'hebrew': '/hebrew.xlsx',\n",
        "        'arabic': '/arabic.xlsx',\n",
        "        'bangla': '/bangla.csv',\n",
        "        'chinese': '/chinese.csv',\n",
        "        'english': '/english.csv',\n",
        "        'german': '/german.csv',\n",
        "        'indonesian': '/indonesian.csv',\n",
        "        'romanian': '/romanianL.xlsx',\n",
        "        'turkish': '/turkish.csv'\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "    # Iterate over combinations of 2 languages\n",
        "    for lang_comb in combinations(languages, 2):\n",
        "        print(f\"Processing combination: {lang_comb}\")\n",
        "\n",
        "        # Load and clean datasets for the two languages\n",
        "        df_lang1 = load_data(file_paths[lang_comb[0]])\n",
        "        df_lang1 = clean_dataset(df_lang1)\n",
        "\n",
        "        df_lang2 = load_data(file_paths[lang_comb[1]])\n",
        "        df_lang2 = clean_dataset(df_lang2)\n",
        "\n",
        "        # Check if datasets are empty after cleaning\n",
        "        if df_lang1.empty:\n",
        "            print(f\"Warning: Dataset for {lang_comb[0]} is empty after cleaning. Skipping this combination.\")\n",
        "            continue\n",
        "        if df_lang2.empty:\n",
        "            print(f\"Warning: Dataset for {lang_comb[1]} is empty after cleaning. Skipping this combination.\")\n",
        "            continue\n",
        "\n",
        "        # Split each dataset into 80% train and 20% test\n",
        "        if len(df_lang1) > 1 and len(df_lang2) > 1:  # Ensure there are enough samples to split\n",
        "            df_train_lang1, df_test_lang1 = train_test_split(df_lang1, test_size=TEST_SPLIT, random_state=RANDOM_SEED)\n",
        "            df_train_lang2, df_test_lang2 = train_test_split(df_lang2, test_size=TEST_SPLIT, random_state=RANDOM_SEED)\n",
        "        else:\n",
        "            print(f\"Warning: Dataset too small for {lang_comb}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Combine train datasets of the two languages\n",
        "        #df_train_combined = pd.concat([df_train_lang1, df_train_lang2], ignore_index=True)\n",
        "\n",
        "        # Combine train datasets of the two languages\n",
        "        df_train_combined = pd.concat([df_train_lang1, df_train_lang2], ignore_index=True)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_data_loader = create_data_loader(\n",
        "            df_train_combined['title'].tolist(),\n",
        "            df_train_combined['label'].tolist(),\n",
        "            tokenizer,\n",
        "            MAX_LEN,\n",
        "            BATCH_SIZE\n",
        "        )\n",
        "        test_data_loader_lang1 = create_data_loader(\n",
        "            df_test_lang1['title'].tolist(),\n",
        "            df_test_lang1['label'].tolist(),\n",
        "            tokenizer,\n",
        "            MAX_LEN,\n",
        "            BATCH_SIZE\n",
        "        )\n",
        "        test_data_loader_lang2 = create_data_loader(\n",
        "            df_test_lang2['title'].tolist(),\n",
        "            df_test_lang2['label'].tolist(),\n",
        "            tokenizer,\n",
        "            MAX_LEN,\n",
        "            BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        # Initialize model\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
        "        model = model.to(device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "        total_steps = len(train_data_loader) * EPOCHS\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(EPOCHS):\n",
        "            print(f'Starting epoch {epoch + 1}/{EPOCHS}')\n",
        "            train_acc, train_loss = train_epoch(\n",
        "                model,\n",
        "                train_data_loader,\n",
        "                loss_fn,\n",
        "                optimizer,\n",
        "                device,\n",
        "                scheduler,\n",
        "                len(df_train_combined)\n",
        "            )\n",
        "            print(f'Train loss {train_loss}, accuracy {train_acc}')\n",
        "\n",
        "        # Evaluate on the two test sets separately\n",
        "        print(f'Evaluating on {lang_comb[0]} test set...')\n",
        "        test_acc_lang1, test_loss_lang1, labels_lang1, preds_lang1 = eval_model(\n",
        "            model,\n",
        "            test_data_loader_lang1,\n",
        "            loss_fn,\n",
        "            device,\n",
        "            len(df_test_lang1)\n",
        "        )\n",
        "        precision_lang1 = precision_score(labels_lang1, preds_lang1, average='weighted')\n",
        "        recall_lang1 = recall_score(labels_lang1, preds_lang1, average='weighted')\n",
        "        f1_lang1 = f1_score(labels_lang1, preds_lang1, average='weighted')\n",
        "\n",
        "        print(f'Evaluating on {lang_comb[1]} test set...')\n",
        "        test_acc_lang2, test_loss_lang2, labels_lang2, preds_lang2 = eval_model(\n",
        "            model,\n",
        "            test_data_loader_lang2,\n",
        "            loss_fn,\n",
        "            device,\n",
        "            len(df_test_lang2)\n",
        "        )\n",
        "        precision_lang2 = precision_score(labels_lang2, preds_lang2, average='weighted')\n",
        "        recall_lang2 = recall_score(labels_lang2, preds_lang2, average='weighted')\n",
        "        f1_lang2 = f1_score(labels_lang2, preds_lang2, average='weighted')\n",
        "\n",
        "        # Save results for the current combination\n",
        "        results.append({\n",
        "            'languages': lang_comb,\n",
        "            'test_results_lang1': {\n",
        "                'accuracy': test_acc_lang1.item(),\n",
        "                'precision': precision_lang1,\n",
        "                'recall': recall_lang1,\n",
        "                'f1': f1_lang1\n",
        "            },\n",
        "            'test_results_lang2': {\n",
        "                'accuracy': test_acc_lang2.item(),\n",
        "                'precision': precision_lang2,\n",
        "                'recall': recall_lang2,\n",
        "                'f1': f1_lang2\n",
        "            }\n",
        "        })\n",
        "\n",
        "    print(f\"Final results: {results}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('hebrew', 'arabic')\n",
            "Texts to tokenize: [' זקוקים להחלפת ברך? ניתוחי החלפת ברך בשיטה הטובה ביותר', '  \"אף אחד לא רוצה להיראות כאילו עבר השתלת שיער\" - השיטה שאתם צריכים להכיר', 'סקר ראשון אחר הבחירות: גנץ מתחזק, רה\"מ נחלש', \" שנתיים לאחר מותה הטרגי בגיל 23 - נחשף שיר חדש של תמר 'טאי' עמר\", 'גאוני. צפו: כך נבנה השער הדרמטי של הנוער']\n",
            "Labels: [1, 1, 0, 0, 1]\n",
            "Texts to tokenize: [' אל תשקיעו במערכת חימום גדולה עד שתראו את ההמצאה המהפכנית הזו...', ' ארגז כלים לטיפול בכאבי גב תחתון באופן עצמאי', '  עינב בובליל: \"זה המוצר שגמל אותי ממתוקים\"', 'אלעל פתחה עמדות צ\\'ק אין עצמי בנתב\"ג', 'הטעות של בני 55+']\n",
            "Labels: [1, 1, 1, 0, 1]\n",
            "Texts to tokenize: ['بالفيديو – شكل ابنتي #أشرف_زكي و #روجينا حديث #الجمهور... كشفتا والدهما؟', 'الموت يفجع حجاج عبد العظيم مرتين في 4 أيام', 'تاكوبيل تعيد البطاطا، الأمور إثارة!', 'الصحة: مد فترة الحملة القومية الثانية للتطعيم ضد مرض شلل الأطفال حتى غد', 'إحباط محاولة تهريب 20 مليون قرص أمفيتامين مخدر مخبأة داخل شحنة فاكهة العنب.\\n']\n",
            "Labels: [1, 0, 1, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.42116064030537437, accuracy 0.803964591056972\n",
            "Starting epoch 2/3\n",
            "Train loss 0.30755317325792764, accuracy 0.872437012937883\n",
            "Starting epoch 3/3\n",
            "Train loss 0.259000814068942, accuracy 0.8893092229704168\n",
            "Evaluating on hebrew test set...\n",
            "Evaluating on arabic test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('hebrew', 'bangla')\n",
            "Texts to tokenize: [' זקוקים להחלפת ברך? ניתוחי החלפת ברך בשיטה הטובה ביותר', '  \"אף אחד לא רוצה להיראות כאילו עבר השתלת שיער\" - השיטה שאתם צריכים להכיר', 'סקר ראשון אחר הבחירות: גנץ מתחזק, רה\"מ נחלש', \" שנתיים לאחר מותה הטרגי בגיל 23 - נחשף שיר חדש של תמר 'טאי' עמר\", 'גאוני. צפו: כך נבנה השער הדרמטי של הנוער']\n",
            "Labels: [1, 1, 0, 0, 1]\n",
            "Texts to tokenize: [' אל תשקיעו במערכת חימום גדולה עד שתראו את ההמצאה המהפכנית הזו...', ' ארגז כלים לטיפול בכאבי גב תחתון באופן עצמאי', '  עינב בובליל: \"זה המוצר שגמל אותי ממתוקים\"', 'אלעל פתחה עמדות צ\\'ק אין עצמי בנתב\"ג', 'הטעות של בני 55+']\n",
            "Labels: [1, 1, 1, 0, 1]\n",
            "Texts to tokenize: ['৬ দিন পর সৌদি আরব যাওয়ার কথা, তার আগে যুবকের রহস্যজনক মৃ’ত্যু', 'এই সেই সন্তান যিনি কিনা তার মাকে গর্ভ;বতী করে ফেলেন, উপায় না পেয়ে মা;য়ের সাথে বিয়ে!', 'কারিনার থেকে তিনগুণ বেশি পারিশ্রমিক কঙ্গনার!', 'ই-কমার্স ব্যবসায় সাকিব - Techzoom.TV', '২৫ লাখ টাকার মোটরসাইকেলে বিশ্বভ্রমণে অজিত']\n",
            "Labels: [0, 1, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.5938983955214509, accuracy 0.6796008869179601\n",
            "Starting epoch 2/3\n",
            "Train loss 0.417978665470022, accuracy 0.811529933481153\n",
            "Starting epoch 3/3\n",
            "Train loss 0.3045252685196104, accuracy 0.8725055432372506\n",
            "Evaluating on hebrew test set...\n",
            "Evaluating on bangla test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('hebrew', 'chinese')\n",
            "Texts to tokenize: [' זקוקים להחלפת ברך? ניתוחי החלפת ברך בשיטה הטובה ביותר', '  \"אף אחד לא רוצה להיראות כאילו עבר השתלת שיער\" - השיטה שאתם צריכים להכיר', 'סקר ראשון אחר הבחירות: גנץ מתחזק, רה\"מ נחלש', \" שנתיים לאחר מותה הטרגי בגיל 23 - נחשף שיר חדש של תמר 'טאי' עמר\", 'גאוני. צפו: כך נבנה השער הדרמטי של הנוער']\n",
            "Labels: [1, 1, 0, 0, 1]\n",
            "Texts to tokenize: [' אל תשקיעו במערכת חימום גדולה עד שתראו את ההמצאה המהפכנית הזו...', ' ארגז כלים לטיפול בכאבי גב תחתון באופן עצמאי', '  עינב בובליל: \"זה המוצר שגמל אותי ממתוקים\"', 'אלעל פתחה עמדות צ\\'ק אין עצמי בנתב\"ג', 'הטעות של בני 55+']\n",
            "Labels: [1, 1, 1, 0, 1]\n",
            "Texts to tokenize: ['他终于承认了长达10年的地下情', '“深圳没有早睡的人”', '“加点旋转更舒服？”男朋友居然和舍友做这种不可描述的事情！', '你为什么不联系微信好友了？', '这4个坏习惯最伤肾 你有做过吗？']\n",
            "Labels: [1, 0, 1, 1, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.19886607851761626, accuracy 0.9205026828579498\n",
            "Starting epoch 2/3\n",
            "Train loss 0.08972263023331888, accuracy 0.9679469076532053\n",
            "Starting epoch 3/3\n",
            "Train loss 0.035207892487426846, accuracy 0.9885625529511438\n",
            "Evaluating on hebrew test set...\n",
            "Evaluating on chinese test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('hebrew', 'english')\n",
            "Texts to tokenize: [' זקוקים להחלפת ברך? ניתוחי החלפת ברך בשיטה הטובה ביותר', '  \"אף אחד לא רוצה להיראות כאילו עבר השתלת שיער\" - השיטה שאתם צריכים להכיר', 'סקר ראשון אחר הבחירות: גנץ מתחזק, רה\"מ נחלש', \" שנתיים לאחר מותה הטרגי בגיל 23 - נחשף שיר חדש של תמר 'טאי' עמר\", 'גאוני. צפו: כך נבנה השער הדרמטי של הנוער']\n",
            "Labels: [1, 1, 0, 0, 1]\n",
            "Texts to tokenize: [' אל תשקיעו במערכת חימום גדולה עד שתראו את ההמצאה המהפכנית הזו...', ' ארגז כלים לטיפול בכאבי גב תחתון באופן עצמאי', '  עינב בובליל: \"זה המוצר שגמל אותי ממתוקים\"', 'אלעל פתחה עמדות צ\\'ק אין עצמי בנתב\"ג', 'הטעות של בני 55+']\n",
            "Labels: [1, 1, 1, 0, 1]\n",
            "Texts to tokenize: ['Filipino activist arrested for disrupting Manila Cathedral mass in Reproductive Health Bill protest', 'International Board fixes soccer field size, halts technology experiments', '24 Rules For Women On A First Date With A Man', 'Political fallout from the sacking of Professor David Nutt gathers momentum', 'Which \"Clueless\" Character Are You Based On Your Zodiac Sign']\n",
            "Labels: [0, 0, 1, 0, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.04999170400094451, accuracy 0.981127080181543\n",
            "Starting epoch 2/3\n",
            "Train loss 0.016949751701004056, accuracy 0.9940242057488653\n",
            "Starting epoch 3/3\n",
            "Train loss 0.004865134308490445, accuracy 0.9984114977307109\n",
            "Evaluating on hebrew test set...\n",
            "Evaluating on english test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('hebrew', 'german')\n",
            "Texts to tokenize: [' זקוקים להחלפת ברך? ניתוחי החלפת ברך בשיטה הטובה ביותר', '  \"אף אחד לא רוצה להיראות כאילו עבר השתלת שיער\" - השיטה שאתם צריכים להכיר', 'סקר ראשון אחר הבחירות: גנץ מתחזק, רה\"מ נחלש', \" שנתיים לאחר מותה הטרגי בגיל 23 - נחשף שיר חדש של תמר 'טאי' עמר\", 'גאוני. צפו: כך נבנה השער הדרמטי של הנוער']\n",
            "Labels: [1, 1, 0, 0, 1]\n",
            "Texts to tokenize: [' אל תשקיעו במערכת חימום גדולה עד שתראו את ההמצאה המהפכנית הזו...', ' ארגז כלים לטיפול בכאבי גב תחתון באופן עצמאי', '  עינב בובליל: \"זה המוצר שגמל אותי ממתוקים\"', 'אלעל פתחה עמדות צ\\'ק אין עצמי בנתב\"ג', 'הטעות של בני 55+']\n",
            "Labels: [1, 1, 1, 0, 1]\n",
            "Texts to tokenize: ['Robert Geiss auf Krücken: \"Ich brauche einen Arzt!\" ', '„Promi Big Brother“ 2017: Das ist der Gewinner', 'Laura Müller: Werbepartner erntet Shitstorm', 'Netflix-Serie zu Claas Relotius-Skandal geplant ', 'Shawn Mendes hat Schmetterlinge im Bauch']\n",
            "Labels: [0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.08724306658017048, accuracy 0.9657534246575342\n",
            "Starting epoch 2/3\n",
            "Train loss 0.03829377601019505, accuracy 0.9859464830719\n",
            "Starting epoch 3/3\n",
            "Train loss 0.01535160619432978, accuracy 0.9943395556817375\n",
            "Evaluating on hebrew test set...\n",
            "Evaluating on german test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('hebrew', 'indonesian')\n",
            "Texts to tokenize: [' זקוקים להחלפת ברך? ניתוחי החלפת ברך בשיטה הטובה ביותר', '  \"אף אחד לא רוצה להיראות כאילו עבר השתלת שיער\" - השיטה שאתם צריכים להכיר', 'סקר ראשון אחר הבחירות: גנץ מתחזק, רה\"מ נחלש', \" שנתיים לאחר מותה הטרגי בגיל 23 - נחשף שיר חדש של תמר 'טאי' עמר\", 'גאוני. צפו: כך נבנה השער הדרמטי של הנוער']\n",
            "Labels: [1, 1, 0, 0, 1]\n",
            "Texts to tokenize: [' אל תשקיעו במערכת חימום גדולה עד שתראו את ההמצאה המהפכנית הזו...', ' ארגז כלים לטיפול בכאבי גב תחתון באופן עצמאי', '  עינב בובליל: \"זה המוצר שגמל אותי ממתוקים\"', 'אלעל פתחה עמדות צ\\'ק אין עצמי בנתב\"ג', 'הטעות של בני 55+']\n",
            "Labels: [1, 1, 1, 0, 1]\n",
            "Texts to tokenize: ['Arkeolog Temukan Situs David Vs Goliath seperti Disebut Alkitab', 'Tampil Classy dengan Rok Tutu, Ini 5 Inspirasi Hijab ala Aghnia Punjabi', 'Hasil China Open 2019 - Marcus/Kevin Sukses Bayar Lunas Kekalahan di Kejuaraan Dunia 2019', 'Sepasang Pengungsi Rohingya Tewas dalam Baku Tembak di Bangladesh', 'Jadwal Wakil Indonesia di Semifinal Vietnam Open 2019']\n",
            "Labels: [0, 1, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.4887080680982559, accuracy 0.7700155763239875\n",
            "Starting epoch 2/3\n",
            "Train loss 0.37563844088228376, accuracy 0.8373831775700935\n",
            "Starting epoch 3/3\n",
            "Train loss 0.2820742687913282, accuracy 0.8876947040498443\n",
            "Evaluating on hebrew test set...\n",
            "Evaluating on indonesian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('hebrew', 'romanian')\n",
            "Texts to tokenize: [' זקוקים להחלפת ברך? ניתוחי החלפת ברך בשיטה הטובה ביותר', '  \"אף אחד לא רוצה להיראות כאילו עבר השתלת שיער\" - השיטה שאתם צריכים להכיר', 'סקר ראשון אחר הבחירות: גנץ מתחזק, רה\"מ נחלש', \" שנתיים לאחר מותה הטרגי בגיל 23 - נחשף שיר חדש של תמר 'טאי' עמר\", 'גאוני. צפו: כך נבנה השער הדרמטי של הנוער']\n",
            "Labels: [1, 1, 0, 0, 1]\n",
            "Texts to tokenize: [' אל תשקיעו במערכת חימום גדולה עד שתראו את ההמצאה המהפכנית הזו...', ' ארגז כלים לטיפול בכאבי גב תחתון באופן עצמאי', '  עינב בובליל: \"זה המוצר שגמל אותי ממתוקים\"', 'אלעל פתחה עמדות צ\\'ק אין עצמי בנתב\"ג', 'הטעות של בני 55+']\n",
            "Labels: [1, 1, 1, 0, 1]\n",
            "Texts to tokenize: ['Top 10 al celor mai doriți angajatori din România este dominat de companii de tehnologie', 'Termenii și condițiile de utilizare, pe limba ta: ce vor oficialii', 'Apple, Huawei sau Oppo, zdrobite de Xiaomi: cum a avut atât de câștigat', 'Spotify a lansat Greenroom, o rețea de socializare audio similară cu Clubhouse', 'Telefoanele ieftine cu Android vor fi mult mai bune, mai rapide și mai eficiente: ce s-a schimbat']\n",
            "Labels: [1, 1, 1, 0, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.3959115047082805, accuracy 0.8198888073009546\n",
            "Starting epoch 2/3\n",
            "Train loss 0.2629180858974289, accuracy 0.8939473408161125\n",
            "Starting epoch 3/3\n",
            "Train loss 0.18315472752293385, accuracy 0.9318157977551663\n",
            "Evaluating on hebrew test set...\n",
            "Evaluating on romanian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('hebrew', 'turkish')\n",
            "Warning: Dataset for turkish is empty after cleaning. Skipping this combination.\n",
            "Processing combination: ('arabic', 'bangla')\n",
            "Texts to tokenize: ['انقسام إيراني حول التعامل بايدن', 'تفاصيل واقعة حى الشيخ جراح بالقدس محاولات إسرائيل لإخلاء منازل الفلسطينيين', 'الأهلى يحذر لاعبيه قبل الإجازة الاختلاط ممنوع ورفع الكمامة مرفوض', 'السويد تعلن وفاة امرأة بعد تطعيمها بلقاح «أسترازينيكا»', '#شرطة_الرياض: القبض 6 وافدين اقتحموا مقار شركات وسرقوا أموالاً ومعدات.\\n\\n']\n",
            "Labels: [1, 1, 0, 1, 1]\n",
            "Texts to tokenize: ['بالفيديو – شكل ابنتي #أشرف_زكي و #روجينا حديث #الجمهور... كشفتا والدهما؟', 'الموت يفجع حجاج عبد العظيم مرتين في 4 أيام', 'تاكوبيل تعيد البطاطا، الأمور إثارة!', 'الصحة: مد فترة الحملة القومية الثانية للتطعيم ضد مرض شلل الأطفال حتى غد', 'إحباط محاولة تهريب 20 مليون قرص أمفيتامين مخدر مخبأة داخل شحنة فاكهة العنب.\\n']\n",
            "Labels: [1, 0, 1, 0, 0]\n",
            "Texts to tokenize: ['৬ দিন পর সৌদি আরব যাওয়ার কথা, তার আগে যুবকের রহস্যজনক মৃ’ত্যু', 'এই সেই সন্তান যিনি কিনা তার মাকে গর্ভ;বতী করে ফেলেন, উপায় না পেয়ে মা;য়ের সাথে বিয়ে!', 'কারিনার থেকে তিনগুণ বেশি পারিশ্রমিক কঙ্গনার!', 'ই-কমার্স ব্যবসায় সাকিব - Techzoom.TV', '২৫ লাখ টাকার মোটরসাইকেলে বিশ্বভ্রমণে অজিত']\n",
            "Labels: [0, 1, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.47591379560905395, accuracy 0.746345851135597\n",
            "Starting epoch 2/3\n",
            "Train loss 0.32677481710303313, accuracy 0.8578067611123603\n",
            "Starting epoch 3/3\n",
            "Train loss 0.279871130263884, accuracy 0.8787946930514954\n",
            "Evaluating on arabic test set...\n",
            "Evaluating on bangla test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('arabic', 'chinese')\n",
            "Texts to tokenize: ['انقسام إيراني حول التعامل بايدن', 'تفاصيل واقعة حى الشيخ جراح بالقدس محاولات إسرائيل لإخلاء منازل الفلسطينيين', 'الأهلى يحذر لاعبيه قبل الإجازة الاختلاط ممنوع ورفع الكمامة مرفوض', 'السويد تعلن وفاة امرأة بعد تطعيمها بلقاح «أسترازينيكا»', '#شرطة_الرياض: القبض 6 وافدين اقتحموا مقار شركات وسرقوا أموالاً ومعدات.\\n\\n']\n",
            "Labels: [1, 1, 0, 1, 1]\n",
            "Texts to tokenize: ['بالفيديو – شكل ابنتي #أشرف_زكي و #روجينا حديث #الجمهور... كشفتا والدهما؟', 'الموت يفجع حجاج عبد العظيم مرتين في 4 أيام', 'تاكوبيل تعيد البطاطا، الأمور إثارة!', 'الصحة: مد فترة الحملة القومية الثانية للتطعيم ضد مرض شلل الأطفال حتى غد', 'إحباط محاولة تهريب 20 مليون قرص أمفيتامين مخدر مخبأة داخل شحنة فاكهة العنب.\\n']\n",
            "Labels: [1, 0, 1, 0, 0]\n",
            "Texts to tokenize: ['他终于承认了长达10年的地下情', '“深圳没有早睡的人”', '“加点旋转更舒服？”男朋友居然和舍友做这种不可描述的事情！', '你为什么不联系微信好友了？', '这4个坏习惯最伤肾 你有做过吗？']\n",
            "Labels: [1, 0, 1, 1, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.2910858484062725, accuracy 0.8727675965915723\n",
            "Starting epoch 2/3\n",
            "Train loss 0.18711275036267672, accuracy 0.9252558266215323\n",
            "Starting epoch 3/3\n",
            "Train loss 0.14276475946796513, accuracy 0.9426092369946695\n",
            "Evaluating on arabic test set...\n",
            "Evaluating on chinese test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('arabic', 'english')\n",
            "Texts to tokenize: ['انقسام إيراني حول التعامل بايدن', 'تفاصيل واقعة حى الشيخ جراح بالقدس محاولات إسرائيل لإخلاء منازل الفلسطينيين', 'الأهلى يحذر لاعبيه قبل الإجازة الاختلاط ممنوع ورفع الكمامة مرفوض', 'السويد تعلن وفاة امرأة بعد تطعيمها بلقاح «أسترازينيكا»', '#شرطة_الرياض: القبض 6 وافدين اقتحموا مقار شركات وسرقوا أموالاً ومعدات.\\n\\n']\n",
            "Labels: [1, 1, 0, 1, 1]\n",
            "Texts to tokenize: ['بالفيديو – شكل ابنتي #أشرف_زكي و #روجينا حديث #الجمهور... كشفتا والدهما؟', 'الموت يفجع حجاج عبد العظيم مرتين في 4 أيام', 'تاكوبيل تعيد البطاطا، الأمور إثارة!', 'الصحة: مد فترة الحملة القومية الثانية للتطعيم ضد مرض شلل الأطفال حتى غد', 'إحباط محاولة تهريب 20 مليون قرص أمفيتامين مخدر مخبأة داخل شحنة فاكهة العنب.\\n']\n",
            "Labels: [1, 0, 1, 0, 0]\n",
            "Texts to tokenize: ['Filipino activist arrested for disrupting Manila Cathedral mass in Reproductive Health Bill protest', 'International Board fixes soccer field size, halts technology experiments', '24 Rules For Women On A First Date With A Man', 'Political fallout from the sacking of Professor David Nutt gathers momentum', 'Which \"Clueless\" Character Are You Based On Your Zodiac Sign']\n",
            "Labels: [0, 0, 1, 0, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.15984121724258418, accuracy 0.9297996155567844\n",
            "Starting epoch 2/3\n",
            "Train loss 0.10513235652008096, accuracy 0.9570266213760961\n",
            "Starting epoch 3/3\n",
            "Train loss 0.08499231410498569, accuracy 0.9643205097822367\n",
            "Evaluating on arabic test set...\n",
            "Evaluating on english test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('arabic', 'german')\n",
            "Texts to tokenize: ['انقسام إيراني حول التعامل بايدن', 'تفاصيل واقعة حى الشيخ جراح بالقدس محاولات إسرائيل لإخلاء منازل الفلسطينيين', 'الأهلى يحذر لاعبيه قبل الإجازة الاختلاط ممنوع ورفع الكمامة مرفوض', 'السويد تعلن وفاة امرأة بعد تطعيمها بلقاح «أسترازينيكا»', '#شرطة_الرياض: القبض 6 وافدين اقتحموا مقار شركات وسرقوا أموالاً ومعدات.\\n\\n']\n",
            "Labels: [1, 1, 0, 1, 1]\n",
            "Texts to tokenize: ['بالفيديو – شكل ابنتي #أشرف_زكي و #روجينا حديث #الجمهور... كشفتا والدهما؟', 'الموت يفجع حجاج عبد العظيم مرتين في 4 أيام', 'تاكوبيل تعيد البطاطا، الأمور إثارة!', 'الصحة: مد فترة الحملة القومية الثانية للتطعيم ضد مرض شلل الأطفال حتى غد', 'إحباط محاولة تهريب 20 مليون قرص أمفيتامين مخدر مخبأة داخل شحنة فاكهة العنب.\\n']\n",
            "Labels: [1, 0, 1, 0, 0]\n",
            "Texts to tokenize: ['Robert Geiss auf Krücken: \"Ich brauche einen Arzt!\" ', '„Promi Big Brother“ 2017: Das ist der Gewinner', 'Laura Müller: Werbepartner erntet Shitstorm', 'Netflix-Serie zu Claas Relotius-Skandal geplant ', 'Shawn Mendes hat Schmetterlinge im Bauch']\n",
            "Labels: [0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.14010760762133947, accuracy 0.9418791333421708\n",
            "Starting epoch 2/3\n",
            "Train loss 0.08350484234638963, accuracy 0.9673898634616234\n",
            "Starting epoch 3/3\n",
            "Train loss 0.05822106578267339, accuracy 0.9765660671939669\n",
            "Evaluating on arabic test set...\n",
            "Evaluating on german test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('arabic', 'indonesian')\n",
            "Texts to tokenize: ['انقسام إيراني حول التعامل بايدن', 'تفاصيل واقعة حى الشيخ جراح بالقدس محاولات إسرائيل لإخلاء منازل الفلسطينيين', 'الأهلى يحذر لاعبيه قبل الإجازة الاختلاط ممنوع ورفع الكمامة مرفوض', 'السويد تعلن وفاة امرأة بعد تطعيمها بلقاح «أسترازينيكا»', '#شرطة_الرياض: القبض 6 وافدين اقتحموا مقار شركات وسرقوا أموالاً ومعدات.\\n\\n']\n",
            "Labels: [1, 1, 0, 1, 1]\n",
            "Texts to tokenize: ['بالفيديو – شكل ابنتي #أشرف_زكي و #روجينا حديث #الجمهور... كشفتا والدهما؟', 'الموت يفجع حجاج عبد العظيم مرتين في 4 أيام', 'تاكوبيل تعيد البطاطا، الأمور إثارة!', 'الصحة: مد فترة الحملة القومية الثانية للتطعيم ضد مرض شلل الأطفال حتى غد', 'إحباط محاولة تهريب 20 مليون قرص أمفيتامين مخدر مخبأة داخل شحنة فاكهة العنب.\\n']\n",
            "Labels: [1, 0, 1, 0, 0]\n",
            "Texts to tokenize: ['Arkeolog Temukan Situs David Vs Goliath seperti Disebut Alkitab', 'Tampil Classy dengan Rok Tutu, Ini 5 Inspirasi Hijab ala Aghnia Punjabi', 'Hasil China Open 2019 - Marcus/Kevin Sukses Bayar Lunas Kekalahan di Kejuaraan Dunia 2019', 'Sepasang Pengungsi Rohingya Tewas dalam Baku Tembak di Bangladesh', 'Jadwal Wakil Indonesia di Semifinal Vietnam Open 2019']\n",
            "Labels: [0, 1, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.44897921340359165, accuracy 0.7900069737867662\n",
            "Starting epoch 2/3\n",
            "Train loss 0.3392272249739275, accuracy 0.8531812774336465\n",
            "Starting epoch 3/3\n",
            "Train loss 0.2698279174724276, accuracy 0.8882963449152891\n",
            "Evaluating on arabic test set...\n",
            "Evaluating on indonesian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('arabic', 'romanian')\n",
            "Texts to tokenize: ['انقسام إيراني حول التعامل بايدن', 'تفاصيل واقعة حى الشيخ جراح بالقدس محاولات إسرائيل لإخلاء منازل الفلسطينيين', 'الأهلى يحذر لاعبيه قبل الإجازة الاختلاط ممنوع ورفع الكمامة مرفوض', 'السويد تعلن وفاة امرأة بعد تطعيمها بلقاح «أسترازينيكا»', '#شرطة_الرياض: القبض 6 وافدين اقتحموا مقار شركات وسرقوا أموالاً ومعدات.\\n\\n']\n",
            "Labels: [1, 1, 0, 1, 1]\n",
            "Texts to tokenize: ['بالفيديو – شكل ابنتي #أشرف_زكي و #روجينا حديث #الجمهور... كشفتا والدهما؟', 'الموت يفجع حجاج عبد العظيم مرتين في 4 أيام', 'تاكوبيل تعيد البطاطا، الأمور إثارة!', 'الصحة: مد فترة الحملة القومية الثانية للتطعيم ضد مرض شلل الأطفال حتى غد', 'إحباط محاولة تهريب 20 مليون قرص أمفيتامين مخدر مخبأة داخل شحنة فاكهة العنب.\\n']\n",
            "Labels: [1, 0, 1, 0, 0]\n",
            "Texts to tokenize: ['Top 10 al celor mai doriți angajatori din România este dominat de companii de tehnologie', 'Termenii și condițiile de utilizare, pe limba ta: ce vor oficialii', 'Apple, Huawei sau Oppo, zdrobite de Xiaomi: cum a avut atât de câștigat', 'Spotify a lansat Greenroom, o rețea de socializare audio similară cu Clubhouse', 'Telefoanele ieftine cu Android vor fi mult mai bune, mai rapide și mai eficiente: ce s-a schimbat']\n",
            "Labels: [1, 1, 1, 0, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.40564835645773234, accuracy 0.810251542477456\n",
            "Starting epoch 2/3\n",
            "Train loss 0.2854398874058222, accuracy 0.8819648789748457\n",
            "Starting epoch 3/3\n",
            "Train loss 0.2261807980679185, accuracy 0.9083056478405315\n",
            "Evaluating on arabic test set...\n",
            "Evaluating on romanian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('arabic', 'turkish')\n",
            "Warning: Dataset for turkish is empty after cleaning. Skipping this combination.\n",
            "Processing combination: ('bangla', 'chinese')\n",
            "Texts to tokenize: ['ভারতীয় দলে ‘গ্রুপিং’ চরমে, অবসর নেয়ার চিন্তায় কোহলি!', 'স্বামীর মাঝে নিজের বাবাকে খুঁজে পেয়েছেন প্রিয়াংকা', 'তুইও বিক্রি হয়ে গেলি, খেলতে নেমে গেলি : সায়নীকে শ্রীলেখা', 'চলতি মৌসুমে আজ ঢাকায় সর্বোচ্চ বৃষ্টি, তলিয়ে গেছে অধিকাংশ সড়ক', 'মেয়েকে ফিরে পেতে তওবা করে অভিনয় ছাড়েন শাবানা']\n",
            "Labels: [0, 0, 0, 0, 0]\n",
            "Texts to tokenize: ['৬ দিন পর সৌদি আরব যাওয়ার কথা, তার আগে যুবকের রহস্যজনক মৃ’ত্যু', 'এই সেই সন্তান যিনি কিনা তার মাকে গর্ভ;বতী করে ফেলেন, উপায় না পেয়ে মা;য়ের সাথে বিয়ে!', 'কারিনার থেকে তিনগুণ বেশি পারিশ্রমিক কঙ্গনার!', 'ই-কমার্স ব্যবসায় সাকিব - Techzoom.TV', '২৫ লাখ টাকার মোটরসাইকেলে বিশ্বভ্রমণে অজিত']\n",
            "Labels: [0, 1, 0, 0, 0]\n",
            "Texts to tokenize: ['他终于承认了长达10年的地下情', '“深圳没有早睡的人”', '“加点旋转更舒服？”男朋友居然和舍友做这种不可描述的事情！', '你为什么不联系微信好友了？', '这4个坏习惯最伤肾 你有做过吗？']\n",
            "Labels: [1, 0, 1, 1, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.2028451325050644, accuracy 0.9170632698768197\n",
            "Starting epoch 2/3\n",
            "Train loss 0.09309682321436454, accuracy 0.964095744680851\n",
            "Starting epoch 3/3\n",
            "Train loss 0.04021895558337085, accuracy 0.9860022396416573\n",
            "Evaluating on bangla test set...\n",
            "Evaluating on chinese test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('bangla', 'english')\n",
            "Texts to tokenize: ['ভারতীয় দলে ‘গ্রুপিং’ চরমে, অবসর নেয়ার চিন্তায় কোহলি!', 'স্বামীর মাঝে নিজের বাবাকে খুঁজে পেয়েছেন প্রিয়াংকা', 'তুইও বিক্রি হয়ে গেলি, খেলতে নেমে গেলি : সায়নীকে শ্রীলেখা', 'চলতি মৌসুমে আজ ঢাকায় সর্বোচ্চ বৃষ্টি, তলিয়ে গেছে অধিকাংশ সড়ক', 'মেয়েকে ফিরে পেতে তওবা করে অভিনয় ছাড়েন শাবানা']\n",
            "Labels: [0, 0, 0, 0, 0]\n",
            "Texts to tokenize: ['৬ দিন পর সৌদি আরব যাওয়ার কথা, তার আগে যুবকের রহস্যজনক মৃ’ত্যু', 'এই সেই সন্তান যিনি কিনা তার মাকে গর্ভ;বতী করে ফেলেন, উপায় না পেয়ে মা;য়ের সাথে বিয়ে!', 'কারিনার থেকে তিনগুণ বেশি পারিশ্রমিক কঙ্গনার!', 'ই-কমার্স ব্যবসায় সাকিব - Techzoom.TV', '২৫ লাখ টাকার মোটরসাইকেলে বিশ্বভ্রমণে অজিত']\n",
            "Labels: [0, 1, 0, 0, 0]\n",
            "Texts to tokenize: ['Filipino activist arrested for disrupting Manila Cathedral mass in Reproductive Health Bill protest', 'International Board fixes soccer field size, halts technology experiments', '24 Rules For Women On A First Date With A Man', 'Political fallout from the sacking of Professor David Nutt gathers momentum', 'Which \"Clueless\" Character Are You Based On Your Zodiac Sign']\n",
            "Labels: [0, 0, 1, 0, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.05452053626515445, accuracy 0.9772624604728204\n",
            "Starting epoch 2/3\n",
            "Train loss 0.02381183109289298, accuracy 0.9906640566179793\n",
            "Starting epoch 3/3\n",
            "Train loss 0.011149908165165102, accuracy 0.9958214124378859\n",
            "Evaluating on bangla test set...\n",
            "Evaluating on english test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('bangla', 'german')\n",
            "Texts to tokenize: ['ভারতীয় দলে ‘গ্রুপিং’ চরমে, অবসর নেয়ার চিন্তায় কোহলি!', 'স্বামীর মাঝে নিজের বাবাকে খুঁজে পেয়েছেন প্রিয়াংকা', 'তুইও বিক্রি হয়ে গেলি, খেলতে নেমে গেলি : সায়নীকে শ্রীলেখা', 'চলতি মৌসুমে আজ ঢাকায় সর্বোচ্চ বৃষ্টি, তলিয়ে গেছে অধিকাংশ সড়ক', 'মেয়েকে ফিরে পেতে তওবা করে অভিনয় ছাড়েন শাবানা']\n",
            "Labels: [0, 0, 0, 0, 0]\n",
            "Texts to tokenize: ['৬ দিন পর সৌদি আরব যাওয়ার কথা, তার আগে যুবকের রহস্যজনক মৃ’ত্যু', 'এই সেই সন্তান যিনি কিনা তার মাকে গর্ভ;বতী করে ফেলেন, উপায় না পেয়ে মা;য়ের সাথে বিয়ে!', 'কারিনার থেকে তিনগুণ বেশি পারিশ্রমিক কঙ্গনার!', 'ই-কমার্স ব্যবসায় সাকিব - Techzoom.TV', '২৫ লাখ টাকার মোটরসাইকেলে বিশ্বভ্রমণে অজিত']\n",
            "Labels: [0, 1, 0, 0, 0]\n",
            "Texts to tokenize: ['Robert Geiss auf Krücken: \"Ich brauche einen Arzt!\" ', '„Promi Big Brother“ 2017: Das ist der Gewinner', 'Laura Müller: Werbepartner erntet Shitstorm', 'Netflix-Serie zu Claas Relotius-Skandal geplant ', 'Shawn Mendes hat Schmetterlinge im Bauch']\n",
            "Labels: [0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.09006822330845195, accuracy 0.9653328611898017\n",
            "Starting epoch 2/3\n",
            "Train loss 0.04010188399811019, accuracy 0.9840651558073654\n",
            "Starting epoch 3/3\n",
            "Train loss 0.01809772559397393, accuracy 0.9928824362606231\n",
            "Evaluating on bangla test set...\n",
            "Evaluating on german test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('bangla', 'indonesian')\n",
            "Texts to tokenize: ['ভারতীয় দলে ‘গ্রুপিং’ চরমে, অবসর নেয়ার চিন্তায় কোহলি!', 'স্বামীর মাঝে নিজের বাবাকে খুঁজে পেয়েছেন প্রিয়াংকা', 'তুইও বিক্রি হয়ে গেলি, খেলতে নেমে গেলি : সায়নীকে শ্রীলেখা', 'চলতি মৌসুমে আজ ঢাকায় সর্বোচ্চ বৃষ্টি, তলিয়ে গেছে অধিকাংশ সড়ক', 'মেয়েকে ফিরে পেতে তওবা করে অভিনয় ছাড়েন শাবানা']\n",
            "Labels: [0, 0, 0, 0, 0]\n",
            "Texts to tokenize: ['৬ দিন পর সৌদি আরব যাওয়ার কথা, তার আগে যুবকের রহস্যজনক মৃ’ত্যু', 'এই সেই সন্তান যিনি কিনা তার মাকে গর্ভ;বতী করে ফেলেন, উপায় না পেয়ে মা;য়ের সাথে বিয়ে!', 'কারিনার থেকে তিনগুণ বেশি পারিশ্রমিক কঙ্গনার!', 'ই-কমার্স ব্যবসায় সাকিব - Techzoom.TV', '২৫ লাখ টাকার মোটরসাইকেলে বিশ্বভ্রমণে অজিত']\n",
            "Labels: [0, 1, 0, 0, 0]\n",
            "Texts to tokenize: ['Arkeolog Temukan Situs David Vs Goliath seperti Disebut Alkitab', 'Tampil Classy dengan Rok Tutu, Ini 5 Inspirasi Hijab ala Aghnia Punjabi', 'Hasil China Open 2019 - Marcus/Kevin Sukses Bayar Lunas Kekalahan di Kejuaraan Dunia 2019', 'Sepasang Pengungsi Rohingya Tewas dalam Baku Tembak di Bangladesh', 'Jadwal Wakil Indonesia di Semifinal Vietnam Open 2019']\n",
            "Labels: [0, 1, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.49017503311910404, accuracy 0.7664301141622956\n",
            "Starting epoch 2/3\n",
            "Train loss 0.37201290190881336, accuracy 0.8369330453563715\n",
            "Starting epoch 3/3\n",
            "Train loss 0.26471382764692886, accuracy 0.8907744523295279\n",
            "Evaluating on bangla test set...\n",
            "Evaluating on indonesian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('bangla', 'romanian')\n",
            "Texts to tokenize: ['ভারতীয় দলে ‘গ্রুপিং’ চরমে, অবসর নেয়ার চিন্তায় কোহলি!', 'স্বামীর মাঝে নিজের বাবাকে খুঁজে পেয়েছেন প্রিয়াংকা', 'তুইও বিক্রি হয়ে গেলি, খেলতে নেমে গেলি : সায়নীকে শ্রীলেখা', 'চলতি মৌসুমে আজ ঢাকায় সর্বোচ্চ বৃষ্টি, তলিয়ে গেছে অধিকাংশ সড়ক', 'মেয়েকে ফিরে পেতে তওবা করে অভিনয় ছাড়েন শাবানা']\n",
            "Labels: [0, 0, 0, 0, 0]\n",
            "Texts to tokenize: ['৬ দিন পর সৌদি আরব যাওয়ার কথা, তার আগে যুবকের রহস্যজনক মৃ’ত্যু', 'এই সেই সন্তান যিনি কিনা তার মাকে গর্ভ;বতী করে ফেলেন, উপায় না পেয়ে মা;য়ের সাথে বিয়ে!', 'কারিনার থেকে তিনগুণ বেশি পারিশ্রমিক কঙ্গনার!', 'ই-কমার্স ব্যবসায় সাকিব - Techzoom.TV', '২৫ লাখ টাকার মোটরসাইকেলে বিশ্বভ্রমণে অজিত']\n",
            "Labels: [0, 1, 0, 0, 0]\n",
            "Texts to tokenize: ['Top 10 al celor mai doriți angajatori din România este dominat de companii de tehnologie', 'Termenii și condițiile de utilizare, pe limba ta: ce vor oficialii', 'Apple, Huawei sau Oppo, zdrobite de Xiaomi: cum a avut atât de câștigat', 'Spotify a lansat Greenroom, o rețea de socializare audio similară cu Clubhouse', 'Telefoanele ieftine cu Android vor fi mult mai bune, mai rapide și mai eficiente: ce s-a schimbat']\n",
            "Labels: [1, 1, 1, 0, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.3862784422489981, accuracy 0.8241689965827897\n",
            "Starting epoch 2/3\n",
            "Train loss 0.26007045032641546, accuracy 0.8929274101687895\n",
            "Starting epoch 3/3\n",
            "Train loss 0.16529321892877782, accuracy 0.934348141244693\n",
            "Evaluating on bangla test set...\n",
            "Evaluating on romanian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('bangla', 'turkish')\n",
            "Warning: Dataset for turkish is empty after cleaning. Skipping this combination.\n",
            "Processing combination: ('chinese', 'english')\n",
            "Texts to tokenize: ['毁童年！《还珠格格》里的大BOSS竟然是毫不起眼的她！！', '用歌声谢谢你，昌平战疫一线的人们！', '那个在优衣库啪啪啪的女生，出道了', '理学院关于在2018级新生中组织实施“逐梦彩虹人生 启航邮理相伴”新生引航工程的通知', '如何在PPT里添加音频，并跨页播放？']\n",
            "Labels: [1, 0, 1, 0, 1]\n",
            "Texts to tokenize: ['他终于承认了长达10年的地下情', '“深圳没有早睡的人”', '“加点旋转更舒服？”男朋友居然和舍友做这种不可描述的事情！', '你为什么不联系微信好友了？', '这4个坏习惯最伤肾 你有做过吗？']\n",
            "Labels: [1, 0, 1, 1, 1]\n",
            "Texts to tokenize: ['Filipino activist arrested for disrupting Manila Cathedral mass in Reproductive Health Bill protest', 'International Board fixes soccer field size, halts technology experiments', '24 Rules For Women On A First Date With A Man', 'Political fallout from the sacking of Professor David Nutt gathers momentum', 'Which \"Clueless\" Character Are You Based On Your Zodiac Sign']\n",
            "Labels: [0, 0, 1, 0, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.08281504158341152, accuracy 0.9690422361525024\n",
            "Starting epoch 2/3\n",
            "Train loss 0.029967423318114906, accuracy 0.9894923440550818\n",
            "Starting epoch 3/3\n",
            "Train loss 0.010173918818434127, accuracy 0.9966344671667866\n",
            "Evaluating on chinese test set...\n",
            "Evaluating on english test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('chinese', 'german')\n",
            "Texts to tokenize: ['毁童年！《还珠格格》里的大BOSS竟然是毫不起眼的她！！', '用歌声谢谢你，昌平战疫一线的人们！', '那个在优衣库啪啪啪的女生，出道了', '理学院关于在2018级新生中组织实施“逐梦彩虹人生 启航邮理相伴”新生引航工程的通知', '如何在PPT里添加音频，并跨页播放？']\n",
            "Labels: [1, 0, 1, 0, 1]\n",
            "Texts to tokenize: ['他终于承认了长达10年的地下情', '“深圳没有早睡的人”', '“加点旋转更舒服？”男朋友居然和舍友做这种不可描述的事情！', '你为什么不联系微信好友了？', '这4个坏习惯最伤肾 你有做过吗？']\n",
            "Labels: [1, 0, 1, 1, 1]\n",
            "Texts to tokenize: ['Robert Geiss auf Krücken: \"Ich brauche einen Arzt!\" ', '„Promi Big Brother“ 2017: Das ist der Gewinner', 'Laura Müller: Werbepartner erntet Shitstorm', 'Netflix-Serie zu Claas Relotius-Skandal geplant ', 'Shawn Mendes hat Schmetterlinge im Bauch']\n",
            "Labels: [0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.09880512463145977, accuracy 0.9637855897733876\n",
            "Starting epoch 2/3\n",
            "Train loss 0.043004906927528425, accuracy 0.9853718768158048\n",
            "Starting epoch 3/3\n",
            "Train loss 0.01666086958468321, accuracy 0.9946252178965718\n",
            "Evaluating on chinese test set...\n",
            "Evaluating on german test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('chinese', 'indonesian')\n",
            "Texts to tokenize: ['毁童年！《还珠格格》里的大BOSS竟然是毫不起眼的她！！', '用歌声谢谢你，昌平战疫一线的人们！', '那个在优衣库啪啪啪的女生，出道了', '理学院关于在2018级新生中组织实施“逐梦彩虹人生 启航邮理相伴”新生引航工程的通知', '如何在PPT里添加音频，并跨页播放？']\n",
            "Labels: [1, 0, 1, 0, 1]\n",
            "Texts to tokenize: ['他终于承认了长达10年的地下情', '“深圳没有早睡的人”', '“加点旋转更舒服？”男朋友居然和舍友做这种不可描述的事情！', '你为什么不联系微信好友了？', '这4个坏习惯最伤肾 你有做过吗？']\n",
            "Labels: [1, 0, 1, 1, 1]\n",
            "Texts to tokenize: ['Arkeolog Temukan Situs David Vs Goliath seperti Disebut Alkitab', 'Tampil Classy dengan Rok Tutu, Ini 5 Inspirasi Hijab ala Aghnia Punjabi', 'Hasil China Open 2019 - Marcus/Kevin Sukses Bayar Lunas Kekalahan di Kejuaraan Dunia 2019', 'Sepasang Pengungsi Rohingya Tewas dalam Baku Tembak di Bangladesh', 'Jadwal Wakil Indonesia di Semifinal Vietnam Open 2019']\n",
            "Labels: [0, 1, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.31526250493508573, accuracy 0.8573685041857526\n",
            "Starting epoch 2/3\n",
            "Train loss 0.20838590882087876, accuracy 0.9124151003001105\n",
            "Starting epoch 3/3\n",
            "Train loss 0.13504842103756634, accuracy 0.9460985626283367\n",
            "Evaluating on chinese test set...\n",
            "Evaluating on indonesian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('chinese', 'romanian')\n",
            "Texts to tokenize: ['毁童年！《还珠格格》里的大BOSS竟然是毫不起眼的她！！', '用歌声谢谢你，昌平战疫一线的人们！', '那个在优衣库啪啪啪的女生，出道了', '理学院关于在2018级新生中组织实施“逐梦彩虹人生 启航邮理相伴”新生引航工程的通知', '如何在PPT里添加音频，并跨页播放？']\n",
            "Labels: [1, 0, 1, 0, 1]\n",
            "Texts to tokenize: ['他终于承认了长达10年的地下情', '“深圳没有早睡的人”', '“加点旋转更舒服？”男朋友居然和舍友做这种不可描述的事情！', '你为什么不联系微信好友了？', '这4个坏习惯最伤肾 你有做过吗？']\n",
            "Labels: [1, 0, 1, 1, 1]\n",
            "Texts to tokenize: ['Top 10 al celor mai doriți angajatori din România este dominat de companii de tehnologie', 'Termenii și condițiile de utilizare, pe limba ta: ce vor oficialii', 'Apple, Huawei sau Oppo, zdrobite de Xiaomi: cum a avut atât de câștigat', 'Spotify a lansat Greenroom, o rețea de socializare audio similară cu Clubhouse', 'Telefoanele ieftine cu Android vor fi mult mai bune, mai rapide și mai eficiente: ce s-a schimbat']\n",
            "Labels: [1, 1, 1, 0, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n",
            "Train loss 0.23665291754093992, accuracy 0.8990779851932598\n",
            "Starting epoch 2/3\n",
            "Train loss 0.13386962877737985, accuracy 0.9483126674842167\n",
            "Starting epoch 3/3\n",
            "Train loss 0.07359250029929446, accuracy 0.9735658809102058\n",
            "Evaluating on chinese test set...\n",
            "Evaluating on romanian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('chinese', 'turkish')\n",
            "Warning: Dataset for turkish is empty after cleaning. Skipping this combination.\n",
            "Processing combination: ('english', 'german')\n",
            "Texts to tokenize: ['Soccer Provides Oasis in Mexican City Ravaged by Drug War', 'Guys Try Tinder', 'Five police officers injured in Naples protest over new garbage tip', 'Michael B. Jordan Got Laid The Fuck Out While Filming \"Creed\"', 'International experts probe deadly Ebola Reston virus outbreak in Philippine pigs']\n",
            "Labels: [0, 1, 0, 1, 0]\n",
            "Texts to tokenize: ['Filipino activist arrested for disrupting Manila Cathedral mass in Reproductive Health Bill protest', 'International Board fixes soccer field size, halts technology experiments', '24 Rules For Women On A First Date With A Man', 'Political fallout from the sacking of Professor David Nutt gathers momentum', 'Which \"Clueless\" Character Are You Based On Your Zodiac Sign']\n",
            "Labels: [0, 0, 1, 0, 1]\n",
            "Texts to tokenize: ['Robert Geiss auf Krücken: \"Ich brauche einen Arzt!\" ', '„Promi Big Brother“ 2017: Das ist der Gewinner', 'Laura Müller: Werbepartner erntet Shitstorm', 'Netflix-Serie zu Claas Relotius-Skandal geplant ', 'Shawn Mendes hat Schmetterlinge im Bauch']\n",
            "Labels: [0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.07047657952433259, accuracy 0.9751836875585581\n",
            "Starting epoch 2/3\n",
            "Train loss 0.026901164189739018, accuracy 0.9907293259036442\n",
            "Starting epoch 3/3\n",
            "Train loss 0.009271657040652212, accuracy 0.9969179939839243\n",
            "Evaluating on english test set...\n",
            "Evaluating on german test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('english', 'indonesian')\n",
            "Texts to tokenize: ['Soccer Provides Oasis in Mexican City Ravaged by Drug War', 'Guys Try Tinder', 'Five police officers injured in Naples protest over new garbage tip', 'Michael B. Jordan Got Laid The Fuck Out While Filming \"Creed\"', 'International experts probe deadly Ebola Reston virus outbreak in Philippine pigs']\n",
            "Labels: [0, 1, 0, 1, 0]\n",
            "Texts to tokenize: ['Filipino activist arrested for disrupting Manila Cathedral mass in Reproductive Health Bill protest', 'International Board fixes soccer field size, halts technology experiments', '24 Rules For Women On A First Date With A Man', 'Political fallout from the sacking of Professor David Nutt gathers momentum', 'Which \"Clueless\" Character Are You Based On Your Zodiac Sign']\n",
            "Labels: [0, 0, 1, 0, 1]\n",
            "Texts to tokenize: ['Arkeolog Temukan Situs David Vs Goliath seperti Disebut Alkitab', 'Tampil Classy dengan Rok Tutu, Ini 5 Inspirasi Hijab ala Aghnia Punjabi', 'Hasil China Open 2019 - Marcus/Kevin Sukses Bayar Lunas Kekalahan di Kejuaraan Dunia 2019', 'Sepasang Pengungsi Rohingya Tewas dalam Baku Tembak di Bangladesh', 'Jadwal Wakil Indonesia di Semifinal Vietnam Open 2019']\n",
            "Labels: [0, 1, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.18547694614702004, accuracy 0.9153457446808511\n",
            "Starting epoch 2/3\n",
            "Train loss 0.12628027204121958, accuracy 0.9449202127659574\n",
            "Starting epoch 3/3\n",
            "Train loss 0.08848643573589915, accuracy 0.9633776595744681\n",
            "Evaluating on english test set...\n",
            "Evaluating on indonesian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('english', 'romanian')\n",
            "Texts to tokenize: ['Soccer Provides Oasis in Mexican City Ravaged by Drug War', 'Guys Try Tinder', 'Five police officers injured in Naples protest over new garbage tip', 'Michael B. Jordan Got Laid The Fuck Out While Filming \"Creed\"', 'International experts probe deadly Ebola Reston virus outbreak in Philippine pigs']\n",
            "Labels: [0, 1, 0, 1, 0]\n",
            "Texts to tokenize: ['Filipino activist arrested for disrupting Manila Cathedral mass in Reproductive Health Bill protest', 'International Board fixes soccer field size, halts technology experiments', '24 Rules For Women On A First Date With A Man', 'Political fallout from the sacking of Professor David Nutt gathers momentum', 'Which \"Clueless\" Character Are You Based On Your Zodiac Sign']\n",
            "Labels: [0, 0, 1, 0, 1]\n",
            "Texts to tokenize: ['Top 10 al celor mai doriți angajatori din România este dominat de companii de tehnologie', 'Termenii și condițiile de utilizare, pe limba ta: ce vor oficialii', 'Apple, Huawei sau Oppo, zdrobite de Xiaomi: cum a avut atât de câștigat', 'Spotify a lansat Greenroom, o rețea de socializare audio similară cu Clubhouse', 'Telefoanele ieftine cu Android vor fi mult mai bune, mai rapide și mai eficiente: ce s-a schimbat']\n",
            "Labels: [1, 1, 1, 0, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.11711099286368601, accuracy 0.9496690286647421\n",
            "Starting epoch 2/3\n",
            "Train loss 0.0686485300834715, accuracy 0.972093430146094\n",
            "Starting epoch 3/3\n",
            "Train loss 0.04073147940173962, accuracy 0.9838450995830053\n",
            "Evaluating on english test set...\n",
            "Evaluating on romanian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('english', 'turkish')\n",
            "Warning: Dataset for turkish is empty after cleaning. Skipping this combination.\n",
            "Processing combination: ('german', 'indonesian')\n",
            "Texts to tokenize: ['Helene Fischer über ihre Auszeit: „Ich werde schon noch wiederkommen“ ', '\"Sturm der Liebe\": Ariane will die Trennung | Lässt Christoph sie\\n', 'Kultstar Terence Hill kommt zur Brückeneinweihung nach Worms ', 'Norwegen: Nach Breivik-Anschlägen sollen Anti-Terrorgesetze verschärft werden', 'Bundesschiedsgericht der Piratenpartei lehnt Parteiausschluss ab']\n",
            "Labels: [0, 0, 0, 1, 1]\n",
            "Texts to tokenize: ['Robert Geiss auf Krücken: \"Ich brauche einen Arzt!\" ', '„Promi Big Brother“ 2017: Das ist der Gewinner', 'Laura Müller: Werbepartner erntet Shitstorm', 'Netflix-Serie zu Claas Relotius-Skandal geplant ', 'Shawn Mendes hat Schmetterlinge im Bauch']\n",
            "Labels: [0, 0, 0, 0, 0]\n",
            "Texts to tokenize: ['Arkeolog Temukan Situs David Vs Goliath seperti Disebut Alkitab', 'Tampil Classy dengan Rok Tutu, Ini 5 Inspirasi Hijab ala Aghnia Punjabi', 'Hasil China Open 2019 - Marcus/Kevin Sukses Bayar Lunas Kekalahan di Kejuaraan Dunia 2019', 'Sepasang Pengungsi Rohingya Tewas dalam Baku Tembak di Bangladesh', 'Jadwal Wakil Indonesia di Semifinal Vietnam Open 2019']\n",
            "Labels: [0, 1, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.15446065379683507, accuracy 0.9337638485692281\n",
            "Starting epoch 2/3\n",
            "Train loss 0.09612597698824821, accuracy 0.9599354227146158\n",
            "Starting epoch 3/3\n",
            "Train loss 0.06019564174822288, accuracy 0.975265122341371\n",
            "Evaluating on german test set...\n",
            "Evaluating on indonesian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('german', 'romanian')\n",
            "Texts to tokenize: ['Helene Fischer über ihre Auszeit: „Ich werde schon noch wiederkommen“ ', '\"Sturm der Liebe\": Ariane will die Trennung | Lässt Christoph sie\\n', 'Kultstar Terence Hill kommt zur Brückeneinweihung nach Worms ', 'Norwegen: Nach Breivik-Anschlägen sollen Anti-Terrorgesetze verschärft werden', 'Bundesschiedsgericht der Piratenpartei lehnt Parteiausschluss ab']\n",
            "Labels: [0, 0, 0, 1, 1]\n",
            "Texts to tokenize: ['Robert Geiss auf Krücken: \"Ich brauche einen Arzt!\" ', '„Promi Big Brother“ 2017: Das ist der Gewinner', 'Laura Müller: Werbepartner erntet Shitstorm', 'Netflix-Serie zu Claas Relotius-Skandal geplant ', 'Shawn Mendes hat Schmetterlinge im Bauch']\n",
            "Labels: [0, 0, 0, 0, 0]\n",
            "Texts to tokenize: ['Top 10 al celor mai doriți angajatori din România este dominat de companii de tehnologie', 'Termenii și condițiile de utilizare, pe limba ta: ce vor oficialii', 'Apple, Huawei sau Oppo, zdrobite de Xiaomi: cum a avut atât de câștigat', 'Spotify a lansat Greenroom, o rețea de socializare audio similară cu Clubhouse', 'Telefoanele ieftine cu Android vor fi mult mai bune, mai rapide și mai eficiente: ce s-a schimbat']\n",
            "Labels: [1, 1, 1, 0, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.11988878521302485, accuracy 0.9538382469747233\n",
            "Starting epoch 2/3\n",
            "Train loss 0.06447986459924956, accuracy 0.9753928576990766\n",
            "Starting epoch 3/3\n",
            "Train loss 0.03590503679449516, accuracy 0.9867931286891246\n",
            "Evaluating on german test set...\n",
            "Evaluating on romanian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('german', 'turkish')\n",
            "Warning: Dataset for turkish is empty after cleaning. Skipping this combination.\n",
            "Processing combination: ('indonesian', 'romanian')\n",
            "Texts to tokenize: ['Ibu Kota Baru Akan Miliki Transportasi Berkonsep Smart City', 'Chapter 956 One Piece Ungkapkan Kematian Orang Dekat Luffy?', 'Ayu Ting Ting Enggan Rayakan Ulang Tahun Keenam Bilqis, Kenapa?   ', 'Komisi IV DPR Setujui Anggaran Kementan Rp 21 T di 2020', 'Heboh Babi Masuk Masjid: Ibu-ibu Teriak, Si Babi Mati Ditembak']\n",
            "Labels: [0, 1, 1, 0, 0]\n",
            "Texts to tokenize: ['Arkeolog Temukan Situs David Vs Goliath seperti Disebut Alkitab', 'Tampil Classy dengan Rok Tutu, Ini 5 Inspirasi Hijab ala Aghnia Punjabi', 'Hasil China Open 2019 - Marcus/Kevin Sukses Bayar Lunas Kekalahan di Kejuaraan Dunia 2019', 'Sepasang Pengungsi Rohingya Tewas dalam Baku Tembak di Bangladesh', 'Jadwal Wakil Indonesia di Semifinal Vietnam Open 2019']\n",
            "Labels: [0, 1, 0, 0, 0]\n",
            "Texts to tokenize: ['Top 10 al celor mai doriți angajatori din România este dominat de companii de tehnologie', 'Termenii și condițiile de utilizare, pe limba ta: ce vor oficialii', 'Apple, Huawei sau Oppo, zdrobite de Xiaomi: cum a avut atât de câștigat', 'Spotify a lansat Greenroom, o rețea de socializare audio similară cu Clubhouse', 'Telefoanele ieftine cu Android vor fi mult mai bune, mai rapide și mai eficiente: ce s-a schimbat']\n",
            "Labels: [1, 1, 1, 0, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.42449577358857926, accuracy 0.8028802010341661\n",
            "Starting epoch 2/3\n",
            "Train loss 0.3143414980983808, accuracy 0.8644469144155028\n",
            "Starting epoch 3/3\n",
            "Train loss 0.2183651244195849, accuracy 0.9137872710578456\n",
            "Evaluating on indonesian test set...\n",
            "Evaluating on romanian test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0feaa53fd63d>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing combination: ('indonesian', 'turkish')\n",
            "Warning: Dataset for turkish is empty after cleaning. Skipping this combination.\n",
            "Processing combination: ('romanian', 'turkish')\n",
            "Warning: Dataset for turkish is empty after cleaning. Skipping this combination.\n",
            "Final results: [{'languages': ('hebrew', 'arabic'), 'test_results_lang1': {'accuracy': 0.8293838862559242, 'precision': 0.829517616635037, 'recall': 0.8293838862559242, 'f1': 0.8293762215607874}, 'test_results_lang2': {'accuracy': 0.8655896607431341, 'precision': 0.8691414458247566, 'recall': 0.865589660743134, 'f1': 0.8650791697953828}}, {'languages': ('hebrew', 'bangla'), 'test_results_lang1': {'accuracy': 0.8341232227488152, 'precision': 0.8365070574902124, 'recall': 0.8341232227488151, 'f1': 0.8337871449938657}, 'test_results_lang2': {'accuracy': 0.7219917012448133, 'precision': 0.7126977788123341, 'recall': 0.7219917012448133, 'f1': 0.7134814133935211}}, {'languages': ('hebrew', 'chinese'), 'test_results_lang1': {'accuracy': 0.7914691943127963, 'precision': 0.7965670190788673, 'recall': 0.7914691943127962, 'f1': 0.7904805842984762}, 'test_results_lang2': {'accuracy': 0.9681872749099639, 'precision': 0.9680516607898542, 'recall': 0.968187274909964, 'f1': 0.9680667590235815}}, {'languages': ('hebrew', 'english'), 'test_results_lang1': {'accuracy': 0.8436018957345972, 'precision': 0.8461824297392722, 'recall': 0.8436018957345972, 'f1': 0.8433486023610652}, 'test_results_lang2': {'accuracy': 0.9971875, 'precision': 0.9971891286171095, 'recall': 0.9971875, 'f1': 0.9971874373425871}}, {'languages': ('hebrew', 'german'), 'test_results_lang1': {'accuracy': 0.8293838862559242, 'precision': 0.8304071520896166, 'recall': 0.8293838862559242, 'f1': 0.8292227539304777}, 'test_results_lang2': {'accuracy': 0.9834293948126802, 'precision': 0.983547303981182, 'recall': 0.9834293948126801, 'f1': 0.9834795745015273}}, {'languages': ('hebrew', 'indonesian'), 'test_results_lang1': {'accuracy': 0.8199052132701422, 'precision': 0.8208951396155187, 'recall': 0.8199052132701422, 'f1': 0.8197351291488377}, 'test_results_lang2': {'accuracy': 0.819, 'precision': 0.8182153831310225, 'recall': 0.819, 'f1': 0.8184499592808245}}, {'languages': ('hebrew', 'romanian'), 'test_results_lang1': {'accuracy': 0.8625592417061612, 'precision': 0.8628341897218038, 'recall': 0.8625592417061612, 'f1': 0.8625221831936452}, 'test_results_lang2': {'accuracy': 0.8831646734130635, 'precision': 0.8834284674931827, 'recall': 0.8831646734130635, 'f1': 0.8832595803224769}}, {'languages': ('arabic', 'bangla'), 'test_results_lang1': {'accuracy': 0.8662358642972537, 'precision': 0.8698920180055858, 'recall': 0.8662358642972536, 'f1': 0.8657168535012005}, 'test_results_lang2': {'accuracy': 0.7385892116182573, 'precision': 0.7302618613540619, 'recall': 0.7385892116182573, 'f1': 0.725377659097778}}, {'languages': ('arabic', 'chinese'), 'test_results_lang1': {'accuracy': 0.8668820678513732, 'precision': 0.8706443056783517, 'recall': 0.8668820678513732, 'f1': 0.8663545288763422}, 'test_results_lang2': {'accuracy': 0.963985594237695, 'precision': 0.9638077160439985, 'recall': 0.963985594237695, 'f1': 0.9637851444923715}}, {'languages': ('arabic', 'english'), 'test_results_lang1': {'accuracy': 0.8688206785137319, 'precision': 0.8716877578332868, 'recall': 0.8688206785137318, 'f1': 0.8684045055620675}, 'test_results_lang2': {'accuracy': 0.99546875, 'precision': 0.9954688307293458, 'recall': 0.99546875, 'f1': 0.9954687660490401}}, {'languages': ('arabic', 'german'), 'test_results_lang1': {'accuracy': 0.867205169628433, 'precision': 0.869928799730426, 'recall': 0.867205169628433, 'f1': 0.8667986345902919}, 'test_results_lang2': {'accuracy': 0.9832132564841499, 'precision': 0.98321669522975, 'recall': 0.9832132564841498, 'f1': 0.9832149660035234}}, {'languages': ('arabic', 'indonesian'), 'test_results_lang1': {'accuracy': 0.863004846526656, 'precision': 0.8676404909615484, 'recall': 0.8630048465266559, 'f1': 0.8623544235102861}, 'test_results_lang2': {'accuracy': 0.8243333333333333, 'precision': 0.8233757078226138, 'recall': 0.8243333333333334, 'f1': 0.8229141974312085}}, {'languages': ('arabic', 'romanian'), 'test_results_lang1': {'accuracy': 0.8642972536348951, 'precision': 0.8684244023064654, 'recall': 0.864297253634895, 'f1': 0.8637133050770022}, 'test_results_lang2': {'accuracy': 0.890524379024839, 'precision': 0.8906166622320975, 'recall': 0.890524379024839, 'f1': 0.890563657504085}}, {'languages': ('bangla', 'chinese'), 'test_results_lang1': {'accuracy': 0.7551867219917012, 'precision': 0.7521941641555767, 'recall': 0.7551867219917012, 'f1': 0.7533505164717053}, 'test_results_lang2': {'accuracy': 0.9657863145258102, 'precision': 0.9656321034178953, 'recall': 0.9657863145258103, 'f1': 0.965656703100833}}, {'languages': ('bangla', 'english'), 'test_results_lang1': {'accuracy': 0.7468879668049793, 'precision': 0.744920854464423, 'recall': 0.7468879668049793, 'f1': 0.7457846393301975}, 'test_results_lang2': {'accuracy': 0.99640625, 'precision': 0.9964067633987455, 'recall': 0.99640625, 'f1': 0.9964062876584187}}, {'languages': ('bangla', 'german'), 'test_results_lang1': {'accuracy': 0.7261410788381742, 'precision': 0.7330304743754442, 'recall': 0.7261410788381742, 'f1': 0.7287852900496298}, 'test_results_lang2': {'accuracy': 0.9824207492795389, 'precision': 0.9825173017525489, 'recall': 0.9824207492795389, 'f1': 0.9824634143756009}}, {'languages': ('bangla', 'indonesian'), 'test_results_lang1': {'accuracy': 0.7136929460580913, 'precision': 0.7054295998102356, 'recall': 0.7136929460580913, 'f1': 0.707377640024609}, 'test_results_lang2': {'accuracy': 0.813, 'precision': 0.8127791113166595, 'recall': 0.813, 'f1': 0.8128820688037666}}, {'languages': ('bangla', 'romanian'), 'test_results_lang1': {'accuracy': 0.7551867219917012, 'precision': 0.7496897440269804, 'recall': 0.7551867219917012, 'f1': 0.7507534545957254}, 'test_results_lang2': {'accuracy': 0.8817847286108556, 'precision': 0.8816995110370923, 'recall': 0.8817847286108556, 'f1': 0.881732848921262}}, {'languages': ('chinese', 'english'), 'test_results_lang1': {'accuracy': 0.9672869147659063, 'precision': 0.9672556889669313, 'recall': 0.9672869147659063, 'f1': 0.9672703352713578}, 'test_results_lang2': {'accuracy': 0.9965625, 'precision': 0.9965643935913409, 'recall': 0.9965625, 'f1': 0.9965625705293548}}, {'languages': ('chinese', 'german'), 'test_results_lang1': {'accuracy': 0.963985594237695, 'precision': 0.9638331431390497, 'recall': 0.963985594237695, 'f1': 0.9638744146581963}, 'test_results_lang2': {'accuracy': 0.9835734870317003, 'precision': 0.9835942392770426, 'recall': 0.9835734870317003, 'f1': 0.9835835087207796}}, {'languages': ('chinese', 'indonesian'), 'test_results_lang1': {'accuracy': 0.9648859543817526, 'precision': 0.9647201446596037, 'recall': 0.9648859543817527, 'f1': 0.9646842061483009}, 'test_results_lang2': {'accuracy': 0.8273333333333333, 'precision': 0.8264602886829289, 'recall': 0.8273333333333334, 'f1': 0.8265981167973738}}, {'languages': ('chinese', 'romanian'), 'test_results_lang1': {'accuracy': 0.9657863145258102, 'precision': 0.9656263013164693, 'recall': 0.9657863145258103, 'f1': 0.9656081468247953}, 'test_results_lang2': {'accuracy': 0.8827046918123275, 'precision': 0.8827506477042837, 'recall': 0.8827046918123275, 'f1': 0.8827259686022164}}, {'languages': ('english', 'german'), 'test_results_lang1': {'accuracy': 0.99640625, 'precision': 0.9964084474183084, 'recall': 0.99640625, 'f1': 0.9964061559788283}, 'test_results_lang2': {'accuracy': 0.9831412103746399, 'precision': 0.9832173106571211, 'recall': 0.9831412103746398, 'f1': 0.9831753490354843}}, {'languages': ('english', 'indonesian'), 'test_results_lang1': {'accuracy': 0.99609375, 'precision': 0.9960942698105051, 'recall': 0.99609375, 'f1': 0.9960937909330638}, 'test_results_lang2': {'accuracy': 0.825, 'precision': 0.8243328420077313, 'recall': 0.825, 'f1': 0.8245506920089994}}, {'languages': ('english', 'romanian'), 'test_results_lang1': {'accuracy': 0.99609375, 'precision': 0.9960963199103544, 'recall': 0.99609375, 'f1': 0.9960938428363022}, 'test_results_lang2': {'accuracy': 0.889604415823367, 'precision': 0.8903745024663664, 'recall': 0.889604415823367, 'f1': 0.8897953387316286}}, {'languages': ('german', 'indonesian'), 'test_results_lang1': {'accuracy': 0.9827809798270893, 'precision': 0.9829656716572975, 'recall': 0.9827809798270893, 'f1': 0.9828554239220341}, 'test_results_lang2': {'accuracy': 0.8146666666666667, 'precision': 0.8148799606014436, 'recall': 0.8146666666666667, 'f1': 0.8147674041535171}}, {'languages': ('german', 'romanian'), 'test_results_lang1': {'accuracy': 0.9834293948126802, 'precision': 0.9835384201415479, 'recall': 0.9834293948126801, 'f1': 0.9834762576861111}, 'test_results_lang2': {'accuracy': 0.890984360625575, 'precision': 0.8908781807185957, 'recall': 0.890984360625575, 'f1': 0.890908107853356}}, {'languages': ('indonesian', 'romanian'), 'test_results_lang1': {'accuracy': 0.8163333333333334, 'precision': 0.8164634405516683, 'recall': 0.8163333333333334, 'f1': 0.8163960720490401}, 'test_results_lang2': {'accuracy': 0.8845446182152714, 'precision': 0.8844068408200297, 'recall': 0.8845446182152714, 'f1': 0.884432944429089}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-cloud-storage\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbYcXD-iJXQX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726730309755,
          "user_tz": -180,
          "elapsed": 4044,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "768f187a-7e1f-49ad-e991-d328ba41864f"
      },
      "id": "xbYcXD-iJXQX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.27.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.24.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.8.30)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load and clean dataset functions\n",
        "def load_data(filepath):\n",
        "    if filepath.endswith('.csv'):\n",
        "        return pd.read_csv(filepath)\n",
        "    elif filepath.endswith('.xlsx'):\n",
        "        return pd.read_excel(filepath)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {filepath}\")\n",
        "\n",
        "def clean_dataset(df):\n",
        "    if 'title' not in df.columns or 'label' not in df.columns:\n",
        "        raise ValueError(\"The dataframe must have 'title' and 'label' columns.\")\n",
        "    df = df.dropna(subset=['title', 'label'])  # Ensure no NaNs in title or label\n",
        "    df = df[df['title'].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]  # Ensure non-empty strings\n",
        "    df = df[df['label'].apply(lambda x: isinstance(x, (int, np.integer)) or str(x).isdigit())]  # Ensure valid labels\n",
        "\n",
        "    df['title'] = df['title'].astype(str)\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    return df\n",
        "\n",
        "# Dataset wrapper to handle input data\n",
        "class DatasetWrapper(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def create_data_loader(texts, labels, tokenizer, max_len, batch_size):\n",
        "    encoding = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    dataset = DatasetWrapper(encoding, labels)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, dataset_len):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for data in data_loader:\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_mask'].to(device)\n",
        "        labels = data['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / dataset_len, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, dataset_len):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            input_ids = data['input_ids'].to(device)\n",
        "            attention_mask = data['attention_mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    return correct_predictions.double() / dataset_len, np.mean(losses), all_labels, all_preds\n",
        "\n",
        "# Main function to train on all data (split Hebrew) and test on Hebrew\n",
        "def main():\n",
        "    RANDOM_SEED = 42\n",
        "    MAX_LEN = 128\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 3\n",
        "    TEST_SPLIT = 0.2  # Split ratio for the Hebrew dataset\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    languages = ['hebrew', 'arabic', 'bangla', 'chinese', 'english', 'german', 'indonesian', 'romanian', 'turkish']\n",
        "    file_paths = {\n",
        "        'hebrew': '/hebrew.xlsx',\n",
        "        'arabic': '/arabic.xlsx',\n",
        "        'bangla': '/bangla.csv',\n",
        "        'chinese': '/chinese.csv',\n",
        "        'english': '/english.csv',\n",
        "        'german': '/german.csv',\n",
        "        'indonesian': '/indonesian.csv',\n",
        "        'romanian': '/romanianL.xlsx',\n",
        "        'turkish': '/turkish.csv'\n",
        "    }\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "    # Load and clean the Hebrew dataset, then split it\n",
        "    df_hebrew = load_data(file_paths['hebrew'])\n",
        "    df_hebrew = clean_dataset(df_hebrew)\n",
        "    df_train_hebrew, df_test_hebrew = train_test_split(df_hebrew, test_size=TEST_SPLIT, random_state=RANDOM_SEED)\n",
        "\n",
        "    # Combine all non-Hebrew datasets for training\n",
        "    train_dfs = [df_train_hebrew]  # Start with Hebrew train split\n",
        "    for lang in languages:\n",
        "        if lang != 'hebrew':\n",
        "            df = load_data(file_paths[lang])\n",
        "            df = clean_dataset(df)\n",
        "            train_dfs.append(df)\n",
        "\n",
        "    # Combine all training data\n",
        "    df_train_combined = pd.concat(train_dfs, ignore_index=True)\n",
        "\n",
        "    # Create DataLoaders for training and testing\n",
        "    train_data_loader = create_data_loader(\n",
        "        df_train_combined['title'].tolist(),\n",
        "        df_train_combined['label'].tolist(),\n",
        "        tokenizer,\n",
        "        MAX_LEN,\n",
        "        BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    test_data_loader_hebrew = create_data_loader(\n",
        "        df_test_hebrew['title'].tolist(),\n",
        "        df_test_hebrew['label'].tolist(),\n",
        "        tokenizer,\n",
        "        MAX_LEN,\n",
        "        BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # Initialize the model\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    total_steps = len(train_data_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'Starting epoch {epoch + 1}/{EPOCHS}')\n",
        "        train_acc, train_loss = train_epoch(\n",
        "            model,\n",
        "            train_data_loader,\n",
        "            loss_fn,\n",
        "            optimizer,\n",
        "            device,\n",
        "            scheduler,\n",
        "            len(df_train_combined)\n",
        "        )\n",
        "        print(f'Train loss {train_loss}, accuracy {train_acc}')\n",
        "\n",
        "    # Evaluate on the Hebrew test set\n",
        "    print(f'Evaluating on Hebrew test set...')\n",
        "    test_acc, test_loss, labels, preds = eval_model(\n",
        "        model,\n",
        "        test_data_loader_hebrew,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(df_test_hebrew)\n",
        "    )\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "\n",
        "    print(f'Test Accuracy: {test_acc}')\n",
        "    print(f'Test Precision: {precision}')\n",
        "    print(f'Test Recall: {recall}')\n",
        "    print(f'Test F1: {f1}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RBvk9JWqW0Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726754529511,
          "user_tz": -180,
          "elapsed": 2646930,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "84521d52-af98-4de9-d6b3-d1ea546421e7"
      },
      "id": "RBvk9JWqW0Qk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-3262acde9b8e>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.1777877322611112, accuracy 0.9228796550955177\n",
            "Starting epoch 2/3\n",
            "Train loss 0.11068957795370421, accuracy 0.9549914517208058\n",
            "Starting epoch 3/3\n",
            "Train loss 0.0742959509393722, accuracy 0.9696845808865433\n",
            "Evaluating on Hebrew test set...\n",
            "Test Accuracy: 0.8341232227488152\n",
            "Test Precision: 0.8341606945917059\n",
            "Test Recall: 0.8341232227488151\n",
            "Test F1: 0.8341232227488151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from google.cloud import storage\n",
        "\n",
        "# Function to load and clean dataset\n",
        "def load_data(filepath):\n",
        "    if filepath.endswith('.csv'):\n",
        "        return pd.read_csv(filepath)\n",
        "    elif filepath.endswith('.xlsx'):\n",
        "        return pd.read_excel(filepath)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {filepath}\")\n",
        "\n",
        "def clean_dataset(df):\n",
        "    if 'title' not in df.columns or 'label' not in df.columns:\n",
        "        raise ValueError(\"The dataframe must have 'title' and 'label' columns.\")\n",
        "    df = df.dropna(subset=['title', 'label'])  # Ensure no NaNs in title or label\n",
        "    df = df[df['title'].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]  # Ensure non-empty strings\n",
        "    df = df[df['label'].apply(lambda x: isinstance(x, (int, np.integer)) or str(x).isdigit())]  # Ensure valid labels\n",
        "\n",
        "    df['title'] = df['title'].astype(str)\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    return df\n",
        "\n",
        "# Dataset wrapper to handle input data\n",
        "class DatasetWrapper(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def create_data_loader(texts, labels, tokenizer, max_len, batch_size):\n",
        "    encoding = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    dataset = DatasetWrapper(encoding, labels)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, dataset_len):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for data in data_loader:\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_mask'].to(device)\n",
        "        labels = data['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / dataset_len, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, dataset_len):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            input_ids = data['input_ids'].to(device)\n",
        "            attention_mask = data['attention_mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    return correct_predictions.double() / dataset_len, np.mean(losses), all_labels, all_preds\n",
        "\n",
        "# Save model to Google Cloud Storage (GCS)\n",
        "#def save_model_to_gcs(model, bucket_name, model_filename):\n",
        " #   local_model_path = f\"/tmp/{model_filename}\"\n",
        " #   torch.save(model.state_dict(), local_model_path)\n",
        "\n",
        "  #  client = storage.Client()\n",
        "  #  bucket = client.get_bucket(bucket_name)\n",
        "  #  blob = bucket.blob(model_filename)\n",
        "  #  blob.upload_from_filename(local_model_path)\n",
        "  #  print(f\"Model saved to GCS at gs://{bucket_name}/{model_filename}\")\n",
        "\n",
        "# Main function to train on 8 languages and test on Hebrew, save models\n",
        "def main():\n",
        "    RANDOM_SEED = 42\n",
        "    MAX_LEN = 128\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 3\n",
        "    TEST_SPLIT = 0.2\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "   # bucket_name = \"your-bucket-name\"  # Replace with your Google Cloud Storage bucket name\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    languages = ['hebrew', 'arabic', 'bangla', 'chinese', 'english', 'german', 'indonesian', 'romanian', 'turkish']\n",
        "    file_paths = {\n",
        "        'hebrew': '/hebrew.xlsx',\n",
        "        'arabic': '/arabic.xlsx',\n",
        "        'bangla': '/bangla.csv',\n",
        "        'chinese': '/chinese.csv',\n",
        "        'english': '/english.csv',\n",
        "        'german': '/german.csv',\n",
        "        'indonesian': '/indonesian.csv',\n",
        "        'romanian': '/romanianL.xlsx',\n",
        "        'turkish': '/turkish.csv'\n",
        "    }\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "    # Load and clean the Hebrew dataset (for testing)\n",
        "    df_hebrew = load_data(file_paths['hebrew'])\n",
        "    df_hebrew = clean_dataset(df_hebrew)\n",
        "\n",
        "    # Split Hebrew data into training and testing\n",
        "    df_train_hebrew, df_test_hebrew = train_test_split(df_hebrew, test_size=TEST_SPLIT, random_state=RANDOM_SEED)\n",
        "\n",
        "    # Loop to train 8 times, excluding one language each time\n",
        "    for excluded_lang in languages:\n",
        "        if excluded_lang == 'hebrew':\n",
        "            continue  # We are not excluding Hebrew, it's used for testing\n",
        "\n",
        "        print(f\"Training model excluding: {excluded_lang}\")\n",
        "\n",
        "        # Combine datasets from all languages except the excluded one\n",
        "        train_dfs = [df_train_hebrew]\n",
        "        for lang in languages:\n",
        "            if lang != excluded_lang and lang != 'hebrew':\n",
        "                df = load_data(file_paths[lang])\n",
        "                df = clean_dataset(df)\n",
        "                train_dfs.append(df)\n",
        "\n",
        "        # Combine all training data\n",
        "        df_train_combined = pd.concat(train_dfs, ignore_index=True)\n",
        "\n",
        "        # Create DataLoader for training and testing\n",
        "        train_data_loader = create_data_loader(\n",
        "            df_train_combined['title'].tolist(),\n",
        "            df_train_combined['label'].tolist(),\n",
        "            tokenizer,\n",
        "            MAX_LEN,\n",
        "            BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        test_data_loader_hebrew = create_data_loader(\n",
        "            df_test_hebrew['title'].tolist(),\n",
        "            df_test_hebrew['label'].tolist(),\n",
        "            tokenizer,\n",
        "            MAX_LEN,\n",
        "            BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        # Initialize the model\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
        "        model = model.to(device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "        total_steps = len(train_data_loader) * EPOCHS\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(EPOCHS):\n",
        "            print(f'Starting epoch {epoch + 1}/{EPOCHS}')\n",
        "            train_acc, train_loss = train_epoch(\n",
        "                model,\n",
        "                train_data_loader,\n",
        "                loss_fn,\n",
        "                optimizer,\n",
        "                device,\n",
        "                scheduler,\n",
        "                len(df_train_combined)\n",
        "            )\n",
        "            print(f'Train loss {train_loss}, accuracy {train_acc}')\n",
        "\n",
        "        # Evaluate on the Hebrew test set\n",
        "        print(f'Evaluating on Hebrew test set...')\n",
        "        test_acc, test_loss, labels, preds = eval_model(\n",
        "            model,\n",
        "            test_data_loader_hebrew,\n",
        "            loss_fn,\n",
        "            device,\n",
        "            len(df_test_hebrew)\n",
        "        )\n",
        "        precision = precision_score(labels, preds, average='weighted')\n",
        "        recall = recall_score(labels, preds, average='weighted')\n",
        "        f1 = f1_score(labels, preds, average='weighted')\n",
        "\n",
        "        print(f'Test Accuracy: {test_acc}')\n",
        "        print(f'Test Precision: {precision}')\n",
        "        print(f'Test Recall: {recall}')\n",
        "        print(f'Test F1: {f1}')\n",
        "\n",
        "        # Save the trained model to GCS\n",
        "        model_filename = f\"bert_model_excluding_{excluded_lang}.pth\"\n",
        "       # save_model_to_gcs(model, bucket_name, model_filename)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wqP3uDrp7ka",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726776816604,
          "user_tz": -180,
          "elapsed": 603623,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "a81b896d-b36b-4357-a0e6-1f1dcbd6ea68"
      },
      "id": "8wqP3uDrp7ka",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model excluding: arabic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-3-682e2fac421a>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n",
            "Train loss 0.14669799948655596, accuracy 0.938279301745636\n",
            "Starting epoch 2/3\n",
            "Train loss 0.0885182198068112, accuracy 0.9638266969937793\n",
            "Starting epoch 3/3\n",
            "Train loss 0.05245648598935557, accuracy 0.9795086459674989\n",
            "Evaluating on Hebrew test set...\n",
            "Test Accuracy: 0.8578199052132702\n",
            "Test Precision: 0.857961301665071\n",
            "Test Recall: 0.8578199052132701\n",
            "Test F1: 0.8578135179673229\n",
            "Training model excluding: bangla\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-3-682e2fac421a>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n",
            "Train loss 0.1691623266761403, accuracy 0.9274297732648489\n",
            "Starting epoch 2/3\n",
            "Train loss 0.10746378200727025, accuracy 0.9569746178954135\n",
            "Starting epoch 3/3\n",
            "Train loss 0.0723411330467237, accuracy 0.9708233737541425\n",
            "Evaluating on Hebrew test set...\n",
            "Test Accuracy: 0.8388625592417063\n",
            "Test Precision: 0.8393792057455183\n",
            "Test Recall: 0.8388625592417062\n",
            "Test F1: 0.8388191181631409\n",
            "Training model excluding: chinese\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-3-682e2fac421a>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n",
            "Train loss 0.17347462150120413, accuracy 0.9254316894598701\n",
            "Starting epoch 2/3\n",
            "Train loss 0.11372298131517568, accuracy 0.9529147672330432\n",
            "Starting epoch 3/3\n",
            "Train loss 0.07957243802399701, accuracy 0.9676267440254178\n",
            "Evaluating on Hebrew test set...\n",
            "Test Accuracy: 0.8483412322274883\n",
            "Test Precision: 0.8495119764314076\n",
            "Test Recall: 0.8483412322274881\n",
            "Test F1: 0.8482389811704677\n",
            "Training model excluding: english\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-3-682e2fac421a>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n",
            "Train loss 0.20743749050166785, accuracy 0.9104885812293334\n",
            "Starting epoch 2/3\n",
            "Train loss 0.13565948560541957, accuracy 0.9445594734077073\n",
            "Starting epoch 3/3\n",
            "Train loss 0.09321078142880164, accuracy 0.9627769708581847\n",
            "Evaluating on Hebrew test set...\n",
            "Test Accuracy: 0.8530805687203792\n",
            "Test Precision: 0.8568916881914764\n",
            "Test Recall: 0.8530805687203792\n",
            "Test F1: 0.8526435548409166\n",
            "Training model excluding: german\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-3-682e2fac421a>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n",
            "Train loss 0.2387304182154496, accuracy 0.8939048239895697\n",
            "Starting epoch 2/3\n",
            "Train loss 0.16195849437323573, accuracy 0.9311495002172968\n",
            "Starting epoch 3/3\n",
            "Train loss 0.11564773336278845, accuracy 0.9529009126466754\n",
            "Evaluating on Hebrew test set...\n",
            "Test Accuracy: 0.8246445497630333\n",
            "Test Precision: 0.8248836905952432\n",
            "Test Recall: 0.8246445497630331\n",
            "Test F1: 0.8245972682125817\n",
            "Training model excluding: indonesian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-3-682e2fac421a>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n",
            "Train loss 0.14218489176923493, accuracy 0.941503455434456\n",
            "Starting epoch 2/3\n",
            "Train loss 0.08237388510862123, accuracy 0.9672143461990221\n",
            "Starting epoch 3/3\n",
            "Train loss 0.05303919568060689, accuracy 0.9786118167663689\n",
            "Evaluating on Hebrew test set...\n",
            "Test Accuracy: 0.8341232227488152\n",
            "Test Precision: 0.8356521374886304\n",
            "Test Recall: 0.8341232227488151\n",
            "Test F1: 0.8339666121015284\n",
            "Training model excluding: romanian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-3-682e2fac421a>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n",
            "Train loss 0.15940602812644888, accuracy 0.9321440668397877\n",
            "Starting epoch 2/3\n",
            "Train loss 0.1008417455350309, accuracy 0.9586701113775079\n",
            "Starting epoch 3/3\n",
            "Train loss 0.06872377068983439, accuracy 0.9720393972198792\n",
            "Evaluating on Hebrew test set...\n",
            "Test Accuracy: 0.8246445497630333\n",
            "Test Precision: 0.8249308527226901\n",
            "Test Recall: 0.8246445497630331\n",
            "Test F1: 0.8246209153604135\n",
            "Training model excluding: turkish\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-3-682e2fac421a>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n",
            "Train loss 0.17953420400624007, accuracy 0.9224026858941005\n",
            "Starting epoch 2/3\n",
            "Train loss 0.11074631710332157, accuracy 0.9543967888203375\n",
            "Starting epoch 3/3\n",
            "Train loss 0.07474985972121385, accuracy 0.9698580242325132\n",
            "Evaluating on Hebrew test set...\n",
            "Test Accuracy: 0.8436018957345972\n",
            "Test Precision: 0.8444102704109273\n",
            "Test Recall: 0.8436018957345972\n",
            "Test F1: 0.843531612348791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#all langs except arabic + predictions\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Function to load and clean dataset\n",
        "def load_data(filepath):\n",
        "    if filepath.endswith('.csv'):\n",
        "        return pd.read_csv(filepath)\n",
        "    elif filepath.endswith('.xlsx'):\n",
        "        return pd.read_excel(filepath)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {filepath}\")\n",
        "\n",
        "def clean_dataset(df):\n",
        "    if 'title' not in df.columns or 'label' not in df.columns:\n",
        "        raise ValueError(\"The dataframe must have 'title' and 'label' columns.\")\n",
        "    df = df.dropna(subset=['title', 'label'])  # Ensure no NaNs in title or label\n",
        "    df = df[df['title'].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]  # Ensure non-empty strings\n",
        "    df = df[df['label'].apply(lambda x: isinstance(x, (int, np.integer)) or str(x).isdigit())]  # Ensure valid labels\n",
        "\n",
        "    df['title'] = df['title'].astype(str)\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    return df\n",
        "\n",
        "# Dataset wrapper to handle input data\n",
        "class DatasetWrapper(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def create_data_loader(texts, labels, tokenizer, max_len, batch_size):\n",
        "    encoding = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    dataset = DatasetWrapper(encoding, labels)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, dataset_len):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for data in data_loader:\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_mask'].to(device)\n",
        "        labels = data['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / dataset_len, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, dataset_len):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            input_ids = data['input_ids'].to(device)\n",
        "            attention_mask = data['attention_mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    return correct_predictions.double() / dataset_len, np.mean(losses), all_labels, all_preds\n",
        "\n",
        "# Main function to train on all languages except Arabic and test on Hebrew\n",
        "def main():\n",
        "    RANDOM_SEED = 42\n",
        "    MAX_LEN = 128\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 3\n",
        "    TEST_SPLIT = 0.2\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    languages = ['hebrew', 'arabic', 'bangla', 'chinese', 'english', 'german', 'indonesian', 'romanian', 'turkish']\n",
        "    file_paths = {\n",
        "        'hebrew': '/hebrew.xlsx',\n",
        "        'arabic': '/arabic.xlsx',\n",
        "        'bangla': '/bangla.csv',\n",
        "        'chinese': '/chinese.csv',\n",
        "        'english': '/english.csv',\n",
        "        'german': '/german.csv',\n",
        "        'indonesian': '/indonesian.csv',\n",
        "        'romanian': '/romanianL.xlsx',\n",
        "        'turkish': '/turkish.csv'\n",
        "    }\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "    # Load and clean the Hebrew dataset (for testing)\n",
        "    df_hebrew = load_data(file_paths['hebrew'])\n",
        "    df_hebrew = clean_dataset(df_hebrew)\n",
        "\n",
        "    # Split Hebrew data into training and testing\n",
        "    df_train_hebrew, df_test_hebrew = train_test_split(df_hebrew, test_size=TEST_SPLIT, random_state=RANDOM_SEED)\n",
        "\n",
        "    # Combine datasets from all languages except Arabic\n",
        "    train_dfs = [df_train_hebrew]  # Hebrew is included in the training set\n",
        "    for lang in languages:\n",
        "        if lang != 'arabic' and lang != 'hebrew':\n",
        "            df = load_data(file_paths[lang])\n",
        "            df = clean_dataset(df)\n",
        "            train_dfs.append(df)\n",
        "\n",
        "    # Combine all training data\n",
        "    df_train_combined = pd.concat(train_dfs, ignore_index=True)\n",
        "\n",
        "    # Create DataLoader for training and testing\n",
        "    train_data_loader = create_data_loader(\n",
        "        df_train_combined['title'].tolist(),\n",
        "        df_train_combined['label'].tolist(),\n",
        "        tokenizer,\n",
        "        MAX_LEN,\n",
        "        BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    test_data_loader_hebrew = create_data_loader(\n",
        "        df_test_hebrew['title'].tolist(),\n",
        "        df_test_hebrew['label'].tolist(),\n",
        "        tokenizer,\n",
        "        MAX_LEN,\n",
        "        BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # Initialize the model\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    total_steps = len(train_data_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'Starting epoch {epoch + 1}/{EPOCHS}')\n",
        "        train_acc, train_loss = train_epoch(\n",
        "            model,\n",
        "            train_data_loader,\n",
        "            loss_fn,\n",
        "            optimizer,\n",
        "            device,\n",
        "            scheduler,\n",
        "            len(df_train_combined)\n",
        "        )\n",
        "        print(f'Train loss {train_loss}, accuracy {train_acc}')\n",
        "\n",
        "    # Evaluate on the Hebrew test set\n",
        "    print(f'Evaluating on Hebrew test set...')\n",
        "    test_acc, test_loss, labels, preds = eval_model(\n",
        "        model,\n",
        "        test_data_loader_hebrew,\n",
        "        loss_fn,\n",
        "        device,\n",
        "        len(df_test_hebrew)\n",
        "    )\n",
        "\n",
        "    # Modify to use the correct order of labels and predictions in the batch\n",
        "    for i in range(len(labels)):\n",
        "        title = df_test_hebrew['title'].iloc[i]  # This indexing should be reconsidered\n",
        "        true_label = labels[i]  # True labels from evaluation\n",
        "        predicted_label = preds[i]  # Predicted labels from evaluation\n",
        "        correctness = \"טוב\" if true_label == predicted_label else \"רע\"\n",
        "        print(title)\n",
        "     #if true_label != predicted_label:\n",
        "        print(f\"Title: {title}\")\n",
        "        print(f\"True label: {true_label}, Predicted label: {predicted_label}, {correctness}\")\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "\n",
        "    print(f'Test Accuracy: {test_acc}')\n",
        "    print(f'Test Precision: {precision}')\n",
        "    print(f'Test Recall: {recall}')\n",
        "    print(f'Test F1: {f1}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "habkOEjSocUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1727133069027,
          "user_tz": -180,
          "elapsed": 2381771,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "abb5b589-a6eb-49eb-9132-69675fde4fc5"
      },
      "id": "habkOEjSocUf",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-7436542dc634>:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.14683798568623488, accuracy 0.9385944479460689\n",
            "Starting epoch 2/3\n",
            "Train loss 0.08883747794400945, accuracy 0.964045929133211\n",
            "Starting epoch 3/3\n",
            "Train loss 0.053359373515291394, accuracy 0.9786865254446302\n",
            "Evaluating on Hebrew test set...\n",
            " אל תשקיעו במערכת חימום גדולה עד שתראו את ההמצאה המהפכנית הזו...\n",
            "Title:  אל תשקיעו במערכת חימום גדולה עד שתראו את ההמצאה המהפכנית הזו...\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " ארגז כלים לטיפול בכאבי גב תחתון באופן עצמאי\n",
            "Title:  ארגז כלים לטיפול בכאבי גב תחתון באופן עצמאי\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "  עינב בובליל: \"זה המוצר שגמל אותי ממתוקים\"\n",
            "Title:   עינב בובליל: \"זה המוצר שגמל אותי ממתוקים\"\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "אלעל פתחה עמדות צ'ק אין עצמי בנתב\"ג\n",
            "Title: אלעל פתחה עמדות צ'ק אין עצמי בנתב\"ג\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "הטעות של בני 55+\n",
            "Title: הטעות של בני 55+\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            " מיכאל בן דוד: 'כעסתי על אמא שלי שלא הגיעה לחתונה שלי'\n",
            "Title:  מיכאל בן דוד: 'כעסתי על אמא שלי שלא הגיעה לחתונה שלי'\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "מעמד ההתנחלויות ישתנה אחרי הכרזת ארה\"ב? \n",
            "Title: מעמד ההתנחלויות ישתנה אחרי הכרזת ארה\"ב? \n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "אמא, אפשר גם ליהנות מחופשת הלידה וככה תוכלי לעשות את זה\n",
            "Title: אמא, אפשר גם ליהנות מחופשת הלידה וככה תוכלי לעשות את זה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " מי הם המרוויחים שיקדימו להכפיל את שווי הקרקע?\n",
            "Title:  מי הם המרוויחים שיקדימו להכפיל את שווי הקרקע?\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " \"נתראה בשישי\": זוהי הפאנליסטית החדשה של \"אולפן שישי\"\n",
            "Title:  \"נתראה בשישי\": זוהי הפאנליסטית החדשה של \"אולפן שישי\"\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " מי במקום הראשון? אלה 10 הספרים הנמכרים ביותר בכל הזמנים\n",
            "Title:  מי במקום הראשון? אלה 10 הספרים הנמכרים ביותר בכל הזמנים\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " מבלי לשבור את החסכון - הסרת שיער בלייזר במחיר זול במיוחד\n",
            "Title:  מבלי לשבור את החסכון - הסרת שיער בלייזר במחיר זול במיוחד\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " יואב קיש: \"בכנות, הממשלה עוד לא עשתה מספיק\"\n",
            "Title:  יואב קיש: \"בכנות, הממשלה עוד לא עשתה מספיק\"\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " גלה את הדרך הטובה ביותר להקל על הכאב ולשפר את זרימת הדם בכפות הרגליים\n",
            "Title:  גלה את הדרך הטובה ביותר להקל על הכאב ולשפר את זרימת הדם בכפות הרגליים\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            " לרגל עליית המופע \"אוסקר\" בכיכובו, אלי גורנשטיין חוזר לסצנת הלבן המפורסמת\n",
            "Title:  לרגל עליית המופע \"אוסקר\" בכיכובו, אלי גורנשטיין חוזר לסצנת הלבן המפורסמת\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " כבר לא נתון אדיר: מי נגס ברייטינג של ‏אדיר מילר בקשת?\n",
            "Title:  כבר לא נתון אדיר: מי נגס ברייטינג של ‏אדיר מילר בקשת?\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "שקד נועדה עם אורבך וקארה - סיכמו לפעול יחד\n",
            "Title: שקד נועדה עם אורבך וקארה - סיכמו לפעול יחד\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "יוצאי סיירת מטכ\"ל במחאה נגד הרפורמה המשפטית\n",
            "Title: יוצאי סיירת מטכ\"ל במחאה נגד הרפורמה המשפטית\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "ועדת הבחירות החלה לגייס מזכירי קלפיות\n",
            "Title: ועדת הבחירות החלה לגייס מזכירי קלפיות\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " המולטיטול הגאוני הזה הוא הגדג'ט המגניב ביותר של 2021\n",
            "Title:  המולטיטול הגאוני הזה הוא הגדג'ט המגניב ביותר של 2021\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " בדיקה השוואה וניתוח לתשואות קופות הגמל וקרנות ההשתלמות!\n",
            "Title:  בדיקה השוואה וניתוח לתשואות קופות הגמל וקרנות ההשתלמות!\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " האישה בזהב: כמויות אסטרונומיות של תכשיטים נתפסו אצל נוסעת בנתב\"ג\n",
            "Title:  האישה בזהב: כמויות אסטרונומיות של תכשיטים נתפסו אצל נוסעת בנתב\"ג\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " דיפלומט לשעבר מעריך: \"ללא המתיחות בין ביידן לנתניהו - הפטור מוויזה היה מאחורינו\"\n",
            "Title:  דיפלומט לשעבר מעריך: \"ללא המתיחות בין ביידן לנתניהו - הפטור מוויזה היה מאחורינו\"\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "המשטרה הקימה גדר כפולה לקראת ההפגנות מחר\n",
            "Title: המשטרה הקימה גדר כפולה לקראת ההפגנות מחר\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "מסתמן: רע\"ם לא תפרוש מהקואליציה\n",
            "Title: מסתמן: רע\"ם לא תפרוש מהקואליציה\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "קרב על תקציב י-ם: מה האוצר דורש?\n",
            "Title: קרב על תקציב י-ם: מה האוצר דורש?\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " אלבומי סוף שנה-מזכרת לשנה שהייתה\n",
            "Title:  אלבומי סוף שנה-מזכרת לשנה שהייתה\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " חופשת סקי מפנקת בחורף 2024 עד 15% הנחה\n",
            "Title:  חופשת סקי מפנקת בחורף 2024 עד 15% הנחה\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " הרב רועי מזרחי - היום שבו התחיל הכול\n",
            "Title:  הרב רועי מזרחי - היום שבו התחיל הכול\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "הישראלי שאותר בירדן לאחר שנתיים מדבר לראשונה\n",
            "Title: הישראלי שאותר בירדן לאחר שנתיים מדבר לראשונה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " ראול סרוגו: ״בחצי השנה הראשונה הממשלה לא התייחסה לסוגיות החשובות ביותר לאזרחים״\n",
            "Title:  ראול סרוגו: ״בחצי השנה הראשונה הממשלה לא התייחסה לסוגיות החשובות ביותר לאזרחים״\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            " צלם העיתונות נינו הרמן חוזר לצילומים המפורסמים שלו לאורך השנים\n",
            "Title:  צלם העיתונות נינו הרמן חוזר לצילומים המפורסמים שלו לאורך השנים\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " אם הקירות יכלו לדבר: אלה הפריטים שמצאה המשטרה בדירה בהרצליה\n",
            "Title:  אם הקירות יכלו לדבר: אלה הפריטים שמצאה המשטרה בדירה בהרצליה\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "הרגע שבו הרב אדלשטיין הפתיע את כולם\n",
            "Title: הרגע שבו הרב אדלשטיין הפתיע את כולם\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " וואטסאפ פוגע בעסק שלך! נתונים חדשים מעוררי דאגה נחשפו.\n",
            "Title:  וואטסאפ פוגע בעסק שלך! נתונים חדשים מעוררי דאגה נחשפו.\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "\"גוווווול סניור!\": השער הישראלי בעולם\n",
            "Title: \"גוווווול סניור!\": השער הישראלי בעולם\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " רכב התנגש בשערי המעון הרשמי של ראש הממשלה הבריטית, חשוד נעצר\n",
            "Title:  רכב התנגש בשערי המעון הרשמי של ראש הממשלה הבריטית, חשוד נעצר\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " לקראת השתתפותה בפסטיבל השירה במטולה: ריאיון עם המשוררת ענת קוריאל\n",
            "Title:  לקראת השתתפותה בפסטיבל השירה במטולה: ריאיון עם המשוררת ענת קוריאל\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " תאונת צלילה באילת: גבר נמשה מהמים ללא רוח חיים\n",
            "Title:  תאונת צלילה באילת: גבר נמשה מהמים ללא רוח חיים\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            "במערכת הפוליטית עובדים על חוק לבחירה ישירה\n",
            "Title: במערכת הפוליטית עובדים על חוק לבחירה ישירה\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " לא תאמינו מה הוא מסוגל לעשות\n",
            "Title:  לא תאמינו מה הוא מסוגל לעשות\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            "משבר הגיוס: תנאי נתניהו לפתרון\n",
            "Title: משבר הגיוס: תנאי נתניהו לפתרון\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "גם אם גנץ ייכשל: כך אפשר להקים ממשלת מיעוט\n",
            "Title: גם אם גנץ ייכשל: כך אפשר להקים ממשלת מיעוט\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " עקבו אחרי אדם מסוים? פרטים חדשים על האסון בצפון איטליה\n",
            "Title:  עקבו אחרי אדם מסוים? פרטים חדשים על האסון בצפון איטליה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " מתעוררים כואבים, נוקשים ועייפים? אתם חייבים לקראו את זה\n",
            "Title:  מתעוררים כואבים, נוקשים ועייפים? אתם חייבים לקראו את זה\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            " האוצר: חסכתם? כך תמנעו מהפסדים בקרנות ההשתלמות והגמל שלכם\n",
            "Title:  האוצר: חסכתם? כך תמנעו מהפסדים בקרנות ההשתלמות והגמל שלכם\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " בצל המתיחות מול איראן - דיווח בטהרן: \"נעצרה חוליית טרור המקושרת לישראל\"\n",
            "Title:  בצל המתיחות מול איראן - דיווח בטהרן: \"נעצרה חוליית טרור המקושרת לישראל\"\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "ניצחון גדול לנתניהו על סער בפריימריז בליכוד\n",
            "Title: ניצחון גדול לנתניהו על סער בפריימריז בליכוד\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "גולדקנופף דורש מנתניהו להצביע על ההתגברות\n",
            "Title: גולדקנופף דורש מנתניהו להצביע על ההתגברות\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            " חריימה בסגנון מרוקאי של השף אביב משה\n",
            "Title:  חריימה בסגנון מרוקאי של השף אביב משה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " גלנט במסר לאיראן: \"עלולים להידרש לממש את חובתנו על מנת להגן על המדינה\"\n",
            "Title:  גלנט במסר לאיראן: \"עלולים להידרש לממש את חובתנו על מנת להגן על המדינה\"\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " אל תיתפס לא מוכן - מקסימום הגנה מפני שריפות\n",
            "Title:  אל תיתפס לא מוכן - מקסימום הגנה מפני שריפות\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " כיצד הגוף מרפא את עצמו?\n",
            "Title:  כיצד הגוף מרפא את עצמו?\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " הישראלים בשוק: המכשיר הזה מוריד בטירוף את צריכת הדלק (נסו אותו היום)\n",
            "Title:  הישראלים בשוק: המכשיר הזה מוריד בטירוף את צריכת הדלק (נסו אותו היום)\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " \"יש חשמל באוויר\": סוף סוף תוכלו להוזיל את חשבון החשמל שלכם\n",
            "Title:  \"יש חשמל באוויר\": סוף סוף תוכלו להוזיל את חשבון החשמל שלכם\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "הושגה הסכמה: הפריימריז בליכוד בעוד כשבועיים\n",
            "Title: הושגה הסכמה: הפריימריז בליכוד בעוד כשבועיים\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " לאחרונה גיליתי כיצד אפשר לקרר את המרחב בבית בתוך 5 דקות מבלי להשתמש במזגן היקר\n",
            "Title:  לאחרונה גיליתי כיצד אפשר לקרר את המרחב בבית בתוך 5 דקות מבלי להשתמש במזגן היקר\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " מדרסי הפלא: תשכחו מכל מה שידעתם על מדרסים לזה לא ציפיתם\n",
            "Title:  מדרסי הפלא: תשכחו מכל מה שידעתם על מדרסים לזה לא ציפיתם\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " עלות רכב חשמלי בליסינג ב2023 - המחיר עשוי להפתיע אותך\n",
            "Title:  עלות רכב חשמלי בליסינג ב2023 - המחיר עשוי להפתיע אותך\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " בדיקה השוואה וניתוח של תיקי השקעות מנוהלים ללא עלות בין כל בתי ההשקעות המובילים בישראל\n",
            "Title:  בדיקה השוואה וניתוח של תיקי השקעות מנוהלים ללא עלות בין כל בתי ההשקעות המובילים בישראל\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " \"צחקו על היכולות שלי, אבל תראו איפה אני\": אלונה טל במסר ל\"פיג'מות\" מהוליווד\n",
            "Title:  \"צחקו על היכולות שלי, אבל תראו איפה אני\": אלונה טל במסר ל\"פיג'מות\" מהוליווד\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "אחרי פסילתו של דרעי: תמה פגישתו עם נתניהו\n",
            "Title: אחרי פסילתו של דרעי: תמה פגישתו עם נתניהו\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " זאת הדרך הכי טובה לנקות את האוזניים שלכם\n",
            "Title:  זאת הדרך הכי טובה לנקות את האוזניים שלכם\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            "איילת שקד לא פוסלת ישיבה עם נתניהו\n",
            "Title: איילת שקד לא פוסלת ישיבה עם נתניהו\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            " רוצים להשוויץ בזרועות חטובות באופן טבעי ובלי ללכת לחד“כ?\n",
            "Title:  רוצים להשוויץ בזרועות חטובות באופן טבעי ובלי ללכת לחד“כ?\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " איך תשמרו על ההון שלכם בהעברה בין דורית?\n",
            "Title:  איך תשמרו על ההון שלכם בהעברה בין דורית?\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            " אזרחים ותיקים ועדיין נוהגים? אל תניעו את הרכב בלי לקרוא את זה\n",
            "Title:  אזרחים ותיקים ועדיין נוהגים? אל תניעו את הרכב בלי לקרוא את זה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "ח\"כ זועבי ממרצ תמונה לקונסולית בסין\n",
            "Title: ח\"כ זועבי ממרצ תמונה לקונסולית בסין\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " מצלמת האבטחה הגאונית הזאת סוף סוף הגיעה לישראל (כל בעל בית חייב שתהיה לו אחת כזאת))\n",
            "Title:  מצלמת האבטחה הגאונית הזאת סוף סוף הגיעה לישראל (כל בעל בית חייב שתהיה לו אחת כזאת))\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " שי-לי ליפא, אביב משה ושרי אנסקי: 3 מתכוני חרוסת טעימים לפסח\n",
            "Title:  שי-לי ליפא, אביב משה ושרי אנסקי: 3 מתכוני חרוסת טעימים לפסח\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            "אבטחה כבדה: מצעד הגאווה בירושלים יוצא לדרך\n",
            "Title: אבטחה כבדה: מצעד הגאווה בירושלים יוצא לדרך\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "איך לבחור את ביטוח הרכב שישתלם לכם\n",
            "Title: איך לבחור את ביטוח הרכב שישתלם לכם\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "שונאת ספורט: מכשיר הרטט שהחליף את חדרי הכושר וכל אחת צריכה\n",
            "Title: שונאת ספורט: מכשיר הרטט שהחליף את חדרי הכושר וכל אחת צריכה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " למרות ההצהרה המשותפת: ביקורת קשה באופוזיציה על הפילוג והפגנת החולשה מול הממשלה\n",
            "Title:  למרות ההצהרה המשותפת: ביקורת קשה באופוזיציה על הפילוג והפגנת החולשה מול הממשלה\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " אאורה האורן קריית אונו - לגור רמה מעל כולם! דירות 4 חדרים החל מ-₪3,020,000\n",
            "Title:  אאורה האורן קריית אונו - לגור רמה מעל כולם! דירות 4 חדרים החל מ-₪3,020,000\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " היועמ\"שית נגד תיקון חוק המשטרה: \"חשש כבד לפוליטיזציה של הכוח המשטרתי\"\n",
            "Title:  היועמ\"שית נגד תיקון חוק המשטרה: \"חשש כבד לפוליטיזציה של הכוח המשטרתי\"\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "בן גביר הודיע: עוצמה יהודית תרוץ לבחירות \n",
            "Title: בן גביר הודיע: עוצמה יהודית תרוץ לבחירות \n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "פעילת ליכוד מוכרת נעצרה: חשד שירקה על פעילים\n",
            "Title: פעילת ליכוד מוכרת נעצרה: חשד שירקה על פעילים\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "בן גביר דורש \"מדיניות שוויונית\" בהפגנות\n",
            "Title: בן גביר דורש \"מדיניות שוויונית\" בהפגנות\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "  מדענים ישראליים הוכיחו: זו הדרך היעילה ביותר\n",
            "Title:   מדענים ישראליים הוכיחו: זו הדרך היעילה ביותר\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "מה מבטיח לנו הליכודניק שרוצה להיות רה\"מ הבא?\n",
            "Title: מה מבטיח לנו הליכודניק שרוצה להיות רה\"מ הבא?\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " הדבר המבחיל שלא ידעתם שנמצא בתאנים שאתם אוכלים\n",
            "Title:  הדבר המבחיל שלא ידעתם שנמצא בתאנים שאתם אוכלים\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " מה חשוב לדעת לפני ניתוח חזה? ד\"ר עומר וולף\n",
            "Title:  מה חשוב לדעת לפני ניתוח חזה? ד\"ר עומר וולף\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "3 לאקי לוזרס בהגרלה הראשית ברולאן גארוס\n",
            "Title: 3 לאקי לוזרס בהגרלה הראשית ברולאן גארוס\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " בנוכחות כ-200 אלף איש: התקיים מסע ההלוויה של הרב גרשון אדלשטיין ז\"ל\n",
            "Title:  בנוכחות כ-200 אלף איש: התקיים מסע ההלוויה של הרב גרשון אדלשטיין ז\"ל\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " מצלמת הרכב הגאונית הזאת סוף סוף הגיעה לישראל. (כל נהג חייב אחת כזאת)\n",
            "Title:  מצלמת הרכב הגאונית הזאת סוף סוף הגיעה לישראל. (כל נהג חייב אחת כזאת)\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " מרהיב, שואב ומלא בהפתעות: זה אחד מסרטי האנימציה הכי טובים של השנים האחרונות\n",
            "Title:  מרהיב, שואב ומלא בהפתעות: זה אחד מסרטי האנימציה הכי טובים של השנים האחרונות\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            " תלונה הוגשה נגד אנטוני בגין תקיפה פיזית\n",
            "Title:  תלונה הוגשה נגד אנטוני בגין תקיפה פיזית\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            "  בוערת בעיר: שרית פולק בשמלת מחוך מטריפה \n",
            "Title:   בוערת בעיר: שרית פולק בשמלת מחוך מטריפה \n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "תצפו לחיוך מושלם: כך תבחרו את הטיפול הנכון עבור החיוך שלכם\n",
            "Title: תצפו לחיוך מושלם: כך תבחרו את הטיפול הנכון עבור החיוך שלכם\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            "גורמים בליכוד: יו\"ר הכנסת הבא יהיה אוחנה\n",
            "Title: גורמים בליכוד: יו\"ר הכנסת הבא יהיה אוחנה\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "הנשיא הרצוג האריך לנתניהו את המנדט ב-10 ימים\n",
            "Title: הנשיא הרצוג האריך לנתניהו את המנדט ב-10 ימים\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " מתקשרים עם העובדים בוואצאפ? התביעה בדרך\n",
            "Title:  מתקשרים עם העובדים בוואצאפ? התביעה בדרך\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "הסכנות הטמונות בשימוש בוואצאפ לצרכים ארגוניים.\n",
            "Title: הסכנות הטמונות בשימוש בוואצאפ לצרכים ארגוניים.\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " עם קרולינה ואסתר רדא: פסטיבל הג׳אז ירושלים יוצא לדרך בפעם ה-9\n",
            "Title:  עם קרולינה ואסתר רדא: פסטיבל הג׳אז ירושלים יוצא לדרך בפעם ה-9\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "היועמ\"ש לשעבר וינשטיין נגד המהפכה המשפטית\n",
            "Title: היועמ\"ש לשעבר וינשטיין נגד המהפכה המשפטית\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            "יותר מחצי מהאגמים הגדולים בעולם - מתייבשים\n",
            "Title: יותר מחצי מהאגמים הגדולים בעולם - מתייבשים\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "הליכוד משך את ההצעה לוועדת החקירה\n",
            "Title: הליכוד משך את ההצעה לוועדת החקירה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "ארה\"ב מנסה \"להוריד\" את הפלסטינים מגינוי באו\"ם\n",
            "Title: ארה\"ב מנסה \"להוריד\" את הפלסטינים מגינוי באו\"ם\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " למה המפורסמים האלה אוכלים ארוחה אחת ביום, ומה זה בדיוק עושה?\n",
            "Title:  למה המפורסמים האלה אוכלים ארוחה אחת ביום, ומה זה בדיוק עושה?\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " הרחפן הזול הזה הינו ההמצאה המדהימה ביותר בישראל ב-2023\n",
            "Title:  הרחפן הזול הזה הינו ההמצאה המדהימה ביותר בישראל ב-2023\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "ד\"ר טטיאנה זייגר חושפת את הסוד שעומד מאחורי טיפולים אסתטיים מוצלחים\n",
            "Title: ד\"ר טטיאנה זייגר חושפת את הסוד שעומד מאחורי טיפולים אסתטיים מוצלחים\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            "נשיא צ'אד יחנוך מחר שגרירות בישראל\n",
            "Title: נשיא צ'אד יחנוך מחר שגרירות בישראל\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " \"לא היה נעים לצפייה\": הרגע מ\"רוקדים עם כוכבים\" שהיה צריך להישאר על רצפת חדר העריכה\n",
            "Title:  \"לא היה נעים לצפייה\": הרגע מ\"רוקדים עם כוכבים\" שהיה צריך להישאר על רצפת חדר העריכה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " בפיתוח שני רופאים ישראלים: בשורה עבור הסובלים מכאבי גב ומפרקים\n",
            "Title:  בפיתוח שני רופאים ישראלים: בשורה עבור הסובלים מכאבי גב ומפרקים\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " גביע או צלחת? הטעות המביכה של השר אמסלם\n",
            "Title:  גביע או צלחת? הטעות המביכה של השר אמסלם\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            " על רקע המאבק באפי נוה: עו\"ד ברק כהן ביקש להפסיק לייצג לקוח\n",
            "Title:  על רקע המאבק באפי נוה: עו\"ד ברק כהן ביקש להפסיק לייצג לקוח\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "כך מנסים בליכוד לפרק את כחול לבן ‎\n",
            "Title: כך מנסים בליכוד לפרק את כחול לבן ‎\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " ״מסוג ההזדמנויות שהיינו מוצאים רק בשנות ה-90״\n",
            "Title:  ״מסוג ההזדמנויות שהיינו מוצאים רק בשנות ה-90״\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " \"המחסומים מגבירים את הפיגועים\"? הפרשן הצבאי של \"מעריב\" על הדילמה של כוחות הביטחון\n",
            "Title:  \"המחסומים מגבירים את הפיגועים\"? הפרשן הצבאי של \"מעריב\" על הדילמה של כוחות הביטחון\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            "רה\"מ נתניהו נפגש עם מלך ירדן בעמאן\n",
            "Title: רה\"מ נתניהו נפגש עם מלך ירדן בעמאן\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "חתונת השנה בעולם הערבי: יורש העצר הירדני התחתן \n",
            "Title: חתונת השנה בעולם הערבי: יורש העצר הירדני התחתן \n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " הזוכה בגמר הקינוח המושלם ששודר הערב בקשת 12: \"שלכת\" של בשמת ויינר\n",
            "Title:  הזוכה בגמר הקינוח המושלם ששודר הערב בקשת 12: \"שלכת\" של בשמת ויינר\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " זה השעון החכם שכל גבר בישראל חיכה לו!\n",
            "Title:  זה השעון החכם שכל גבר בישראל חיכה לו!\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            " הפשע משתולל בדרום: תושבים בעדויות קשות על מכת פריצות וגניבות שלא נגמרת\n",
            "Title:  הפשע משתולל בדרום: תושבים בעדויות קשות על מכת פריצות וגניבות שלא נגמרת\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " \"משת\"פ של דיקטטור\": מפגינים מחו מחוץ לביתו של יו\"ר פורום קהלת\n",
            "Title:  \"משת\"פ של דיקטטור\": מפגינים מחו מחוץ לביתו של יו\"ר פורום קהלת\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "  הלם: כך נראתה משתתפת \"האח הגדול\" לפני 12 שנה\n",
            "Title:   הלם: כך נראתה משתתפת \"האח הגדול\" לפני 12 שנה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " \"כתובת שלא מופנית אלינו\": אחותו של שליו אפללו ז\"ל על הגרפיטי שרוסס בביתם\n",
            "Title:  \"כתובת שלא מופנית אלינו\": אחותו של שליו אפללו ז\"ל על הגרפיטי שרוסס בביתם\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "ה\"מחליפה\" של נשיא הונדורס בטקס\n",
            "Title: ה\"מחליפה\" של נשיא הונדורס בטקס\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "נתניהו התקפל: הנהגים שהודחו יחזרו לשיירה \n",
            "Title: נתניהו התקפל: הנהגים שהודחו יחזרו לשיירה \n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " \"אין לאן לברוח\" הוא כמו תבשיל שלא עושה חשק למנה נוספת\n",
            "Title:  \"אין לאן לברוח\" הוא כמו תבשיל שלא עושה חשק למנה נוספת\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "  חדש בישראל: מכשיר לחיזוק הריאות, מנקה ליחה וריר (למצבי קוצר בנשימה ולזרימת אויר חלקה)\n",
            "Title:   חדש בישראל: מכשיר לחיזוק הריאות, מנקה ליחה וריר (למצבי קוצר בנשימה ולזרימת אויר חלקה)\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " כוחו של המיתוס: ביקרנו בשווארמיה המפורסמת בחדרה\n",
            "Title:  כוחו של המיתוס: ביקרנו בשווארמיה המפורסמת בחדרה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " איך להוריד בחשבון החשמל שלכם עד 90%\n",
            "Title:  איך להוריד בחשבון החשמל שלכם עד 90%\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "תמונת מצב: שפל באמון הציבור במוסדות המדינה\n",
            "Title: תמונת מצב: שפל באמון הציבור במוסדות המדינה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "הנריקה צימרמן הצטרף למחנה הציוני\n",
            "Title: הנריקה צימרמן הצטרף למחנה הציוני\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " צלי בשר: מתכון חגיגי לצלי עם ערמונים ותמרים\n",
            "Title:  צלי בשר: מתכון חגיגי לצלי עם ערמונים ותמרים\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " גדג'ט הכיס המצליח ביותר של 2021\n",
            "Title:  גדג'ט הכיס המצליח ביותר של 2021\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " ניצולי שואה נמצאים בסיכון גבוה למחלות לב ושבץ | בלעדי\n",
            "Title:  ניצולי שואה נמצאים בסיכון גבוה למחלות לב ושבץ | בלעדי\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " על מי אתם עובדים? 5 סימנים לכך שאתם רק נראים שמחים\n",
            "Title:  על מי אתם עובדים? 5 סימנים לכך שאתם רק נראים שמחים\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " המחלה שממנה סובלת ארונוב\n",
            "Title:  המחלה שממנה סובלת ארונוב\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " שום שפה לא “זרה” יותר עם המכשיר הגאוני הזה…\n",
            "Title:  שום שפה לא “זרה” יותר עם המכשיר הגאוני הזה…\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "\"ברזיל פייבוריטית, נגמר המזל של ישראל\"\n",
            "Title: \"ברזיל פייבוריטית, נגמר המזל של ישראל\"\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " הבנייה בעיצומה! פרויקט אאורה בן שמן מציע דירות 4-6 חד' עם נוף פתוח\n",
            "Title:  הבנייה בעיצומה! פרויקט אאורה בן שמן מציע דירות 4-6 חד' עם נוף פתוח\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "שקד צפויה להעיד על בני הזוג נתניהו\n",
            "Title: שקד צפויה להעיד על בני הזוג נתניהו\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " אז מהי השיטה לקנייית רכב חדש שמאיימת על הליסינג המסורתי?\n",
            "Title:  אז מהי השיטה לקנייית רכב חדש שמאיימת על הליסינג המסורתי?\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " סובל מערמונית מוגדלת? טכנולוגיה מאושרת FDA ללא פגיעה בתפקוד המיני\n",
            "Title:  סובל מערמונית מוגדלת? טכנולוגיה מאושרת FDA ללא פגיעה בתפקוד המיני\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " \"עם הארץ ממולדובה\": ח\"כ מש\"ס תקף את ליברמן, עורר סערה והתנצל | צפו\n",
            "Title:  \"עם הארץ ממולדובה\": ח\"כ מש\"ס תקף את ליברמן, עורר סערה והתנצל | צפו\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            "  דאצ׳יה דאסטר החדש זמין עכשיו במלאי החל מ 113,900\n",
            "Title:   דאצ׳יה דאסטר החדש זמין עכשיו במלאי החל מ 113,900\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "בקואליציה קוראים למו\"מ, בצד השני לא מתלהבים\n",
            "Title: בקואליציה קוראים למו\"מ, בצד השני לא מתלהבים\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            " איתות לישראל? המדינה עימה סעודיה מחדשת את קשריה הדיפלומטיים\n",
            "Title:  איתות לישראל? המדינה עימה סעודיה מחדשת את קשריה הדיפלומטיים\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "סקר מנדטים: גוש נתניהו מתחזק במנדט\n",
            "Title: סקר מנדטים: גוש נתניהו מתחזק במנדט\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " שפית חושפת חמישה שלבים לפירה מושלם (כבר הראשון הפתיע אותנו)\n",
            "Title:  שפית חושפת חמישה שלבים לפירה מושלם (כבר הראשון הפתיע אותנו)\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            " פורטונה כולא פיצה בר - היפיפייה הנחבאת בסמטאות אילת\n",
            "Title:  פורטונה כולא פיצה בר - היפיפייה הנחבאת בסמטאות אילת\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            "זהבה גלאון הודיעה רשמית שתתמודד על ראשות מרצ\n",
            "Title: זהבה גלאון הודיעה רשמית שתתמודד על ראשות מרצ\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " יצאתם מבית חולים עם נזק רפואי? אם לא תתבעו תפסידו מאות אלפי ש\"ח\n",
            "Title:  יצאתם מבית חולים עם נזק רפואי? אם לא תתבעו תפסידו מאות אלפי ש\"ח\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " כמה לוקח עו\"ד על צוואה? מצאו מומחים בתחום\n",
            "Title:  כמה לוקח עו\"ד על צוואה? מצאו מומחים בתחום\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " סוד החיטוב של הסלביות: 10 דקות מהבית ללא חדר כושר לכל הגוף\n",
            "Title:  סוד החיטוב של הסלביות: 10 דקות מהבית ללא חדר כושר לכל הגוף\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " סובלים מיתושים? יש לנו פתרון בשבילכם\n",
            "Title:  סובלים מיתושים? יש לנו פתרון בשבילכם\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            "הקשיים של נתניהו בדרך להרכבת הממשלה נמשכים\n",
            "Title: הקשיים של נתניהו בדרך להרכבת הממשלה נמשכים\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "לפיד יצא לקראת נאומו מול עצרת האו\"ם\n",
            "Title: לפיד יצא לקראת נאומו מול עצרת האו\"ם\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "ליברמן: \"ממשלה צרה - צרה גדולה לישראל\"\n",
            "Title: ליברמן: \"ממשלה צרה - צרה גדולה לישראל\"\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " \"אני שומע מקרנות מחו\"ל שהן לא מעוניינות להשקיע בישראל\"\n",
            "Title:  \"אני שומע מקרנות מחו\"ל שהן לא מעוניינות להשקיע בישראל\"\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "מתיחות חריפה בין דרעי ללוין בשל עיתוי ההצהרה\n",
            "Title: מתיחות חריפה בין דרעי ללוין בשל עיתוי ההצהרה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " מתן אדלסון רכש את הפועל ירושלים: 'נהפוך לקבוצה הטובה בישראל'\n",
            "Title:  מתן אדלסון רכש את הפועל ירושלים: 'נהפוך לקבוצה הטובה בישראל'\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " לחיות ללא פחד | סיפור קצר  | הרב רועי מזרחי\n",
            "Title:  לחיות ללא פחד | סיפור קצר  | הרב רועי מזרחי\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " רוצים למצוא איזון בין ההיגיון והאינטואיציה? זה מה שעליכם לעשות\n",
            "Title:  רוצים למצוא איזון בין ההיגיון והאינטואיציה? זה מה שעליכם לעשות\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " יעזרו במלחמה ביוקר המחיה? הצעות החוק החדשות של יו\"ר ועדת הכלכלה דוד ביטן\n",
            "Title:  יעזרו במלחמה ביוקר המחיה? הצעות החוק החדשות של יו\"ר ועדת הכלכלה דוד ביטן\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "גרוניס יעמוד בראש הוועדה לחקר הצוללות \n",
            "Title: גרוניס יעמוד בראש הוועדה לחקר הצוללות \n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " חולמים על רכב חדש?  זה המקום שלכם\n",
            "Title:  חולמים על רכב חדש?  זה המקום שלכם\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "הקמפיין של בן גביר בחברה הערבית: אלחם בפשיעה\n",
            "Title: הקמפיין של בן גביר בחברה הערבית: אלחם בפשיעה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " 'תיזהר עמרי, לפני שיהיה מאוחר מדי': רגע לפני שידור הראיון עם 'שמפניה', איומים על כתב עובדה\n",
            "Title:  'תיזהר עמרי, לפני שיהיה מאוחר מדי': רגע לפני שידור הראיון עם 'שמפניה', איומים על כתב עובדה\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " מלי לוי: \"לא האמנתי שסוכריית גומי, תעשה את זה לשיער ולציפורניים שלי\"\n",
            "Title:  מלי לוי: \"לא האמנתי שסוכריית גומי, תעשה את זה לשיער ולציפורניים שלי\"\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            " רוכבת אופנוע כבת 30 נהרגה בתאונת דרכים בכביש 4, סמוך למחלף השבעה\n",
            "Title:  רוכבת אופנוע כבת 30 נהרגה בתאונת דרכים בכביש 4, סמוך למחלף השבעה\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " לא תאמינו למחיר: הוכרז אחד הקרוזים הארוכים בעולם, ויעצור בישראל\n",
            "Title:  לא תאמינו למחיר: הוכרז אחד הקרוזים הארוכים בעולם, ויעצור בישראל\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "המרוץ לתפקיד יו\"ר הכנסת נפתח מחדש\n",
            "Title: המרוץ לתפקיד יו\"ר הכנסת נפתח מחדש\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " אבי דיכטר: \"פועלים להורדת העלויות בענף החקלאות\"\n",
            "Title:  אבי דיכטר: \"פועלים להורדת העלויות בענף החקלאות\"\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "\"מסי החליט איפה ישחק, לא יחזור לברצלונה\"\n",
            "Title: \"מסי החליט איפה ישחק, לא יחזור לברצלונה\"\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " קניתם רכב חשמלי? יש לנו הצעה מיוחדת עבורכם\n",
            "Title:  קניתם רכב חשמלי? יש לנו הצעה מיוחדת עבורכם\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            " ישראל: מנתחי ברכיים מתרשמים מאוד משרוולי הברכיים האלה\n",
            "Title:  ישראל: מנתחי ברכיים מתרשמים מאוד משרוולי הברכיים האלה\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            " חסרה לכם עצם להשתלת שיניים? זה מה שאתם צריכים לעשות\n",
            "Title:  חסרה לכם עצם להשתלת שיניים? זה מה שאתם צריכים לעשות\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " \"חבל שהכסף הולך למגזר שברובו מעודד ומחזק את העוני\" | האזינו\n",
            "Title:  \"חבל שהכסף הולך למגזר שברובו מעודד ומחזק את העוני\" | האזינו\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " איזו טעות: השלב שאתם מדלגים עליו בניקוי השירותים | צפו\n",
            "Title:  איזו טעות: השלב שאתם מדלגים עליו בניקוי השירותים | צפו\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            "עקב פרסום הקלטה של בן גביר: הצעד של המשטרה\n",
            "Title: עקב פרסום הקלטה של בן גביר: הצעד של המשטרה\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "ראש הממשלה נתניהו החזיר את המנדט לנשיא \n",
            "Title: ראש הממשלה נתניהו החזיר את המנדט לנשיא \n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            " \"איך נוצר המין האנושי\": מאמר מאת מייסד הפאלון גונג, מר לי הונג-ג'י\n",
            "Title:  \"איך נוצר המין האנושי\": מאמר מאת מייסד הפאלון גונג, מר לי הונג-ג'י\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " לאחר התקיפות הרוסיות על קייב: מתקפת כטב\"מים חריגה על מוסקבה\n",
            "Title:  לאחר התקיפות הרוסיות על קייב: מתקפת כטב\"מים חריגה על מוסקבה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " אסטרטגיית השעה: זה הצעד הלא פופולרי שהאופוזיציה חייבת לעשות כדי לייצב את המדינה\n",
            "Title:  אסטרטגיית השעה: זה הצעד הלא פופולרי שהאופוזיציה חייבת לעשות כדי לייצב את המדינה\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " בטומי: דירות באזור המבוקש ביותר לתיירות בהחל מ-63,000 יורו\n",
            "Title:  בטומי: דירות באזור המבוקש ביותר לתיירות בהחל מ-63,000 יורו\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " הדבר הזה יבדר את כל המשפחה שלכם במשך שעות!\n",
            "Title:  הדבר הזה יבדר את כל המשפחה שלכם במשך שעות!\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " שאיבת שומן ללא ניתוח בישראל - איך זה עובד ומה המחירים?\n",
            "Title:  שאיבת שומן ללא ניתוח בישראל - איך זה עובד ומה המחירים?\n",
            "True label: 1, Predicted label: 0, רע\n",
            "\n",
            "השיחה של בן גביר עם שרה נתניהו על איילת שקד\n",
            "Title: השיחה של בן גביר עם שרה נתניהו על איילת שקד\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " הזמינו עכשיו טיסה לקיץ - טסים חכם עם אל על\n",
            "Title:  הזמינו עכשיו טיסה לקיץ - טסים חכם עם אל על\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "איזה בנק נותן תנאים מצוינים? [לחצו לגלות]\n",
            "Title: איזה בנק נותן תנאים מצוינים? [לחצו לגלות]\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "   זה הזמן להפסיק להתרוצץ בין סוכנויות\n",
            "Title:    זה הזמן להפסיק להתרוצץ בין סוכנויות\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "הזמר אדם חושף כי חלה בסרטן: \"מתקשה לעכל\"\n",
            "Title: הזמר אדם חושף כי חלה בסרטן: \"מתקשה לעכל\"\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " העברת נכסים לדור הבא: איך עושים את זה נכון?\n",
            "Title:  העברת נכסים לדור הבא: איך עושים את זה נכון?\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " הזמינו עכשיו את חופשת הקיץ המשפחתית - מבצעים בדקה 90\n",
            "Title:  הזמינו עכשיו את חופשת הקיץ המשפחתית - מבצעים בדקה 90\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "\"בכר כאן יומיים ופיטר אותי בשיחת טלפון\"\n",
            "Title: \"בכר כאן יומיים ופיטר אותי בשיחת טלפון\"\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " המומחים מבולבלים: מכשיר חדש לניקוי ריאות עוזר לכולם לנשום יותר בקלות?\n",
            "Title:  המומחים מבולבלים: מכשיר חדש לניקוי ריאות עוזר לכולם לנשום יותר בקלות?\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "ההבהרה של האופוזיציה והמתקפה על לוין\n",
            "Title: ההבהרה של האופוזיציה והמתקפה על לוין\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " הפיתוח הישראלי שמצעיר את עור הפנים בדקות ספורות\n",
            "Title:  הפיתוח הישראלי שמצעיר את עור הפנים בדקות ספורות\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "זה הזמן הטוב ביותר ביום כדי להתאמן ולרדת במשקל\n",
            "Title: זה הזמן הטוב ביותר ביום כדי להתאמן ולרדת במשקל\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "ההפגנות בפ\"ת - הפעם עם צוללת\n",
            "Title: ההפגנות בפ\"ת - הפעם עם צוללת\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " 24 שעות בירושלים: אלו הדברים שאתם חייבים לעשות בעיר הבירה\n",
            "Title:  24 שעות בירושלים: אלו הדברים שאתם חייבים לעשות בעיר הבירה\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " המחבל שהורשע בדריסת לוחמי גולני מערער על הרשעתו \n",
            "Title:  המחבל שהורשע בדריסת לוחמי גולני מערער על הרשעתו \n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "  שון דאוסון: \"מה אפשר לשנות? את השופטים\"\n",
            "Title:   שון דאוסון: \"מה אפשר לשנות? את השופטים\"\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " משלמים ביוקר על טיפול בקנביס? משאף מדיד לקנביס רפואי מציע הנחה\n",
            "Title:  משלמים ביוקר על טיפול בקנביס? משאף מדיד לקנביס רפואי מציע הנחה\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "סטודנטים? לחצו לגלות למה משתלם יותר בדיסקונט!\n",
            "Title: סטודנטים? לחצו לגלות למה משתלם יותר בדיסקונט!\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " השיטה המפתיעה שתגרום לאבוקדו להבשיל מהר יותר | מאיה רוזמן\n",
            "Title:  השיטה המפתיעה שתגרום לאבוקדו להבשיל מהר יותר | מאיה רוזמן\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " אבישי רביב מנסה להציג את עצמו כשעיר לעזאזל. הריאיון ל'עובדה' רק הוכיח כמה הוא לא\n",
            "Title:  אבישי רביב מנסה להציג את עצמו כשעיר לעזאזל. הריאיון ל'עובדה' רק הוכיח כמה הוא לא\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " \"לך תתאשפז\": עימות חריף בין גדעון סער ויריב לוין במליאה | צפו\n",
            "Title:  \"לך תתאשפז\": עימות חריף בין גדעון סער ויריב לוין במליאה | צפו\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " \"מצטער שלא הספקתי לעדכן אותך\": יהודה פוליקר משיק שיר שכתב לו יהונתן גפן\n",
            "Title:  \"מצטער שלא הספקתי לעדכן אותך\": יהודה פוליקר משיק שיר שכתב לו יהונתן גפן\n",
            "True label: 0, Predicted label: 1, רע\n",
            "\n",
            " זאת הגלידה החדשה של פאקינג ישראל אהרוני, מה כבר חשבתם שיהיה בה?\n",
            "Title:  זאת הגלידה החדשה של פאקינג ישראל אהרוני, מה כבר חשבתם שיהיה בה?\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            "יועמ\"שית הכנסת: לא ניתן לשנות מעמד היועמ\"שים\n",
            "Title: יועמ\"שית הכנסת: לא ניתן לשנות מעמד היועמ\"שים\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " מכשיר הEMS נגד נחירות שכולם בישראל מדברים עליו\n",
            "Title:  מכשיר הEMS נגד נחירות שכולם בישראל מדברים עליו\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "לקראת בחירות: הוגשה הצעת החוק לפיזור הכנסת\n",
            "Title: לקראת בחירות: הוגשה הצעת החוק לפיזור הכנסת\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " עינב בובליל: \"זה התוסף שעזר לי לחזור לגזרה\"\n",
            "Title:  עינב בובליל: \"זה התוסף שעזר לי לחזור לגזרה\"\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "חולדאי בתגובה ראשונה על תלונת הליכוד \n",
            "Title: חולדאי בתגובה ראשונה על תלונת הליכוד \n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            " \"מוטרדת מכך שבן גביר יגיע למצעד הגאווה בירושלים\" | האזינו\n",
            "Title:  \"מוטרדת מכך שבן גביר יגיע למצעד הגאווה בירושלים\" | האזינו\n",
            "True label: 0, Predicted label: 0, טוב\n",
            "\n",
            " \"לא לפחד מהשמאל\": המסר של לוין לנתניהו בישיבת סיעת הליכוד\n",
            "Title:  \"לא לפחד מהשמאל\": המסר של לוין לנתניהו בישיבת סיעת הליכוד\n",
            "True label: 1, Predicted label: 1, טוב\n",
            "\n",
            "Test Accuracy: 0.8530805687203792\n",
            "Test Precision: 0.8531188921960629\n",
            "Test Recall: 0.8530805687203792\n",
            "Test F1: 0.8530805687203792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(labels)):\n",
        "        title = df_test_hebrew['title'].iloc[i]  # This indexing should be reconsidered\n",
        "        true_label = labels[i]  # True labels from evaluation\n",
        "        predicted_label = preds[i]  # Predicted labels from evaluation\n",
        "        correctness = \"טוב\" if true_label == predicted_label else \"רע\"\n",
        "        print(true_label)\n",
        "   # if true_label != predicted_label:\n",
        "   #     print(f\"Title: {title}\")\n",
        "   #     print(f\"True label: {true_label}, Predicted label: {predicted_label}, {correctness}\")\n",
        "   #     print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "J4auzKiI_m8s",
        "executionInfo": {
          "status": "error",
          "timestamp": 1727130631106,
          "user_tz": -180,
          "elapsed": 452,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "a3f85546-d7fb-4c3f-fe3f-a6cf3754a356"
      },
      "id": "J4auzKiI_m8s",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-75cac70599d7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test_hebrew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# This indexing should be reconsidered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtrue_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# True labels from evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Predicted labels from evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcorrectness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"טוב\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrue_label\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpredicted_label\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"רע\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Function to load and clean dataset\n",
        "def load_data(filepath):\n",
        "    if filepath.endswith('.csv'):\n",
        "        return pd.read_csv(filepath)\n",
        "    elif filepath.endswith('.xlsx'):\n",
        "        return pd.read_excel(filepath)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {filepath}\")\n",
        "\n",
        "def clean_dataset(df):\n",
        "    if 'title' not in df.columns or 'label' not in df.columns:\n",
        "        raise ValueError(\"The dataframe must have 'title' and 'label' columns.\")\n",
        "    df = df.dropna(subset=['title', 'label'])  # Ensure no NaNs in title or label\n",
        "    df = df[df['title'].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]  # Ensure non-empty strings\n",
        "    df = df[df['label'].apply(lambda x: isinstance(x, (int, np.integer)) or str(x).isdigit())]  # Ensure valid labels\n",
        "\n",
        "    df['title'] = df['title'].astype(str)\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    return df\n",
        "\n",
        "# Dataset wrapper to handle input data\n",
        "class DatasetWrapper(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def create_data_loader(texts, labels, tokenizer, max_len, batch_size):\n",
        "    encoding = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    dataset = DatasetWrapper(encoding, labels)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, dataset_len):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for data in data_loader:\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_mask'].to(device)\n",
        "        labels = data['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / dataset_len, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, dataset_len):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            input_ids = data['input_ids'].to(device)\n",
        "            attention_mask = data['attention_mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    return correct_predictions.double() / dataset_len, np.mean(losses), all_labels, all_preds\n",
        "\n",
        "# Main function to train and test models\n",
        "def main():\n",
        "    RANDOM_SEED = 42\n",
        "    MAX_LEN = 128\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 3\n",
        "    TEST_SPLIT = 0.2\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    languages = ['hebrew', 'arabic', 'bangla', 'chinese', 'english', 'german', 'indonesian', 'romanian']\n",
        "    file_paths = {\n",
        "        'hebrew': '/hebrew.xlsx',\n",
        "        'arabic': '/arabic.xlsx',\n",
        "        'bangla': '/bangla.csv',\n",
        "        'chinese': '/chinese.csv',\n",
        "        'english': '/english.csv',\n",
        "        'german': '/german.csv',\n",
        "        'indonesian': '/indonesian.csv',\n",
        "        'romanian': '/romanianL.xlsx'\n",
        "    }\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "    # Load and clean datasets for all languages\n",
        "    dfs = {lang: clean_dataset(load_data(file_paths[lang])) for lang in languages}\n",
        "\n",
        "    # Initialize a list to store the results\n",
        "    results = []\n",
        "\n",
        "    # Loop to train on each language and test on all others\n",
        "    for train_lang in languages:\n",
        "        print(f\"\\nTraining on {train_lang}\")\n",
        "        df_train = dfs[train_lang]\n",
        "\n",
        "        # Train/Test split for the training language\n",
        "        df_train_lang, df_val_lang = train_test_split(df_train, test_size=TEST_SPLIT, random_state=RANDOM_SEED)\n",
        "\n",
        "        # Create DataLoader for training\n",
        "        train_data_loader = create_data_loader(\n",
        "            df_train_lang['title'].tolist(),\n",
        "            df_train_lang['label'].tolist(),\n",
        "            tokenizer,\n",
        "            MAX_LEN,\n",
        "            BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        val_data_loader = create_data_loader(\n",
        "            df_val_lang['title'].tolist(),\n",
        "            df_val_lang['label'].tolist(),\n",
        "            tokenizer,\n",
        "            MAX_LEN,\n",
        "            BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        # Initialize the model\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)\n",
        "        model = model.to(device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "        total_steps = len(train_data_loader) * EPOCHS\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(EPOCHS):\n",
        "            print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "            train_acc, train_loss = train_epoch(\n",
        "                model,\n",
        "                train_data_loader,\n",
        "                loss_fn,\n",
        "                optimizer,\n",
        "                device,\n",
        "                scheduler,\n",
        "                len(df_train_lang)\n",
        "            )\n",
        "            print(f'Train loss: {train_loss}, accuracy: {train_acc}')\n",
        "\n",
        "        # Evaluate on all other languages\n",
        "        for test_lang in languages:\n",
        "            if test_lang == train_lang:\n",
        "                continue  # Skip the training language for testing\n",
        "\n",
        "            print(f\"\\nEvaluating on {test_lang}\")\n",
        "            df_test = dfs[test_lang]\n",
        "\n",
        "            test_data_loader = create_data_loader(\n",
        "                df_test['title'].tolist(),\n",
        "                df_test['label'].tolist(),\n",
        "                tokenizer,\n",
        "                MAX_LEN,\n",
        "                BATCH_SIZE\n",
        "            )\n",
        "\n",
        "            test_acc, test_loss, labels, preds = eval_model(\n",
        "                model,\n",
        "                test_data_loader,\n",
        "                loss_fn,\n",
        "                device,\n",
        "                len(df_test)\n",
        "            )\n",
        "\n",
        "            precision = precision_score(labels, preds, average='weighted')\n",
        "            recall = recall_score(labels, preds, average='weighted')\n",
        "            f1 = f1_score(labels, preds, average='weighted')\n",
        "\n",
        "            # Store the result\n",
        "            results.append({\n",
        "                'Source Language': train_lang,\n",
        "                'Target Language': test_lang,\n",
        "                'Accuracy': test_acc.item(),  # Convert to float for compatibility with DataFrame\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1-Score': f1\n",
        "            })\n",
        "\n",
        "    # Convert the results to a DataFrame and print as a table\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(\"\\nCross-Language Results:\")\n",
        "    print(results_df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bdcTkcV5CaF",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726964752436,
          "user_tz": -180,
          "elapsed": 2729364,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "92f6cf61-cce4-4a64-a77a-766e11364cb4"
      },
      "id": "1bdcTkcV5CaF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on hebrew\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.5344139430320488, accuracy: 0.7285714285714286\n",
            "Epoch 2/3\n",
            "Train loss: 0.31471379312141884, accuracy: 0.8726190476190477\n",
            "Epoch 3/3\n",
            "Train loss: 0.18415789861442908, accuracy: 0.9416666666666668\n",
            "\n",
            "Evaluating on arabic\n",
            "\n",
            "Evaluating on bangla\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on chinese\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on english\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on german\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on indonesian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on romanian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on arabic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.4082080039222188, accuracy: 0.8072230750585764\n",
            "Epoch 2/3\n",
            "Train loss: 0.3063827399020047, accuracy: 0.8748485093318252\n",
            "Epoch 3/3\n",
            "Train loss: 0.26823153216824974, accuracy: 0.8864021976246264\n",
            "\n",
            "Evaluating on hebrew\n",
            "\n",
            "Evaluating on bangla\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on chinese\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on english\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on german\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on indonesian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on romanian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on bangla\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.5924757650641145, accuracy: 0.6721991701244814\n",
            "Epoch 2/3\n",
            "Train loss: 0.4838067510577499, accuracy: 0.7572614107883817\n",
            "Epoch 3/3\n",
            "Train loss: 0.358433407593946, accuracy: 0.8537344398340249\n",
            "\n",
            "Evaluating on hebrew\n",
            "\n",
            "Evaluating on arabic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on chinese\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on english\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on german\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on indonesian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on romanian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on chinese\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.18551950572066384, accuracy: 0.9256229360552387\n",
            "Epoch 2/3\n",
            "Train loss: 0.07580692258983233, accuracy: 0.973131191834284\n",
            "Epoch 3/3\n",
            "Train loss: 0.031064480184527093, accuracy: 0.9905433803662564\n",
            "\n",
            "Evaluating on hebrew\n",
            "\n",
            "Evaluating on arabic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on bangla\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on english\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on german\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on indonesian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on romanian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on english\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.036678601822732165, accuracy: 0.9878125\n",
            "Epoch 2/3\n",
            "Train loss: 0.009648211446710775, accuracy: 0.9971093750000001\n",
            "Epoch 3/3\n",
            "Train loss: 0.0027898219449662065, accuracy: 0.9992578125\n",
            "\n",
            "Evaluating on hebrew\n",
            "\n",
            "Evaluating on arabic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on bangla\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on chinese\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on german\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on indonesian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on romanian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on german\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.07802034882167341, accuracy: 0.9718099286692126\n",
            "Epoch 2/3\n",
            "Train loss: 0.03185688894267965, accuracy: 0.988759997117948\n",
            "Epoch 3/3\n",
            "Train loss: 0.012535029535835357, accuracy: 0.9956769219684416\n",
            "\n",
            "Evaluating on hebrew\n",
            "\n",
            "Evaluating on arabic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on bangla\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on chinese\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on english\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on indonesian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on romanian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on indonesian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "Train loss: 0.500509320229292, accuracy: 0.7618333333333334\n",
            "Epoch 2/3\n",
            "Train loss: 0.39669222151239714, accuracy: 0.825\n",
            "Epoch 3/3\n",
            "Train loss: 0.33304236811896165, accuracy: 0.85725\n",
            "\n",
            "Evaluating on hebrew\n",
            "\n",
            "Evaluating on arabic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on bangla\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on chinese\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on english\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on german\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on romanian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on romanian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.3839570510968128, accuracy: 0.8253767399056713\n",
            "Epoch 2/3\n",
            "Train loss: 0.2559551752090235, accuracy: 0.8937075808121477\n",
            "Epoch 3/3\n",
            "Train loss: 0.17417108927860253, accuracy: 0.9282181065224894\n",
            "\n",
            "Evaluating on hebrew\n",
            "\n",
            "Evaluating on arabic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on bangla\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on chinese\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on english\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on german\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on indonesian\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-590ea965ac34>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cross-Language Results:\n",
            "   Source Language Target Language  Accuracy  Precision    Recall  F1-Score\n",
            "0           hebrew          arabic  0.514995   0.527476  0.514995  0.452858\n",
            "1           hebrew          bangla  0.670539   0.732278  0.670539  0.677997\n",
            "2           hebrew         chinese  0.822226   0.822998  0.822226  0.804932\n",
            "3           hebrew         english  0.890062   0.893807  0.890062  0.889801\n",
            "4           hebrew          german  0.457678   0.664310  0.457678  0.538242\n",
            "5           hebrew      indonesian  0.691533   0.712008  0.691533  0.663056\n",
            "6           hebrew        romanian  0.728720   0.727873  0.728720  0.727772\n",
            "7           arabic          hebrew  0.411989   0.411209  0.411989  0.408571\n",
            "8           arabic          bangla  0.451452   0.521020  0.451452  0.460844\n",
            "9           arabic         chinese  0.467219   0.570224  0.467219  0.492709\n",
            "10          arabic         english  0.446844   0.442482  0.446844  0.436135\n",
            "11          arabic          german  0.392155   0.730122  0.392155  0.463334\n",
            "12          arabic      indonesian  0.446467   0.510318  0.446467  0.388941\n",
            "13          arabic        romanian  0.497561   0.496275  0.497561  0.496843\n",
            "14          bangla          hebrew  0.736441   0.754144  0.736441  0.730780\n",
            "15          bangla          arabic  0.509372   0.533703  0.509372  0.401321\n",
            "16          bangla         chinese  0.822767   0.816865  0.822767  0.812387\n",
            "17          bangla         english  0.630313   0.748234  0.630313  0.580477\n",
            "18          bangla          german  0.451236   0.656188  0.451236  0.532675\n",
            "19          bangla      indonesian  0.671933   0.740616  0.671933  0.615197\n",
            "20          bangla        romanian  0.618018   0.654167  0.618018  0.611035\n",
            "21         chinese          hebrew  0.737393   0.749418  0.737393  0.733357\n",
            "22         chinese          arabic  0.529149   0.541210  0.529149  0.491981\n",
            "23         chinese          bangla  0.673859   0.703464  0.673859  0.681100\n",
            "24         chinese         english  0.888687   0.889350  0.888687  0.888640\n",
            "25         chinese          german  0.338463   0.606588  0.338463  0.431557\n",
            "26         chinese      indonesian  0.726667   0.724832  0.726667  0.725180\n",
            "27         chinese        romanian  0.669090   0.699865  0.669090  0.665730\n",
            "28         english          hebrew  0.517602   0.558450  0.517602  0.387449\n",
            "29         english          arabic  0.504072   0.517142  0.504072  0.387278\n",
            "30         english          bangla  0.651452   0.618539  0.651452  0.613504\n",
            "31         english         chinese  0.386167   0.636935  0.386167  0.354697\n",
            "32         english          german  0.745908   0.717800  0.745908  0.731578\n",
            "33         english      indonesian  0.605933   0.605838  0.605933  0.605885\n",
            "34         english        romanian  0.449526   0.642501  0.449526  0.279768\n",
            "35          german          hebrew  0.379638   0.368729  0.379638  0.368537\n",
            "36          german          arabic  0.507821   0.507822  0.507821  0.507800\n",
            "37          german          bangla  0.474689   0.432683  0.474689  0.450679\n",
            "38          german         chinese  0.209834   0.216574  0.209834  0.138293\n",
            "39          german         english  0.153312   0.119100  0.153312  0.133865\n",
            "40          german      indonesian  0.383867   0.361045  0.383867  0.370088\n",
            "41          german        romanian  0.357596   0.350201  0.357596  0.353205\n",
            "42      indonesian          hebrew  0.753568   0.754519  0.753568  0.753104\n",
            "43      indonesian          arabic  0.535483   0.536922  0.535483  0.530914\n",
            "44      indonesian          bangla  0.613278   0.719283  0.613278  0.616553\n",
            "45      indonesian         chinese  0.797490   0.798607  0.797490  0.770786\n",
            "46      indonesian         english  0.884563   0.891877  0.884563  0.884021\n",
            "47      indonesian          german  0.458557   0.657456  0.458557  0.538636\n",
            "48      indonesian        romanian  0.715009   0.723613  0.715009  0.715630\n",
            "49        romanian          hebrew  0.738344   0.764491  0.738344  0.730570\n",
            "50        romanian          arabic  0.509566   0.518360  0.509566  0.442849\n",
            "51        romanian          bangla  0.691286   0.676656  0.691286  0.677339\n",
            "52        romanian         chinese  0.724604   0.765191  0.724604  0.735721\n",
            "53        romanian         english  0.767563   0.816764  0.767563  0.758168\n",
            "54        romanian          german  0.580826   0.691968  0.580826  0.630882\n",
            "55        romanian      indonesian  0.661133   0.682410  0.661133  0.620321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gcloud storage buckets list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "Avwdx7gIJZ2I",
        "executionInfo": {
          "status": "error",
          "timestamp": 1726730565973,
          "user_tz": -180,
          "elapsed": 451,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "0d07a35b-fb50-4568-8b7a-7f35aa1c44ec"
      },
      "id": "Avwdx7gIJZ2I",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-10-277ce96ec27d>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-277ce96ec27d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    gcloud storage buckets list\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "miri.zlotnik (Sep 17, 2024, 10:41:10 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}